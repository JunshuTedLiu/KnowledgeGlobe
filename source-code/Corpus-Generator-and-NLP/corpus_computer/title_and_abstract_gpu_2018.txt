Deep learning for DNase I hypersensitive sites identification BackgroundThe DNase I hypersensitive sites (DHSs) are associated with the cis-regulatory DNA elements. An efficient method of identifying DHSs can enhance the understanding on the accessibility of chromatin. Despite a multitude of resources available on line including experimental datasets and computational tools, the complex language of DHSs remains incompletely understood.MethodsHere, we address this challenge using an approach based on a state-of-the-art machine learning method. We present a novel convolutional neural network (CNN) which combined Inception like networks with a gating mechanism for the response of multiple patterns and longterm association in DNA sequences to predict multi-scale DHSs in Arabidopsis, rice and Homo sapiens.ResultsOur method obtains 0.961 area under curve (AUC) on Arabidopsis, 0.969 AUC on rice and 0.918 AUC on Homo sapiens.ConclusionsOur method provides an efficient and accurate way to identify multi-scale DHSs sequences by deep learning. Predicting protein-protein interactions using high-quality non-interacting pairs BackgroundIdentifying protein-protein interactions (PPIs) is of paramount importance for understanding cellular processes. Machine learning-based approaches have been developed to predict PPIs, but the effectiveness of these approaches is unsatisfactory. One major reason is that they randomly choose non-interacting protein pairs (negative samples) or heuristically select non-interacting pairs with low quality.ResultsTo boost the effectiveness of predicting PPIs, we propose two novel approaches (NIP-SS and NIP-RW) to generate high quality non-interacting pairs based on sequence similarity and random walk, respectively. Specifically, the known PPIs collected from public databases are used to generate the positive samples. NIP-SS then selects the top-m dissimilar protein pairs as negative examples and controls the degree distribution of selected proteins to construct the negative dataset. NIP-RW performs random walk on the PPI network to update the adjacency matrix of the network, and then selects protein pairs not connected in the updated network as negative samples. Next, we use auto covariance (AC) descriptor to encode the feature information of amino acid sequences. After that, we employ deep neural networks (DNNs) to predict PPIs based on extracted features, positive and negative examples. Extensive experiments show that NIP-SS and NIP-RW can generate negative samples with higher quality than existing strategies and thus enable more accurate prediction.ConclusionsThe experimental results prove that negative datasets constructed by NIP-SS and NIP-RW can reduce the bias and have good generalization ability. NIP-SS and NIP-RW can be used as a plugin to boost the effectiveness of PPIs prediction. Codes and datasets are available at http://mlda.swu.edu.cn/codes.php?name=NIP. Convolutional neural network based on SMILES representation of compounds for detecting chemical motif BackgroundPrevious studies have suggested deep learning to be a highly effective approach for screening lead compounds for new drugs. Several deep learning models have been developed by addressing the use of various kinds of fingerprints and graph convolution architectures. However, these methods are either advantageous or disadvantageous depending on whether they (1) can distinguish structural differences including chirality of compounds, and (2) can automatically discover effective features.ResultsWe developed another deep learning model for compound classification. In this method, we constructed a distributed representation of compounds based on the SMILES notation, which linearly represents a compound structure, and applied the SMILES-based representation to a convolutional neural network (CNN). The use of SMILES allows us to process all types of compounds while incorporating a broad range of structure information, and representation learning by CNN automatically acquires a low-dimensional representation of input features. In a benchmark experiment using the TOX 21 dataset, our method outperformed conventional fingerprint methods, and performed comparably against the winning model of the TOX 21 Challenge. Multivariate analysis confirmed that the chemical space consisting of the features learned by SMILES-based representation learning adequately expressed a richer feature space that enabled the accurate discrimination of compounds. Using motif detection with the learned filters, not only important known structures (motifs) such as protein-binding sites but also structures of unknown functional groups were detected.ConclusionsThe source code of our SMILES-based convolutional neural network software in the deep learning framework Chainer is available at http://www.dna.bio.keio.ac.jp/smiles/, and the dataset used for performance evaluation in this work is available at the same URL. Relative trajectory-driven virtual dynamic occlusal adjustment for dental restorations The abnormal occlusal contact can disrupt the coordination and health of the oral jaw system. Therefore, the dynamic adjustment of the occlusal surface is of great significance for assessing the status of occlusal contact and clarifying jaw factors of stomatognathic system diseases. To solve this problem, a trajectory subtraction algorithm based on screw theory to improve the accuracy of the occlusal movement trajectory is proposed in our paper. Driving by the relative trajectory, a virtual dynamic occlusal adjustment system is developed to realize 3D occlusal movement simulating, automatic occluding relation detection, and automatic occlusal adjustment. Furthermore, we adapt an active occlusal adjustment method based on Laplacian deformation to increase the contact areas of the occlusal surface, which can aid dentists to realize the automatic adjustment of the non-interference regions. As a consequence, the proposed subtraction algorithm is feasible and the root-mean-square is 0.097 mm, and the adjusted occlusal surface is more consistent with the natural occlusal morphology.
                Graphical abstractᅟ Augmented reality surgical navigation with accurate CBCT-patient registration for dental implant placement It is challenging to achieve high implant accuracy in dental implant placement, because high risk tissues need to be avoided. In this study, we present an augmented reality (AR) surgical navigation with an accurate cone beam computed tomography (CBCT)-patient registration method to provide clinically desired dental implant accuracy. A registration device is used for registration between preoperative data and patient outside the patient’s mouth. After registration, the registration device is worn on the patient’s teeth for tracking the patient. Naked-eye 3D images of the planning path and the mandibular nerve are superimposed onto the patient in situ to form an AR scene. Simultaneously, a 3D image of the drill is overlaid accurately on the real one to guide the implant procedure. Finally, implant accuracy is evaluated postoperatively. A model experiment was performed by an experienced dentist. Totally, ten parallel pins were inserted into five 3D-printed mandible models guided by our AR navigation method and through the dentist’s experience, respectively. AR-guided dental implant placement showed better results than the dentist’s experience (mean target error = 1.25 mm vs. 1.63 mm; mean angle error = 4.03° vs. 6.10°). Experimental results indicate that the proposed method is expected to be applied in the clinic.
                Graphical abstractᅟ DPM as a radiation transport engine for PRIMO BackgroundPRIMO is a dose verification system based on the general-purpose Monte Carlo radiation transport code penelope, which implements an accurate physics model of the interaction cross sections and the radiation transport process but with low computational efficiency as compared with fast Monte Carlo codes. One of these fast Monte Carlo codes is the Dose Planning Method (DPM). The purpose of this work is to describe the adaptation of DPM as an alternative PRIMO computation engine, to validate its performance against penelope and to validate it for some specific cases.MethodsDPM was parallelized and modified to perform radiation transport in quadric geometries, which are used to describe linacs, thus allowing the simulation of dynamic treatments. To benchmark the new code versus penelope, both in terms of accuracy of results and simulation time, several tests were performed, namely, irradiation of a multi-layer phantom, irradiation of a water phantom using a collimating pattern defined by the multileaf collimator (MLC), and four clinical cases. The gamma index, with passing criteria of 1 mm/1%, was used to compare the absorbed dose distributions. Clinical cases were compared using a 3-D gamma analysis.ResultsThe percentage of voxels passing the gamma criteria always exceeded 99% for the phantom cases, with the exception of the transport through air, for which dose differences between DPM and penelope were as large as 24%. The corresponding percentage for the clinical cases was larger than 99%. The speedup factor between DPM and penelope ranged from 2.5 ×, for the simulation of the radiation transport through a MLC and the subsequent dose estimation in a water phantom, up to 11.8 × for a lung treatment. A further increase of the computational speed, up to 25 ×, can be obtained in the clinical cases when a voxel size of (2.5 mm)3 is used.ConclusionsDPM has been incorporated as an efficient and accurate Monte Carlo engine for dose estimation in PRIMO. It allows the concatenated simulation of the patient-dependent part of the linac and the patient geometry in static and dynamic treatments. The discrepancy observed between DPM and penelope, which is due to an artifact of the cross section interpolation algorithm for low energy electrons in air, does not affect the results in other materials. CNN-MGP: Convolutional Neural Networks for Metagenomics Gene Prediction Accurate gene prediction in metagenomics fragments is a computationally challenging task due to the short-read length, incomplete, and fragmented nature of the data. Most gene-prediction programs are based on extracting a large number of features and then applying statistical approaches or supervised classification approaches to predict genes. In our study, we introduce a convolutional neural network for metagenomics gene prediction (CNN-MGP) program that predicts genes in metagenomics fragments directly from raw DNA sequences, without the need for manual feature extraction and feature selection stages. CNN-MGP is able to learn the characteristics of coding and non-coding regions and distinguish coding and non-coding open reading frames (ORFs). We train 10 CNN models on 10 mutually exclusive datasets based on pre-defined GC content ranges. We extract ORFs from each fragment; then, the ORFs are encoded numerically and inputted into an appropriate CNN model based on the fragment-GC content. The output from the CNN is the probability that an ORF will encode a gene. Finally, a greedy algorithm is used to select the final gene list. Overall, CNN-MGP is effective and achieves a 91% accuracy on testing dataset. CNN-MGP shows the ability of deep learning to predict genes in metagenomics fragments, and it achieves an accuracy higher than or comparable to state-of-the-art gene-prediction programs that use pre-defined features. The region based MMTD energy function for image segmentation This paper presents a region-based model based on measure of medium truth degree for image segmentation. Firstly, a new energy function based on measure of medium truth degree is constructed. To enhance the robustness against noise, a noise penalty term which is built by spatial distance measure is embedded to the conventional active contour energy function. Then local information is added to the internal and external energy term of the conventional active contour energy function to deal with intensity inhomogeneity images. Finally, to obtain more accurate and smoother boundary, a new stop function is introduced into the boundary smooth term of the conventional active contour energy function. Experiments results demonstrate that relatively complete and accurate boundaries can be obtained by the proposed model compared with the state-of-art methods on aurora images, images with intensity inhomogeneity, images with multi-objects, natural images, medical images. Abstracts from hydrocephalus 2018: the tenth meeting of the International Society for Hydrocephalus and Cerebrospinal Fluid Disorders  Fast and scalable neural embedding models for biomedical sentence classification BackgroundBiomedical literature is expanding rapidly, and tools that help locate information of interest are needed. To this end, a multitude of different approaches for classifying sentences in biomedical publications according to their coarse semantic and rhetoric categories (e.g., Background, Methods, Results, Conclusions) have been devised, with recent state-of-the-art results reported for a complex deep learning model. Recent evidence showed that shallow and wide neural models such as fastText can provide results that are competitive or superior to complex deep learning models while requiring drastically lower training times and having better scalability. We analyze the efficacy of the fastText model in the classification of biomedical sentences in the PubMed 200k RCT benchmark, and introduce a simple pre-processing step that enables the application of fastText on sentence sequences. Furthermore, we explore the utility of two unsupervised pre-training approaches in scenarios where labeled training data are limited.ResultsOur fastText-based methodology yields a state-of-the-art F1 score of.917 on the PubMed 200k benchmark when sentence ordering is taken into account, with a training time of only 73 s on standard hardware. Applying fastText on single sentences, without taking sentence ordering into account, yielded an F1 score of.852 (training time 13 s). Unsupervised pre-training of N-gram vectors greatly improved the results for small training set sizes, with an increase of F1 score of.21 to.74 when trained on only 1000 randomly picked sentences without taking sentence ordering into account.ConclusionsBecause of it’s ease of use and performance, fastText should be among the first choices of tools when tackling biomedical text classification problems with large corpora. Unsupervised pre-training of N-gram vectors on domain-specific corpora also makes it possible to apply fastText when labeled training data are limited. Artificial intelligence-assisted interpretation of bone age radiographs improves accuracy and decreases variability ObjectiveRadiographic bone age assessment (BAA) is used in the evaluation of pediatric endocrine and metabolic disorders. We previously developed an automated artificial intelligence (AI) deep learning algorithm to perform BAA using convolutional neural networks. We compared the BAA performance of a cohort of pediatric radiologists with and without AI assistance.Materials and methodsSix board-certified, subspecialty trained pediatric radiologists interpreted 280 age- and gender-matched bone age radiographs ranging from 5 to 18 years. Three of those radiologists then performed BAA with AI assistance. Bone age accuracy and root mean squared error (RMSE) were used as measures of accuracy. Intraclass correlation coefficient evaluated inter-rater variation.ResultsAI BAA accuracy was 68.2% overall and 98.6% within 1 year, and the mean six-reader cohort accuracy was 63.6 and 97.4% within 1 year. AI RMSE was 0.601 years, while mean single-reader RMSE was 0.661 years. Pooled RMSE decreased from 0.661 to 0.508 years, all individually decreasing with AI assistance. ICC without AI was 0.9914 and with AI was 0.9951.ConclusionsAI improves radiologist’s bone age assessment by increasing accuracy and decreasing variability and RMSE. The utilization of AI by radiologists improves performance compared to AI alone, a radiologist alone, or a pooled cohort of experts. This suggests that AI may optimally be utilized as an adjunct to radiologist interpretation of imaging studies to improve performance. A primer on deep learning in genomics Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial.This perspective presents a primer on deep learning applications for the genomics field. It includes a general guide for how to use deep learning and describes the current tools and resources that are available to the community. Real-time data analysis for medical diagnosis using FPGA-accelerated neural networks BackgroundReal-time analysis of patient data during medical procedures can provide vital diagnostic feedback that significantly improves chances of success. With sensors becoming increasingly fast, frameworks such as Deep Neural Networks are required to perform calculations within the strict timing constraints for real-time operation. However, traditional computing platforms responsible for running these algorithms incur a large overhead due to communication protocols, memory accesses, and static (often generic) architectures. In this work, we implement a low-latency Multi-Layer Perceptron (MLP) processor using Field Programmable Gate Arrays (FPGAs). Unlike CPUs and Graphics Processing Units (GPUs), our FPGA-based design can directly interface sensors, storage devices, display devices and even actuators, thus reducing the delays of data movement between ports and compute pipelines. Moreover, the compute pipelines themselves are tailored specifically to the application, improving resource utilization and reducing idle cycles. We demonstrate the effectiveness of our approach using mass-spectrometry data sets for real-time cancer detection.ResultsWe demonstrate that correct parameter sizing, based on the application, can reduce latency by 20% on average. Furthermore, we show that in an application with tightly coupled data-path and latency constraints, having a large amount of computing resources can actually reduce performance. Using mass-spectrometry benchmarks, we show that our proposed FPGA design outperforms both CPU and GPU implementations, with an average speedup of 144x and 21x, respectively.ConclusionIn our work, we demonstrate the importance of application-specific optimizations in order to minimize latency and maximize resource utilization for MLP inference. By directly interfacing and processing sensor data with ultra-low latency, FPGAs can perform real-time analysis during procedures and provide diagnostic feedback that can be critical to achieving higher percentages of successful patient outcomes. Large scale hybrid Monte Carlo simulations for structure and property prediction The Monte Carlo method is one of the first and most widely used algorithms in modern computational physics. In condensed matter physics, the particularly popular flavor of this technique is the Metropolis Monte Carlo scheme. While being incredibly robust and easy to implement, the Metropolis sampling is not well-suited for situations where energy and force evaluations are computationally demanding. In search for a more efficient technique, we here explore the performance of Hybrid Monte Carlo sampling, an algorithm widely used in quantum electrodynamics, as a structure prediction scheme for systems with long-range interactions. Our results show that the Hybrid Monte Carlo algorithm stands out as an excellent computational scheme that can not only significantly outperform the Metropolis sampling but also complement molecular dynamics in materials science applications, while allowing ultra-large-scale simulations of systems containing millions of particles.Monte Carlo simulations: scaling-up property predictionA hybrid Monte Carlo sampling algorithm is adopted to predict structures and properties in large-scale simulations with millions of particles. A team led by Laurent Bellaiche from the University of Arkansas perform hybrid Monte Carlo (HMC) sampling on effective Hamiltonian models of solid-state systems with long-range interactions, such as ferroelectric, relaxor and multiferroic materials at finite temperatures. They compare the results with those obtained by the Metropolis Monte Carlo (MMC) algorithm and thermalized molecular dynamics (MD). They find that the HMC scheme significantly outperforms MMC and MD for selected model cases. By implementing the HMC algorithm for GPU-oriented parallelization architectures, they can perform HMC simulations for a large scale material simulations with the particle number reaching 106. This algorithm may also be implemented for large-scale density functional theory calculations so that a more broad space of applications might open. Scalable deep text comprehension for Cancer surveillance on high-performance computing BackgroundDeep Learning (DL) has advanced the state-of-the-art capabilities in bioinformatics applications which has resulted in trends of increasingly sophisticated and computationally demanding models trained by larger and larger data sets. This vastly increased computational demand challenges the feasibility of conducting cutting-edge research. One solution is to distribute the vast computational workload across multiple computing cluster nodes with data parallelism algorithms. In this study, we used a High-Performance Computing environment and implemented the Downpour Stochastic Gradient Descent algorithm for data parallelism to train a Convolutional Neural Network (CNN) for the natural language processing task of information extraction from a massive dataset of cancer pathology reports. We evaluated the scalability improvements using data parallelism training and the Titan supercomputer at Oak Ridge Leadership Computing Facility. To evaluate scalability, we used different numbers of worker nodes and performed a set of experiments comparing the effects of different training batch sizes and optimizer functions.ResultsWe found that Adadelta would consistently converge at a lower validation loss, though requiring over twice as many training epochs as the fastest converging optimizer, RMSProp. The Adam optimizer consistently achieved a close 2nd place minimum validation loss significantly faster; using a batch size of 16 and 32 allowed the network to converge in only 4.5 training epochs.ConclusionsWe demonstrated that the networked training process is scalable across multiple compute nodes communicating with message passing interface while achieving higher classification accuracy compared to a traditional machine learning algorithm. High-throughput cancer hypothesis testing with an integrated PhysiCell-EMEWS workflow BackgroundCancer is a complex, multiscale dynamical system, with interactions between tumor cells and non-cancerous host systems. Therapies act on this combined cancer-host system, sometimes with unexpected results. Systematic investigation of mechanistic computational models can augment traditional laboratory and clinical studies, helping identify the factors driving a treatment’s success or failure. However, given the uncertainties regarding the underlying biology, these multiscale computational models can take many potential forms, in addition to encompassing high-dimensional parameter spaces. Therefore, the exploration of these models is computationally challenging. We propose that integrating two existing technologies—one to aid the construction of multiscale agent-based models, the other developed to enhance model exploration and optimization—can provide a computational means for high-throughput hypothesis testing, and eventually, optimization.ResultsIn this paper, we introduce a high throughput computing (HTC) framework that integrates a mechanistic 3-D multicellular simulator (PhysiCell) with an extreme-scale model exploration platform (EMEWS) to investigate high-dimensional parameter spaces. We show early results in applying PhysiCell-EMEWS to 3-D cancer immunotherapy and show insights on therapeutic failure. We describe a generalized PhysiCell-EMEWS workflow for high-throughput cancer hypothesis testing, where hundreds or thousands of mechanistic simulations are compared against data-driven error metrics to perform hypothesis optimization.ConclusionsWhile key notational and computational challenges remain, mechanistic agent-based models and high-throughput model exploration environments can be combined to systematically and rapidly explore key problems in cancer. These high-throughput computational experiments can improve our understanding of the underlying biology, drive future experiments, and ultimately inform clinical practice. An SVM-based method for assessment of transcription factor-DNA complex models BackgroundAtomic details of protein-DNA complexes can provide insightful information for better understanding of the function and binding specificity of DNA binding proteins. In addition to experimental methods for solving protein-DNA complex structures, protein-DNA docking can be used to predict native or near-native complex models. A docking program typically generates a large number of complex conformations and predicts the complex model(s) based on interaction energies between protein and DNA. However, the prediction accuracy is hampered by current approaches to model assessment, especially when docking simulations fail to produce any near-native models.ResultsWe present here a Support Vector Machine (SVM)-based approach for quality assessment of the predicted transcription factor (TF)-DNA complex models. Besides a knowledge-based protein-DNA interaction potential DDNA3, we applied several structural features that have been shown to play important roles in binding specificity between transcription factors and DNA molecules to quality assessment of complex models. To address the issue of unbalanced positive and negative cases in the training dataset, we applied hard-negative mining, an iterative training process that selects an initial training dataset by combining all of the positive cases and a random sample from the negative cases. Results show that the SVM model greatly improves prediction accuracy (84.2%) over two knowledge-based protein-DNA interaction potentials, orientation potential (60.8%) and DDNA3 (68.4%). The improvement is achieved through reducing the number of false positive predictions, especially for the hard docking cases, in which a docking algorithm fails to produce any near-native complex models.ConclusionsA learning-based SVM scoring model with structural features for specific protein-DNA binding and an atomic-level protein-DNA interaction potential DDNA3 significantly improves prediction accuracy of complex models by successfully identifying cases without near-native structural models. Sparse coding of pathology slides compared to transfer learning with deep neural networks BackgroundHistopathology images of tumor biopsies present unique challenges for applying machine learning to the diagnosis and treatment of cancer. The pathology slides are high resolution, often exceeding 1GB, have non-uniform dimensions, and often contain multiple tissue slices of varying sizes surrounded by large empty regions. The locations of abnormal or cancerous cells, which may constitute a small portion of any given tissue sample, are not annotated. Cancer image datasets are also extremely imbalanced, with most slides being associated with relatively common cancers. Since deep representations trained on natural photographs are unlikely to be optimal for classifying pathology slide images, which have different spectral ranges and spatial structure, we here describe an approach for learning features and inferring representations of cancer pathology slides based on sparse coding.ResultsWe show that conventional transfer learning using a state-of-the-art deep learning architecture pre-trained on ImageNet (RESNET) and fine tuned for a binary tumor/no-tumor classification task achieved between 85% and 86% accuracy. However, when all layers up to the last convolutional layer in RESNET are replaced with a single feature map inferred via a sparse coding using a dictionary optimized for sparse reconstruction of unlabeled pathology slides, classification performance improves to over 93%, corresponding to a 54% error reduction.ConclusionsWe conclude that a feature dictionary optimized for biomedical imagery may in general support better classification performance than does conventional transfer learning using a dictionary pre-trained on natural images. CANDLE/Supervisor: a workflow framework for machine learning applied to cancer research BackgroundCurrent multi-petaflop supercomputers are powerful systems, but present challenges when faced with problems requiring large machine learning workflows. Complex algorithms running at system scale, often with different patterns that require disparate software packages and complex data flows cause difficulties in assembling and managing large experiments on these machines.ResultsThis paper presents a workflow system that makes progress on scaling machine learning ensembles, specifically in this first release, ensembles of deep neural networks that address problems in cancer research across the atomistic, molecular and population scales. The initial release of the application framework that we call CANDLE/Supervisor addresses the problem of hyper-parameter exploration of deep neural networks.ConclusionsInitial results demonstrating CANDLE on DOE systems at ORNL, ANL and NERSC (Titan, Theta and Cori, respectively) demonstrate both scaling and multi-platform execution. Low-loss YIG-based magnonic crystals with large tunable bandgaps Control of spin waves in magnonic crystals is essential for magnon-based computing. Crystals made of ferromagnetic metals offer versatility in band structure design, but strong magnetic damping restricts their transmission efficiency. Yttrium iron garnet (YIG) with ultralow damping is the palpable alternative, yet its small saturation magnetization limits dipolar coupling between discrete units. Here, we experimentally demonstrate low-loss spin-wave manipulation in magnonic crystals of physically separated nanometer-thick YIG stripes. We enhance the transmission of spin waves in allowed minibands by filling the gaps between YIG stripes with CoFeB. Thus-formed magnonic crystals exhibit tunable bandgaps of 50–200 MHz with nearly complete suppression of the spin-wave signal. We also show that Bragg scattering on only two units produces clear frequency gaps in spin-wave transmission spectra. The integration of strong ferromagnets in nanometer-thick YIG-based magnonic crystals provides effective spin-wave manipulation and low-loss propagation, a vital parameter combination for magnonic technologies.Control of spin wave transport in magnonic crystals is vital for magnonic devices. Here the authors show low-loss spin-wave manipulation in nanometer thick magnonic crystals of discrete YIG stripes separated by air or CoFeB filled grooves exhibiting tunable bandgaps of 50–200 MHz. An experimental design framework for Markovian gene regulatory networks under stationary control policy BackgroundA fundamental problem for translational genomics is to find optimal therapies based on gene regulatory intervention. Dynamic intervention involves a control policy that optimally reduces a cost function based on phenotype by externally altering the state of the network over time. When a gene regulatory network (GRN) model is fully known, the problem is addressed using classical dynamic programming based on the Markov chain associated with the network. When the network is uncertain, a Bayesian framework can be applied, where policy optimality is with respect to both the dynamical objective and the uncertainty, as characterized by a prior distribution. In the presence of uncertainty, it is of great practical interest to develop an experimental design strategy and thereby select experiments that optimally reduce a measure of uncertainty.ResultsIn this paper, we employ mean objective cost of uncertainty (MOCU), which quantifies uncertainty based on the degree to which uncertainty degrades the operational objective, that being the cost owing to undesirable phenotypes. We assume that a number of conditional probabilities characterizing regulatory relationships among genes are unknown in the Markovian GRN. In sum, there is a prior distribution which can be updated to a posterior distribution by observing a regulatory trajectory, and an optimal control policy, known as an “intrinsically Bayesian robust” (IBR) policy. To obtain a better IBR policy, we select an experiment that minimizes the MOCU remaining after applying its output to the network. At this point, we can either stop and find the resulting IBR policy or proceed to determine more unknown conditional probabilities via regulatory observation and find the IBR policy from the resulting posterior distribution. For sequential experimental design this entire process is iterated. Owing to the computational complexity of experimental design, which requires computation of many potential IBR policies, we implement an approximate method utilizing mean first passage times (MFPTs) – but only in experimental design, the final policy being an IBR policy.ConclusionsComprehensive performance analysis based on extensive simulations on synthetic and real GRNs demonstrate the efficacy of the proposed method, including the accuracy and computational advantage of the approximate MFPT-based design. Biomedical semantic indexing by deep neural network with multi-task learning BackgroundBiomedical semantic indexing is important for information retrieval and many other research fields in bioinformatics. It annotates biomedical citations with Medical Subject Headings. In face of unbalanced category distribution in the training data, sampling methods are difficult to apply for semantic indexing task.ResultsIn this paper, we present a novel deep serial multi-task learning model. The primary task treats the biomedical semantic indexing as a multi-label text classification issue that considers the relations of the labels. The auxiliary task is a regression task that predicts the MeSH number of the citation and provides hints for the network to make it converge faster. The experimental results on the BioASQ-Task5A open dataset show that our model outperforms the state-of-the-art solution “MTI”, proposed by the US National Library of Medicine. Further, it not only achieves the highest precision among all the solutions in BioASQ-Task5A but also has faster convergence speed compared with some naive deep learning methods.ConclusionsRather than parallel in an ordinary multi-task structure, the tasks in our model are serial and tightly coupled. It can achieve satisfied performance without any handcrafted feature. Fast animal pose estimation using deep neural networks The need for automated and efficient systems for tracking full animal pose has increased with the complexity of behavioral data and analyses. Here we introduce LEAP (LEAP estimates animal pose), a deep-learning-based method for predicting the positions of animal body parts. This framework consists of a graphical interface for labeling of body parts and training the network. LEAP offers fast prediction on new data, and training with as few as 100 frames results in 95% of peak performance. We validated LEAP using videos of freely behaving fruit flies and tracked 32 distinct points to describe the pose of the head, body, wings and legs, with an error rate of <3% of body length. We recapitulated reported findings on insect gait dynamics and demonstrated LEAP’s applicability for unsupervised behavioral classification. Finally, we extended the method to more challenging imaging situations and videos of freely moving mice.LEAP is a deep-learning-based approach for the analysis of animal pose. LEAP’s graphical user interface facilitates training of the deep network. The authors illustrate the method by analyzing Drosophila and mouse behavior. Deep learning enables cross-modality super-resolution in fluorescence microscopy We present deep-learning-enabled super-resolution across different fluorescence microscopy modalities. This data-driven approach does not require numerical modeling of the imaging process or the estimation of a point-spread-function, and is based on training a generative adversarial network (GAN) to transform diffraction-limited input images into super-resolved ones. Using this framework, we improve the resolution of wide-field images acquired with low-numerical-aperture objectives, matching the resolution that is acquired using high-numerical-aperture objectives. We also demonstrate cross-modality super-resolution, transforming confocal microscopy images to match the resolution acquired with a stimulated emission depletion (STED) microscope. We further demonstrate that total internal reflection fluorescence (TIRF) microscopy images of subcellular structures within cells and tissues can be transformed to match the results obtained with a TIRF-based structured illumination microscope. The deep network rapidly outputs these super-resolved images, without any iterations or parameter search, and could serve to democratize super-resolution imaging.Deep learning enables cross-modality super-resolution imaging, including confocal-to-STED and TIRF-to-TIRF-SIM image transformation. Imaging of a larger FOV and greater depth of field is possible with higher resolution and SNR at lower light doses. Detection of suspected brain infarctions on CT can be significantly improved with temporal subtraction images ObjectiveTo assess whether temporal subtraction (TS) images of brain CT improve the detection of suspected brain infarctions.MethodsStudy protocols were approved by our institutional review board, and informed consent was waived because of the retrospective nature of this study. Forty-two sets of brain CT images of 41 patients, each consisting of a pair of brain CT images scanned at two time points (previous and current) between January 2011 and November 2016, were collected for an observer performance study. The 42 sets consisted of 23 cases with a total of 77 newly developed brain infarcts or hyperdense artery signs confirmed by two radiologists who referred to additional clinical information and 19 negative control cases. To create TS images, the previous images were registered to the current images by partly using a non-rigid registration algorithm and then subtracted. Fourteen radiologists independently interpreted the images to identify the lesions with and without TS images with an interval of over 4 weeks. A figure of merit (FOM) was calculated along with the jackknife alternative free-response receiver-operating characteristic analysis. Sensitivity, number of false positives per case (FPC) and reading time were analyzed by the Wilcoxon signed-rank test.ResultsThe mean FOM increased from 0.528 to 0.737 with TS images (p < 0.0001). The mean sensitivity and FPC improved from 26.5% and 0.243 to 56.0% and 0.153 (p < 0.0001 and p = 0.239), respectively. The mean reading time was 173 s without TS and 170 s with TS (p = 0.925).ConclusionThe detectability of suspected brain infarctions was significantly improved with TS CT images.Key Points• Although it is established that MRI is superior to CT in the detection of strokes, the first choice of modality for suspected stroke patients is often CT.• An observer performance study with 14 radiologists was performed to evaluate whether temporal subtraction images derived from a non-rigid transformation algorithm can significantly improve the detectability of newly developed brain infarcts on CT.• Temporal subtraction images were shown to significantly improve the detectability of newly developed brain infarcts on CT. Imaging whole nervous systems: insights into behavior from worms to fish lopment of systems combining rapid volumetric imaging with three-dimensional tracking has enabled the measurement of brain-wide dynamics in freely behaving animals such as worms, flies, and fish. These advances provide an exciting opportunity to understand the organization of neural circuits in the context of voluntary and natural behaviors. In this Comment, we highlight recent progress in this burgeoning area of research. Structure of native lens connexin 46/50 intercellular channels by cryo-EM Gap junctions establish direct pathways for cell-to-cell communication through the assembly of twelve connexin subunits that form intercellular channels connecting neighbouring cells. Co-assembly of different connexin isoforms produces channels with unique properties and enables communication across cell types. Here we used single-particle cryo-electron microscopy to investigate the structural basis of connexin co-assembly in native lens gap junction channels composed of connexin 46 and connexin 50 (Cx46/50). We provide the first comparative analysis to connexin 26 (Cx26), which—together with computational studies—elucidates key energetic features governing gap junction permselectivity. Cx46/50 adopts an open-state conformation that is distinct from the Cx26 crystal structure, yet it appears to be stabilized by a conserved set of hydrophobic anchoring residues. ‘Hot spots’ of genetic mutations linked to hereditary cataract formation map to the core structural–functional elements identified in Cx46/50, suggesting explanations for many of the disease-causing effects.Cryo-electron microscopy structures of connexin channels composed of connexin 46 and connexin 50 in an open-state reveal features that govern permselectivity and the location of mutated residues linked to herediatry cataracts. Subcellular spatial resolution achieved for deep-brain imaging in vivo using a minimally invasive multimode fiber Achieving intravital optical imaging with diffraction-limited spatial resolution of deep-brain structures represents an important step toward the goal of understanding the mammalian central nervous system1–4. Advances in wavefront-shaping methods and computational power have recently allowed for a novel approach to high-resolution imaging, utilizing deterministic light propagation through optically complex media and, of particular importance for this work, multimode optical fibers (MMFs)5–7. We report a compact and highly optimized approach for minimally invasive in vivo brain imaging applications. The volume of tissue lesion was reduced by more than 100-fold, while preserving diffraction-limited imaging performance utilizing wavefront control of light propagation through a single 50-μm-core MMF. Here, we demonstrated high-resolution fluorescence imaging of subcellular neuronal structures, dendrites and synaptic specializations, in deep-brain regions of living mice, as well as monitored stimulus-driven functional Ca2+ responses. These results represent a major breakthrough in the compromise between high-resolution imaging and tissue damage, heralding new possibilities for deep-brain imaging in vivo. Disparity estimation in stereo video sequence with adaptive spatiotemporally consistent constraints Numerous stereo matching algorithms have been proposed to obtain disparity estimation for a single pair of stereo images. However, simply even applying the best of them to temporal frames independently, i.e., without considering the temporal consistency between consecutive frames, may suffer from the undesirable artifacts. Here, we proposed an adaptive, spatiotemporally consistent, constraints-based systematic method that generates spatiotemporally consistent disparity maps for stereo video image sequences. Firstly, a reliable temporal neighborhood is used to enforce the “self-similarity” assumption and prevent errors caused by false optical flow matching from propagating between consecutive frames. Furthermore, we formulate the adaptive temporal predicted disparity map as prior knowledge of the current frame. It is used as a soft constraint to enhance the temporal consistency of disparities, increase the robustness to luminance variance, and restrict the range of the potential disparities for each pixel. Additionally, to further strengthen smooth variation of disparities, the adaptive temporal segment confidence is incorporated as a soft constraint to reduce ambiguities caused by under- and over-segmentation, and retain the disparity discontinuities that align with 3D object boundaries from geometrically smooth, but strong color gradient regions. Experimental evaluations demonstrate that our method significantly improves the spatiotemporal consistency both quantitatively and qualitatively compared with other state-of-the-art methods on the synthetic DCB and realistic KITTI datasets. Small Animal Multivariate Brain Analysis (SAMBA) – a High Throughput Pipeline with a Validation Framework While many neuroscience questions aim to understand the human brain, much current knowledge has been gained using animal models, which replicate genetic, structural, and connectivity aspects of the human brain. While voxel-based analysis (VBA) of preclinical magnetic resonance images is widely-used, a thorough examination of the statistical robustness, stability, and error rates is hindered by high computational demands of processing large arrays, and the many parameters involved therein. Thus, workflows are often based on intuition or experience, while preclinical validation studies remain scarce. To increase throughput and reproducibility of quantitative small animal brain studies, we have developed a publicly shared, high throughput VBA pipeline in a high-performance computing environment, called SAMBA. The increased computational efficiency allowed large multidimensional arrays to be processed in 1–3 days—a task that previously took ~1 month. To quantify the variability and reliability of preclinical VBA in rodent models, we propose a validation framework consisting of morphological phantoms, and four metrics. This addresses several sources that impact VBA results, including registration and template construction strategies. We have used this framework to inform the VBA workflow parameters in a VBA study for a mouse model of epilepsy. We also present initial efforts towards standardizing small animal neuroimaging data in a similar fashion with human neuroimaging. We conclude that verifying the accuracy of VBA merits attention, and should be the focus of a broader effort within the community. The proposed framework promotes consistent quality assurance of VBA in preclinical neuroimaging, thus facilitating the creation and communication of robust results. Multi-task learning using a hybrid representation for text classification Text classification is an important task in machine learning. Specifically, deep neural network has been shown strong capability to improve performance in different fields, for example speech recognition, objects recognition and natural language processing. However, in most previous work, the extracted feature models do not achieve the relative text tasks well. To address this issue, we introduce a novel multi-task learning approach called a hybrid representation-learning network for text classification tasks. Our method consists of two network components: a bidirectional gated recurrent unit with attention network module and a convolutional neural network module. In particular, the attention module allows for the task learning private feature representation in local dependence from training texts and that the convolutional neural network module can learn the global representation on sharing. Experiments on 16 subsets of Amazon review data show that our method outperforms several baselines and also proves the effectiveness of joint learning multi-relative tasks. Two-stage deep learning for supervised cross-modal retrieval This paper deals with the problem of modeling internet images and associated texts for cross-modal retrieval such as text-to-image retrieval and image-to-text retrieval. Recently, supervised cross-modal retrieval has attracted increasing attention. Inspired by a typical two-stage method, i.e., semantic correlation matching(SCM), we propose a novel two-stage deep learning method for supervised cross-modal retrieval. Limited by the fact that traditional canonical correlation analysis (CCA) is a 2-view method, the supervised semantic information is only considered in the second stage of SCM. To maximize the value of semantics, we expand CCA from 2-view to 3-view and conduct supervised learning in both stages. In the first learning stage, we embed 3-view CCA into a deep architecture to learn non-linear correlation between image, text and semantics. To overcome over-fitting, we add the reconstruct loss of each view into the loss function, which includes the correlation loss of every two views and regularization of parameters. In the second stage, we build a novel fully-convolutional network (FCN), which is trained by joint supervision of contrastive loss and center loss to learn better features. The proposed method is evaluated on two publicly available data sets, and the experimental results show that our method is competitive with state-of-the-art methods. Decision-driven scheduling This paper presents a scheduling model, called decision-driven scheduling, elaborates key optimality results for a fundamental scheduling model, and evaluates new heuristics solving more general versions of the problem. In the context of applications that need control and actuation, the traditional execution model has often been either time-driven or event-driven. In time-driven applications, sensors are sampled periodically, leading to the classical periodic task model. In event-driven applications, sensors are sampled when an event of interest occurs, such as motion-activated cameras, leading to an event-driven task activation model. In contrast, in decision-driven applications, sensors are sampled when a particular decision must be made. We offer a justification for why decision-driven scheduling might be of increasing interest to Internet-of-things applications, and explain why it leads to interesting new scheduling problems (unlike time-driven and event-driven scheduling), including the problems addressed in this paper. HEVC optimization based on human perception for real-time environments High-Efficiency Video Coding (HEVC) is the new emerging video coding standard of the ITU-T Video Coding Experts Group (VCEG) and the ISO/IEC Moving Picture Experts Group (MPEG). The HEVC standard provides a significant improvement in compression efficiency in comparison with existing standards such as H264/AVC by means of greater complexity. In this paper we will examine several HEVC optimizations based on image analysis to reduce its huge CPU, resource and memory expensive encoding process. The proposed algorithms optimize the HEVC quad-tree partitioning procedure, intra/inter prediction and mode decision by means of H264-based methods and spatial and temporal homogeneity analysis which is directly applied to the original video. The validation process of these approaches was conducted by taking into account the human visual system (HVS). The adopted solution makes it possible to perform HEVC real time encoding for HD sequences on a low cost processor with negligible quality loss. Moreover, the frames pre-processing leverages the logic units and embedded hardware available on an Intel GPU, so the execution time of these stages are negligible for the encoding processor. Incorporating word embeddings into topic modeling of short text Short texts have become the prevalent format of information on the Internet. Inferring the topics of this type of messages becomes a critical and challenging task for many applications. Due to the length of short texts, conventional topic models (e.g., latent Dirichlet allocation and its variants) suffer from the severe data sparsity problem which makes topic modeling of short texts difficult and unreliable. Recently, word embeddings have been proved effective to capture semantic and syntactic information about words, which can be used to induce similarity measures and semantic correlations among words. Enlightened by this, in this paper, we design a novel model for short text topic modeling, referred as Conditional Random Field regularized Topic Model (CRFTM). CRFTM not only develops a generalized solution to alleviate the sparsity problem by aggregating short texts into pseudo-documents, but also leverages a Conditional Random Field regularized model that encourages semantically related words to share the same topic assignment. Experimental results on two real-world datasets show that our method can extract more coherent topics, and significantly outperform state-of-the-art baselines on several evaluation metrics. Towards A Next Generation of CORSIKA: A Framework for the Simulation of Particle Cascades in Astroparticle Physics A large scientific community depends on the precise modeling of complex processes in particle cascades in various types of matter. These models are used most prevalently in cosmic ray physics, astrophysical-neutrino physics, and gamma ray astronomy. In this white paper, we summarize the necessary steps to ensure the evolution and future availability of optimal simulation tools. The purpose of this document is not to act as a strict blueprint for next-generation software, but to provide guidance for the vital aspects of its design. The topics considered here are driven by physics and scientific applications. Furthermore, the main consequences of implementation decisions on performance are outlined. We highlight the computational performance as an important aspect guiding the design, since future scientific applications will heavily depend on an efficient use of computational resources. Collection, pre-processing and on-the-fly analysis of data for high-resolution, single-particle cryo-electron microscopy The dramatic growth in the use of cryo-electron microscopy (cryo-EM) to generate high-resolution structures of macromolecular complexes has changed the landscape of structural biology. The majority of structures deposited in the Electron Microscopy Data Bank (EMDB) at higher than 4-Å resolution were collected on Titan Krios microscopes. Although the pipeline for single-particle data collection is becoming routine, there is much variation in how sessions are set up. Furthermore, when collection is under way, there are a range of approaches for efficiently moving and pre-processing these data. Here, we present a standard operating procedure for single-particle data collection with Thermo Fisher Scientific EPU software, using the two most common direct electron detectors (the Thermo Fisher Scientific Falcon 3 (F3EC) and the Gatan K2), as well as a strategy for structuring these data to enable efficient pre-processing and on-the-fly monitoring of data collection. This protocol takes 3–6 h to set up a typical automated data collection session.This protocol describes a pipeline for data collection, pre-processing and on-the-fly analysis for single-particle cryo-electron microscopy using EPU software and two direct electron detectors: the Thermo Fisher Scientific Falcon 3 and the Gatan K2. Local hippocampal fast gamma rhythms precede brain-wide hyperemic patterns during spontaneous rodent REM sleep Rapid eye movement sleep (REMS) is a peculiar brain state combining the behavioral components of sleep and the electrophysiological profiles of wake. After decades of research our understanding of REMS still is precluded by the difficulty to observe its spontaneous dynamics and the lack of multimodal recording approaches to build comprehensive datasets. We used functional ultrasound (fUS) imaging concurrently with extracellular recordings of local field potentials (LFP) to reveal brain-wide spatiotemporal hemodynamics of single REMS episodes. We demonstrate for the first time the close association between global hyperemic events – largely outmatching wake levels in most brain regions – and local hippocampal theta (6–10 Hz) and fast gamma (80–110 Hz) events in the CA1 region. In particular, the power of fast gamma oscillations strongly correlated with the amplitude of subsequent vascular events. Our findings challenge our current understanding of neurovascular coupling and question the evolutionary benefit of such energy-demanding patterns in REMS function.Neural activity during REM sleep is similar to the waking state. Here, the authors measure blood volume with neurofunctional ultrasound imaging together with hippocampal neural activity during REM sleep and report that fast gamma oscillations are coupled to a brain-wide upregulation of vascular flow. A spatiotemporal attention-based ResC3D model for large-scale gesture recognition Abnormal gesture recognition has many applications in the fields of visual surveillance, crowd behavior analysis, and sensitive video content detection. However, the recognition of dynamic gestures with large-scale videos remains a challenging task due to the barriers of gesture-irrelevant factors like the variations in illumination, movement path, and background. In this paper, we propose a spatiotemporal attention-based ResC3D model for abnormal gesture recognition with large-scale videos. One key idea is to find a compact and effective representation of the gesture in both spatial and temporal contexts. To eliminate the influence of gesture-irrelevant factors, we first employ the enhancement techniques such as Retinex and hybrid median filer to improve the quality of RGB and depth inputs. Then, we design a spatiotemporal attention scheme to focus on the most valuable cues related to the moving parts for the gesture. Upon these representations, a ResC3D network, which leverages the advantages of both residual network and C3D model, is developed to extract features, together with a canonical correlation analysis-based fusion scheme for blending features from different modalities. The performance of our method is evaluated on the Chalearn IsoGD Dataset. Experiments demonstrate the effectiveness of each module of our method and show the ultimate accuracy reaches 68.14%, which outperforms other state-of-the-art methods, including our basic work in 2017 Chalearn Looking at People Workshop of ICCV. An Automatic Classification Method on Chronic Venous Insufficiency Images Chronic venous insufficiency (CVI) affect a large population, and it cannot heal without doctors’ interventions. However, many patients do not get the medical advisory service in time. At the same time, the doctors also need an assistant tool to classify the patients according to the severity level of CVI. We propose an automatic classification method, named CVI-classifier to help doctors and patients. In this approach, first, low-level image features are mapped into middle-level semantic features by a concept classifier, and a multi-scale semantic model is constructed to form the image representation with rich semantics. Second, a scene classifier is trained using an optimized feature subset calculated by the high-order dependency based feature selection approach, and is used to estimate CVI’s severity. At last, classification accuracy, kappa coefficient, F1-score are used to evaluate classification performance. Experiments on the CVI images from 217 patients’ medical records demonstrated superior performance and efficiency for CVI-classifier, with classification accuracy up to 90.92%, kappa coefficient of 0.8735 and F1score of 0.9006. This method also outperformed doctors’ diagnosis (doctors rely solely on images to make judgments) with accuracy, kappa and F1-score improved by 9.11%, 0.1250 and 0.0955 respectively. Anti-steganalysis for image on convolutional neural networks Nowadays, convolutional neural network (CNN) based steganalysis methods achieved great performance. While those methods are also facing security problems. In this paper, we proposed an attack scheme aiming at CNN based steganalyzer including two different attack methods 1) the LSB-Jstego Gradient Based Attack; 2) LSB-Jstego Evolutionary Algorithms Based Attack. The experiment results show that the attack strategies could achieve 96.02% and 90.25% success ratio separately on the target CNN. The proposed attack scheme is an effective way to fool the CNN based steganalyzer and in addition demonstrates the vulnerability of the neural networks in steganalysis. A neural network approach to chemical and gene/protein entity recognition in patents In biomedical research, patents contain the significant amount of information, and biomedical text mining has received much attention in patents recently. To accelerate the development of biomedical text mining for patents, the BioCreative V.5 challenge organized three tracks, i.e., chemical entity mention recognition (CEMP), gene and protein related object recognition (GPRO) and technical interoperability and performance of annotation servers, to focus on biomedical entity recognition in patents. This paper describes our neural network approach for the CEMP and GPRO tracks. In the approach, a bidirectional long short-term memory with a conditional random field layer is employed to recognize biomedical entities from patents. To improve the performance, we explored the effect of additional features (i.e., part of speech, chunking and named entity recognition features generated by the GENIA tagger) for the neural network model. In the official results, our best runs achieve the highest performances (a precision of 88.32%, a recall of 92.62%, and an F-score of 90.42% in the CEMP track; a precision of 76.65%, a recall of 81.91%, and an F-score of 79.19% in the GPRO track) among all participating teams in both tracks. Highly parallel steered mixture-of-experts rendering at pixel-level for image and light field data A novel image approximation framework called steered mixture-of-experts (SMoE) was recently presented. SMoE has multiple applications in coding, scale-conversion, and general processing of image modalities. In particular, it has strong potential for coding and streaming higher dimensional image modalities that are necessary to leverage full translational and rotational freedom (6 degrees-of-freedom) in virtual reality for camera captured images. In this paper, we analyze the rendering performance of SMoE for 2D images and 4D light fields. Two different GPU implementations that parallelize the SMoE regression step at pixel-level are presented, including experimental evaluations based on rendering performance and quality. In this paper it is shown that on appropriate hardware, an OpenCL implementation can achieve 85 fps and 22 fps for, respectively, 1080p and 4K renderings of large models with more than 100,000 of Gaussian kernels. Training Deep Nets with Progressive Batch Normalization on Multi-GPUs Batch normalization (BN) enables us to train various deep neural networks faster. However, the training accuracy will be significantly influenced with the decrease of input mini-batch size. To increase the model accuracy, a global mean and variance among all the input batch can be used, nevertheless communication across all devices is required in each BN layer, which reduces the training speed greatly. To address this problem, we propose progressive batch normalization, which can achieve a good balance between model accuracy and efficiency in multiple-GPU training. Experimental results show that our algorithm can obtain significant performance improvement over traditional BN without data synchronization across GPUs, achieving up to 18.4% improvement on training DeepLab for semantic segmentation task across 8 GPUs. ParDSL: a domain-specific language framework for supporting deployment of parallel algorithms An important challenge in parallel computing is the mapping of parallel algorithms to parallel computing platforms. This requires several activities such as the analysis of the parallel algorithm, the definition of the logical configuration of the platform and the implementation and deployment of the algorithm to the computing platform. However, in current parallel computing approaches very often only conceptual and idiosyncratic models are used which fall short in supporting the communication and analysis of the design decisions. In this article, we present ParDSL, a domain-specific language framework for providing explicit models to support the activities for mapping parallel algorithms to parallel computing platforms. The language framework includes four coherent set of domain-specific languages each of which focuses on an activity of the mapping process. We use the domain-specific languages for modeling the design as well as for generating the required platform-specific models and the code of the selected parallel algorithm. In addition to the languages, a library is defined to support systematic reuse. We discuss the overall architecture of the language framework, the separate DSLs, the corresponding model transformations and the toolset. The framework is illustrated for four different parallel computing algorithms.
 Hierarchical quantum classifiers Quantum circuits with hierarchical structure have been used to perform binary classification of classical data encoded in a quantum state. We demonstrate that more expressive circuits in the same family achieve better accuracy and can be used to classify highly entangled quantum states, for which there is no known efficient classical method. We compare performance for several different parameterizations on two classical machine learning datasets, Iris and MNIST, and on a synthetic dataset of quantum states. Finally, we demonstrate that performance is robust to noise and deploy an Iris dataset classifier on the ibmqx4 quantum computer.Machine learning: Quantum networks for classical and quantum dataQuantum algorithms with hierarchical tensor network structures may provide an efficient approach to machine learning using quantum computers. Recent theoretical work has indicated that quantum algorithms could have an advantage over classical methods for the linear algebra computations involved in machine learning. At the same time, mathematical structures called tensor networks, with some similarities to neural networks, have been shown to represent quantum states and circuits that can be efficiently evaluated. Edward Grant from University College London and colleagues from the UK and China have shown how quantum algorithms based on two tensor network structures can be used to classify both classical and quantum data. If implemented on a large scale quantum computer, their approach may enable classification of two-dimensional images and entangled quantum data more efficiently than is possible with classical methods. Advanced digital image stabilization using similarity-constrained optimization As many people have portable video devices such as cameras on cell phones and camcorders, image stabilization technique is a crucial and challenging task in computer vision applications, and many image stabilization techniques have been researched over many years. We propose a digital image stabilization method that only uses a software algorithm without additional hardware devices. Furthermore, a novel digital image stabilization method composed of three steps that use similarity-constrained nonlinear optimizer is introduced and applied to many unstabilized videos. First, a feature detection technique called moment-based speeded-up robust features (MSURF) is utilized to obtain the transformation matrix. Second, the k-means clustering algorithm is used to detect and remove some of the outliers that cause residual errors during feature matching. Third, the transformation matrix is optimized using nonlinear optimization algorithms to maintain the similarity of the transformation matrix. The experimental results prove that the proposed algorithm provides accurate image stabilization performance. A massively parallel Eikonal solver on unstructured meshes Algorithms for the numerical solution of the Eikonal equation discretized with tetrahedra are discussed. Several massively parallel algorithms for GPU computing are developed. This includes domain decomposition concepts for tracking the moving wave fronts in sub-domains and over the sub-domain boundaries. Furthermore a low memory footprint implementation of the solver is introduced which reduces the number of arithmetic operations and enables improved memory access schemes. The numerical tests for different meshes originating from the geometry of a human heart document the decreased runtime of the new algorithms. Simulating Developmental Cardiac Morphology in Virtual Reality Using a Deformable Image Registration Approach While virtual reality (VR) has potential in enhancing cardiovascular diagnosis and treatment, prerequisite labor-intensive image segmentation remains an obstacle for seamlessly simulating 4-dimensional (4-D, 3-D + time) imaging data in an immersive, physiological VR environment. We applied deformable image registration (DIR) in conjunction with 3-D reconstruction and VR implementation to recapitulate developmental cardiac contractile function from light-sheet fluorescence microscopy (LSFM). This method addressed inconsistencies that would arise from independent segmentations of time-dependent data, thereby enabling the creation of a VR environment that fluently simulates cardiac morphological changes. By analyzing myocardial deformation at high spatiotemporal resolution, we interfaced quantitative computations with 4-D VR. We demonstrated that our LSFM-captured images, followed by DIR, yielded average dice similarity coefficients of 0.92 ± 0.05 (n = 510) and 0.93 ± 0.06 (n = 240) when compared to ground truth images obtained from Otsu thresholding and manual segmentation, respectively. The resulting VR environment simulates a wide-angle zoomed-in view of motion in live embryonic zebrafish hearts, in which the cardiac chambers are undergoing structural deformation throughout the cardiac cycle. Thus, this technique allows for an interactive micro-scale VR visualization of developmental cardiac morphology to enable high resolution simulation for both basic and clinical science. PFNet: a novel part fusion network for fine-grained visual categorization The existing methods in fine-grained visual categorization focus on integrating multiple deep CNN models or complicated attention mechanism, resulting in increasing cumbersome networks. In addition, most methods rely on part annotations which requires expensive expert guidance. In this paper, without extra annotation, we propose a novel part fusion network (PFNet) to effectively fuse discriminative image parts for classification. More specifically, PFNet consists of a part feature extractor to extract part features and a two-level classification network to utilize part-level and image-level features simultaneously. Part-level features are trained with the weighted part loss, which embeds a weighting mechanism based on different parts’ characteristics. Easy parts, hard parts and background parts are proposed and discriminatively used for classification. Moreover, part-level features are fused to form an image-level feature so as to introduce global supervision and generate final predictions. Experiments on three popular benchmark datasets show that our framework achieves competitive performance compared with the state-of-the-art. Code is available at https://github.com/MichaelLiang12/PFNet-FGVC.