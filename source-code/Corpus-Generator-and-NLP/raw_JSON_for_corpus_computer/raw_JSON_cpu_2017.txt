{"apiKey": "47282fe2ca1e60b3e62fc3351ed3dc56", "apiMessage": "This JSON was provided by Springer Nature", "facets": [{"values": [{"count": "5553", "value": "Computer Science"}, {"count": "2765", "value": "Artificial Intelligence (incl. Robotics)"}, {"count": "2255", "value": "Engineering"}, {"count": "2144", "value": "Computer Communication Networks"}, {"count": "1303", "value": "Information Systems Applications (incl. Internet)"}, {"count": "1135", "value": "Image Processing and Computer Vision"}, {"count": "1052", "value": "Data Mining and Knowledge Discovery"}, {"count": "905", "value": "Algorithm Analysis and Problem Complexity"}, {"count": "885", "value": "Systems and Data Security"}, {"count": "882", "value": "Computational Intelligence"}, {"count": "827", "value": "Software Engineering"}, {"count": "790", "value": "Special Purpose and Application-Based Systems"}, {"count": "707", "value": "Mathematics"}, {"count": "684", "value": "Processor Architectures"}, {"count": "668", "value": "Communications Engineering, Networks"}, {"count": "657", "value": "Programming Languages, Compilers, Interpreters"}, {"count": "584", "value": "Database Management"}, {"count": "566", "value": "Pattern Recognition"}, {"count": "557", "value": "Multimedia Information Systems"}, {"count": "528", "value": "Information Storage and Retrieval"}], "name": "subject"}, {"values": [{"count": "270", "value": "Cloud computing"}, {"count": "106", "value": "Optimization"}, {"count": "99", "value": "Machine learning"}, {"count": "97", "value": "Big data"}, {"count": "97", "value": "Deep learning"}, {"count": "92", "value": "GPU"}, {"count": "82", "value": "Genetic algorithm"}, {"count": "81", "value": "Parallel computing"}, {"count": "79", "value": "Scheduling"}, {"count": "71", "value": "Clustering"}, {"count": "66", "value": "Classification"}, {"count": "66", "value": "MapReduce"}, {"count": "64", "value": "Security"}, {"count": "59", "value": "Energy efficiency"}, {"count": "58", "value": "Data mining"}, {"count": "55", "value": "CUDA"}, {"count": "54", "value": "Particle swarm optimization"}, {"count": "47", "value": "Simulation"}, {"count": "46", "value": "90C30"}, {"count": "46", "value": "Virtualization"}], "name": "keyword"}, {"values": [{"count": "335", "value": "Multimedia Tools and Applications"}, {"count": "198", "value": "Cluster Computing"}, {"count": "161", "value": "The Journal of Supercomputing"}, {"count": "156", "value": "Scientific Reports"}, {"count": "119", "value": "Soft Computing"}, {"count": "113", "value": "Wireless Personal Communications"}, {"count": "89", "value": "BMC Bioinformatics"}, {"count": "84", "value": "Neural Computing and Applications"}, {"count": "69", "value": "Annals of Operations Research"}, {"count": "69", "value": "High Performance Computing"}, {"count": "62", "value": "Journal of Scientific Computing"}, {"count": "59", "value": "International Journal of Parallel Programming"}, {"count": "57", "value": "Journal of Real-Time Image Processing"}, {"count": "56", "value": "Neural Information Processing"}, {"count": "53", "value": "Database Systems for Advanced Applications"}, {"count": "52", "value": "Journal of Global Optimization"}, {"count": "52", "value": "Structural and Multidisciplinary Optimization"}, {"count": "51", "value": "Journal of Signal Processing Systems"}, {"count": "50", "value": "The Visual Computer"}, {"count": "49", "value": "Handbook of Hardware/Software Codesign"}], "name": "pub"}, {"values": [{"count": "10670", "value": "2017"}], "name": "year"}, {"values": [{"count": "2487", "value": "China"}, {"count": "1720", "value": "United States"}, {"count": "972", "value": "Germany"}, {"count": "679", "value": "France"}, {"count": "677", "value": "India"}, {"count": "623", "value": "United Kingdom"}, {"count": "480", "value": "Italy"}, {"count": "408", "value": "Spain"}, {"count": "381", "value": "Japan"}, {"count": "368", "value": "Canada"}, {"count": "343", "value": "Australia"}, {"count": "338", "value": "South Korea"}, {"count": "269", "value": "Iran"}, {"count": "210", "value": "Brazil"}, {"count": "200", "value": "Poland"}, {"count": "200", "value": "Taiwan"}, {"count": "182", "value": "Russia"}, {"count": "180", "value": "Switzerland"}, {"count": "171", "value": "Netherlands"}, {"count": "151", "value": "Austria"}], "name": "country"}, {"values": [{"count": "5654", "value": "Book"}, {"count": "5016", "value": "Journal"}], "name": "type"}], "records": [{"journalid": "521", "abstract": "AbstractThis paper proposes an automatic and simple approach to design a neo-fuzzy neuron for identification purposes. The proposed approach uses the backfitting algorithm to learn multiple univariate additive models, where each additive model is a zero-order T-S fuzzy system which is a function of one input variable, and there is one additive model for each input variable. The multiple zero-order T-S fuzzy models constitute a neo-fuzzy neuron. The structure of the model used in this paper allows to have results with good interpretability and accuracy. To validate and demonstrate the performance and effectiveness of the proposed approach, it is applied on 10 benchmark data sets and compared with the extreme learning machine (ELM), support vector regression (SVR) algorithms, and two algorithms for design neo-fuzzy neuron systems, an adaptive learning algorithm for a neo-fuzzy neuron systems (ALNFN), and a fuzzy Kolmogorov\u2019s network (FKN). A statistical paired t test analysis is also presented to compare the proposed approach with ELM, SVR, ALNFN, and FKN with the aim to see whether the results of the proposed approach are statistically different from ELM, SVR, ALNFN, and FKN. The results indicate that the proposed approach outperforms ELM and FKN in all data sets and outperforms SVR and ALNFN in almost all data sets that they were statistically different in almost all data sets and that in most data sets the number of fuzzy rules selected by cross-validation was small obtaining a model with a small complexity and good interpretability capability.", "coverDate": "", "number": "", "publicationName": "Neural Computing and Applications", "publicationDate": "2017-12-30", "endingPage": "10", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 The Natural Computing Applications Forum", "title": "Neo-fuzzy neuron learning using backfitting algorithm", "identifier": "doi:10.1007/s00521-017-3301-4", "contentType": "Article", "topicalCollection": "", "volume": "", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s00521-017-3301-4", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s00521-017-3301-4", "format": ""}], "articleCategory": "", "issn": "1433-3058", "issuetype": "", "creators": [{"creator": "Mendes, J\u00e9r\u00f4me"}, {"creator": "Souza, Francisco"}, {"creator": "Ara\u00fajo, Rui"}, {"creator": "Rastegar, Saeid"}]}, {"journalid": "10586", "abstract": "AbstractLatest scientific application and simulation are requiring powerful computing resources. To get the sufficient resources, more and more computer experts are constructing high-end supercomputer, cluster computing or cloud computing. Although most corporations are starting to use high performance computing by organizing cluster, they rarely add some extra resources dynamically when computing power is not sufficient. In addition, they are also expensive to use insufficient resources as cloud computing resources. To overcome resource limitations when using cloud computing to conduct large computations, we exploit underutilized personal desktops by organizing a heterogeneous cluster environment. An open-source based scheduler, Son of Grid Engine (SGE) is provided in the integrated environment to manage or control the extra computing resources. Two kinds of groups are classified according to the individuals degree of utilization: a full-time group that provides resources at any time, and a custom-time group that provides resources only at designated times. We developed a resource manager that can turn on/off and schedule the SGE execution daemon; this manager performs as the core engine. The resource running time manager efficiently allocates and controls the members of affiliated computing resources.", "coverDate": "", "number": "", "publicationName": "Cluster Computing", "publicationDate": "2017-12-30", "endingPage": "10", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media, LLC, part of Springer Nature", "title": "The resource running time manager for integrated environment", "identifier": "doi:10.1007/s10586-017-1520-1", "contentType": "Article", "topicalCollection": "", "volume": "", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s10586-017-1520-1", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s10586-017-1520-1", "format": ""}], "articleCategory": "", "issn": "1573-7543", "issuetype": "", "creators": [{"creator": "Jung, Daeyong"}, {"creator": "Jeong, Heeseok"}, {"creator": "Kim, Jaesung"}, {"creator": "Lee, Daewon"}, {"creator": "Kim, Myungil"}]}, {"journalid": "11227", "abstract": "AbstractWith the development of cloud computing, energy consumption has become a major and costly problem in data centers. To improve the energy efficiency of data centers, we analyze the influence factors of energy consumption and discover that reducing the idle servers can effectively cut down the energy consumption of data centers. Then the load demand forecasting algorithm using weighted random forests is proposed. And time factor matching coefficient obtained by considering the day type and the time span is employed to calculate the weights. To enhance the forecasting performance, an error correction strategy is also introduced into the forecasting model. The experimental results show that these strategies further improve the prediction accuracy, and the root-mean-square error is 2.6\u20134.1% lower than other forecasting algorithms. We finally design an adaptive scheduling technology that utilizes short-term prediction of load demand. This technology adaptively adjusts the scale of the data center cluster based on the forecast results. The simulation results indicate that the technology can reduce 12.5% energy consumption while ensuring the service quality.", "coverDate": "", "number": "", "publicationName": "The Journal of Supercomputing", "publicationDate": "2017-12-30", "endingPage": "19", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media, LLC, part of Springer Nature", "title": "An adaption scheduling based on dynamic weighted random forests for load demand forecasting", "identifier": "doi:10.1007/s11227-017-2223-3", "contentType": "Article", "topicalCollection": "", "volume": "", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s11227-017-2223-3", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s11227-017-2223-3", "format": ""}], "articleCategory": "", "issn": "1573-0484", "issuetype": "", "creators": [{"creator": "Chen, Mincheng"}, {"creator": "Yuan, Jingling"}, {"creator": "Liu, Dongling"}, {"creator": "Li, Tao"}]}, {"journalid": "10586", "abstract": "AbstractSpam mail classification has been playing a vital role in recent days due to the uncontrollable growth happening in the electronic media. Literature presents several algorithms for email spam classification based on classification methods. In this paper, we propose a spam classification framework using S-Cuckoo and hybrid kernel based support vector machine (HKSVM). At first, the features are extracted from the e-mails based on the text as well as the image. For the textual features, TF-term frequency is used. For the image dependent features, correrlogram and wavelet moment are taken. The hybrid features have then high dimension so the optimum features are identified with the help of hybrid algorithm, called S-Cuckoo search. Then, the classification is done using proposed classifier HKSVM model which is designed based on the hybrid kernel by blending three different kernel functions and then it is used in the SVM classifier. The additional features provided based on image and the modification of SVM classifier provides significant improvement as compared with existing algorithms. The spam classification performance is measured by db1 (combining bare-ling spam and Spam Archive corpus) and db2 (combining lemm-ling spam and Spam Archive corpus). Experimental results show that the proposed spam classification framework has outperformed by having better accuracy of 97.235% when compared with existing approach which is able to achieve only 94.117%.", "coverDate": "", "number": "", "publicationName": "Cluster Computing", "publicationDate": "2017-12-30", "endingPage": "14", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media, LLC, part of Springer Nature", "title": "Visual and textual features based email spam classification using S-Cuckoo search and hybrid kernel support vector machine", "identifier": "doi:10.1007/s10586-017-1615-8", "contentType": "Article", "topicalCollection": "", "volume": "", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s10586-017-1615-8", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s10586-017-1615-8", "format": ""}], "articleCategory": "", "issn": "1573-7543", "issuetype": "", "creators": [{"creator": "Kumaresan, T."}, {"creator": "Saravanakumar, S."}, {"creator": "Balamurugan, R."}]}, {"journalid": "13040", "abstract": "AbstractBackgroundMatrix factorization is a well established pattern discovery tool that has seen numerous applications in biomedical data analytics, such as gene expression co-clustering, patient stratification, and gene-disease association mining. Matrix factorization learns a latent data model that takes a data matrix and transforms it into a latent feature space enabling generalization, noise removal and feature discovery. However, factorization algorithms are numerically intensive, and hence there is a pressing challenge to scale current algorithms to work with large datasets. Our focus in this paper is matrix tri-factorization, a popular method that is not limited by the assumption of standard matrix factorization about data residing in one latent space. Matrix tri-factorization solves this by inferring a separate latent space for each dimension in a data matrix, and a latent mapping of interactions between the inferred spaces, making the approach particularly suitable for biomedical data mining.ResultsWe developed a block-wise approach for latent factor learning in matrix tri-factorization. The approach partitions a data matrix into disjoint submatrices that are treated independently and fed into a parallel factorization system. An appealing property of the proposed approach is its mathematical equivalence with serial matrix tri-factorization. In a study on large biomedical datasets we show that our approach scales well on multi-processor and multi-GPU architectures. On a four-GPU system we demonstrate that our approach can be more than 100-times faster than its single-processor counterpart.ConclusionsA general approach for scaling non-negative matrix tri-factorization is proposed. The approach is especially useful parallel matrix factorization implemented in a multi-GPU environment. We expect the new approach will be useful in emerging procedures for latent factor analysis, notably for data integration, where many large data matrices need to be collectively factorized.", "coverDate": "", "number": "1", "publicationName": "BioData Mining", "publicationDate": "2017-12-29", "endingPage": "16", "openaccess": "true", "printDate": "", "copyright": "\u00a92017 The Author(s)", "title": "Scalable non-negative matrix tri-factorization", "identifier": "doi:10.1186/s13040-017-0160-6", "contentType": "Article", "topicalCollection": "", "volume": "10", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "BioMed Central", "doi": "10.1186/s13040-017-0160-6", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1186/s13040-017-0160-6", "format": ""}], "articleCategory": "", "issn": "1756-0381", "issuetype": "", "creators": [{"creator": "\u010copar, Andrej"}, {"creator": "\u017eitnik, Marinka"}, {"creator": "Zupan, Bla\u017e"}]}, {"journalid": "10489", "abstract": "AbstractSupport Vector Regression (SVR) solves regression problems based on the concept of Support Vector Machine (SVM). In this paper, we introduce a novel model of SVR in which any training samples containing inputs and outputs are considered the random variables with known or unknown distribution functions. Constraints occurrence have a probability density function which helps to obtain maximum margin and achieve robustness. The optimal hyperplane regression can be obtained by solving a quadratic optimization problem. The proposed method is illustrated by several experiments including artificial data sets and real-world benchmark data sets.", "coverDate": "", "number": "1", "publicationName": "Applied Intelligence", "publicationDate": "2017-12-29", "endingPage": "256", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media New York", "title": "Stochastic support vector regression with probabilistic constraints", "identifier": "doi:10.1007/s10489-017-0964-6", "contentType": "Article", "topicalCollection": "", "volume": "48", "startingPage": "243", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s10489-017-0964-6", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s10489-017-0964-6", "format": ""}], "articleCategory": "", "issn": "1573-7497", "issuetype": "", "creators": [{"creator": "Abaszade, Maryam"}, {"creator": "Effati, Sohrab"}]}, {"journalid": "10652", "abstract": "AbstractWe present a brief review of the recent investigations on gravity currents in horizontal channels with non-rectangular cross-section area (such as triangle, $$\\bigvee $$\u22c1-valley, circle/semi-circle, trapezoid) which occur in nature (e.g., rivers) and constructed environment (tunnels, reservoirs, canals). To be specific, we discuss the propagation of a gravity current (GC) in a horizontal channel along the horizontal coordinate x, with gravity g acting in the $$-z$$-z direction, and y the horizontal\u2013lateral coordinate. The bottom and top of the channel are at $$z=0,H$$z=0,H. The \u201cstandard\u201d problem is concerned with 2D flow in a channel with rectangular (or laterally unbounded) cross-section area (CSA). Recent investigations have successfully extended the standard knowledge to the channels of CSA given by the quite general $$-f_1(z)\\le y \\le f_2(z)$$-f1(z)\u2264y\u2264f2(z) for $$0 \\le z \\le H$$0\u2264z\u2264H. This includes the practical $$\\bigvee $$\u22c1-valley, triangle, circle/semi-circle and trapezoid; these geometries may be in \u201cup\u201d or \u201cdown\u201d setting with respect to gravity, e.g., $$\\bigtriangleup $$\u25b3 and $$\\bigtriangledown $$\u25bd. The major objective of the extended theory is to predict the height of the interface $$z=h(x,t)$$z=h(x,t) and the velocity (averaged over the CSA) u(x,\u00a0t), where t is time; the prediction includes the speed and position of the nose $$u_N(t), x_N(t)$$uN(t),xN(t). We show that the motion is governed by a set of simplified equations, called \u201cmodel,\u201d that provides versatile and insightful solutions and trends. The emphasis in on a high-Reynolds-number current whose motion is dominated by buoyancy\u2013inertia balance; in particular a GC released from a lock, which also contains general effects such as front and internal jumps (shocks), and reflected bore. We discuss two-layer, one-layer, and box models; Boussinesq and non-Boussinesq systems; compositional and particle-driven cases; and the effect of stratification of the ambient fluid. The models are self-contained, and admit realistic initial and boundary conditions. The governing equations are amenable to analytical solutions in some special circumstances. Some salient features of the buoyancy-viscous regime, and the estimate for the length at which transition to this regime takes place, are also presented. Some experimental support to the theory, and open questions for further investigations, are also mentioned. The major conclusions are (1) The CSA geometry has significant influence on the motion of the GC; and (2) The new theory is a useful, very significant, extension of the standard two-dimensional GC problem. The standard current is just a particular case, $$f_{1,2} =$$f1,2= constants, among many other covered by the new theory\n.", "coverDate": "", "number": "1", "publicationName": "Environmental Fluid Mechanics", "publicationDate": "2017-12-29", "endingPage": "333", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media Dordrecht", "title": "Thin-layer models for gravity currents in channels of general cross-section area, a review ", "identifier": "doi:10.1007/s10652-017-9535-y", "contentType": "Article", "topicalCollection": "", "volume": "18", "startingPage": "283", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s10652-017-9535-y", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s10652-017-9535-y", "format": ""}], "articleCategory": "", "issn": "1573-1510", "issuetype": "", "creators": [{"creator": "Ungarish, Marius"}]}, {"journalid": "10489", "abstract": "AbstractThe maximum margin of twin spheres support vector machine (MMTSVM) is an effective method for the imbalanced data classification. However, the hinge loss is used in the MMTSVM and easily leads to sensitivity for the noises and instability for re-sampling. In contrast, the pinball loss is related to the quantile distance and less sensitive to noises. To enhance the performance of MMTSVM, we propose a maximum margin of twin spheres machine with pinball loss (Pin-MMTSM) for the imbalanced data classification in this paper. The Pin-MMTSM finds two homocentric spheres by solving a quadratic programming problem (QPP) and a linear programming problem (LPP). The small sphere captures as many majority samples as possible; and the large sphere pushes out most minority samples by increasing the margin between two homocentric spheres. Moreover, our Pin-MMTSM is equipped with noise insensitivity by employing the pinball loss. Experimental results on eighteen imbalanced datasets indicate that our proposed Pin-MMTSM yields a good generalization performance.", "coverDate": "", "number": "1", "publicationName": "Applied Intelligence", "publicationDate": "2017-12-29", "endingPage": "34", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media New York", "title": "Maximum margin of twin spheres machine with pinball loss for imbalanced data classification", "identifier": "doi:10.1007/s10489-017-0961-9", "contentType": "Article", "topicalCollection": "", "volume": "48", "startingPage": "23", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s10489-017-0961-9", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s10489-017-0961-9", "format": ""}], "articleCategory": "", "issn": "1573-7497", "issuetype": "", "creators": [{"creator": "Xu, Yitian"}, {"creator": "Wang, Qian"}, {"creator": "Pang, Xinying"}, {"creator": "Tian, Ying"}]}, {"journalid": "10489", "abstract": "AbstractIn recent years, the rapid growth of multimedia content makes content-based image retrieval (CBIR) a challenging research problem. The content-based attributes of the image are associated with the position of objects and regions within the image. The addition of image content-based attributes to image retrieval enhances its performance. In the last few years, the bag-of-visual-words (BoVW) based image representation model gained attention and significantly improved the efficiency and effectiveness of CBIR. In BoVW-based image representation model, an image is represented as an order-less histogram of visual words by ignoring the spatial attributes. In this paper, we present a novel image representation based on the weighted average of triangular histograms (WATH) of visual words. The proposed approach adds the image spatial contents to the inverted index of the BoVW model, reduces overfitting problem on larger sizes of the dictionary and semantic gap issues between high-level image semantic and low-level image features. The qualitative and quantitative analysis conducted on three image benchmarks demonstrates the effectiveness of the proposed approach based on WATH.", "coverDate": "", "number": "1", "publicationName": "Applied Intelligence", "publicationDate": "2017-12-29", "endingPage": "181", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media New York", "title": "Content-based image retrieval and semantic automatic image annotation based on the weighted average of triangular histograms using support vector machine", "identifier": "doi:10.1007/s10489-017-0957-5", "contentType": "Article", "topicalCollection": "", "volume": "48", "startingPage": "166", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s10489-017-0957-5", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s10489-017-0957-5", "format": ""}], "articleCategory": "", "issn": "1573-7497", "issuetype": "", "creators": [{"creator": "Mehmood, Zahid"}, {"creator": "Mahmood, Toqeer"}, {"creator": "Javid, Muhammad Arshad"}]}, {"journalid": "13677", "abstract": "AbstractData storage on the cloud is growing every day and many companies, administrations, and individuals are now outsourcing storage of their data to large-scale Cloud Service Providers (CSP). However, because of today\u2019s cloud infrastructure virtualization, data owners cannot easily know the location where their data are stored. Even in case of the establishment of a strong Service-Level Agreement, which includes an initial guarantee regarding data location, the CSP may then move data to another location, like another country, in order to cut storage costs or for any other reasons, including backup mistakes and fraudulent use of data. Data location verification is required due to legal, privacy, and performance constraints. Recently \u201cWhere are my data located in the cloud?\u201d has become a challenge and solutions have been proposed to verify data location, under given assumptions regarding CSP behavior. The objective of this paper is twofold: propose a comprehensive classification of the location verification approaches and discuss their vulnerabilities regarding malicious CSP attacks. Location verification solutions may be Framework-based, Hardware-based or Landmark-based. This paper addresses only landmark-based approaches for their deployment flexibility and low cost.", "coverDate": "", "number": "1", "publicationName": "Journal of Cloud Computing", "publicationDate": "2017-12-29", "endingPage": "20", "openaccess": "true", "printDate": "", "copyright": "\u00a92017 The Author(s)", "title": "Landmark-based data location verification in the cloud: review of approaches and challenges", "identifier": "doi:10.1186/s13677-017-0095-y", "contentType": "Article", "topicalCollection": "", "volume": "6", "startingPage": "1", "genre": "ReviewPaper", "eissn": "", "publisher": "Springer", "doi": "10.1186/s13677-017-0095-y", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1186/s13677-017-0095-y", "format": ""}], "articleCategory": "", "issn": "2192-113X", "issuetype": "", "creators": [{"creator": "Irain, Malik"}, {"creator": "Jorda, Jacques"}, {"creator": "Mammeri, Zoubir"}]}, {"journalid": "10489", "abstract": "AbstractForecasting the future values of a time series is a common research topic and is studied using probabilistic and non-probabilistic methods. For probabilistic methods, the autoregressive integrated moving average and exponential smoothing methods are commonly used, whereas for non-probabilistic methods, artificial neural networks and fuzzy inference systems (FIS) are commonly used. There are numerous FIS methods. While most of these methods are rule-based, there are a few methods that do not require rules, such as the type-1 fuzzy function (T1FF) approach. While it is possible to encounter a method such as an autoregressive (AR) model integrated with a T1FF, no method that includes T1FF and the moving average (MA) model in one algorithm has yet been proposed. The aim of this study is to improve forecasting by taking the disturbance terms into account. The input dataset is organized using the following variables. First, the lagged values of the time series are used for the AR model. Second, a fuzzy c-means clustering algorithm is used to cluster the inputs. Third, for the MA, the residuals of fuzzy functions are used. Hence, AR, MA, and the degree of memberships of the objects are included in the input dataset. Because the objective function is not derivative, particle swarm optimization is preferable for solving it. The results on several datasets show that the proposed method outperforms most of the methods in literature.", "coverDate": "", "number": "1", "publicationName": "Applied Intelligence", "publicationDate": "2017-12-29", "endingPage": "77", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media New York", "title": "Recurrent type-1 fuzzy functions approach for time series forecasting", "identifier": "doi:10.1007/s10489-017-0962-8", "contentType": "Article", "topicalCollection": "", "volume": "48", "startingPage": "68", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s10489-017-0962-8", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s10489-017-0962-8", "format": ""}], "articleCategory": "", "issn": "1573-7497", "issuetype": "", "creators": [{"creator": "Tak, Nihat"}, {"creator": "Evren, Atif A."}, {"creator": "Tez, Mujgan"}, {"creator": "Egrioglu, Erol"}]}, {"journalid": "10489", "abstract": "AbstractDecomposition-based multi-objective evolutionary algorithms have been found to be very promising for many-objective optimization. The recently presented non-dominated sorting genetic algorithm III (NSGA-III) employs the decomposition idea to efficiently promote the population diversity. However, due to the low selection pressure of the Pareto-dominance relation the convergence of NSGA-III could still be improved. For this purpose, an improved NSGA-III algorithm based on niche-elimination operation (we call it NSGA-III-NE) is proposed. In the proposed algorithm, an adaptive penalty distance (APD) function is presented to consider the importance of convergence and diversity in the different stages of the evolutionary process. Moreover, the niche-elimination operation is designed by exploiting the niching technique and the worse-elimination strategy. The niching technique identifies the most crowded subregion, and the worse-elimination strategy finds and further eliminates the worst individual. The proposed NSGA-III-NE is tested on a number of well-known benchmark problems with up to fifteen objectives and shows the competitive performance compared with five state-of-the-art decomposition-based algorithms. Additionally, a vector angle based selection strategy is also proposed for handling irregular Pareto fronts.", "coverDate": "", "number": "1", "publicationName": "Applied Intelligence", "publicationDate": "2017-12-29", "endingPage": "141", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media New York", "title": "A niche-elimination operation based NSGA-III algorithm for many-objective optimization", "identifier": "doi:10.1007/s10489-017-0958-4", "contentType": "Article", "topicalCollection": "", "volume": "48", "startingPage": "118", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s10489-017-0958-4", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s10489-017-0958-4", "format": ""}], "articleCategory": "", "issn": "1573-7497", "issuetype": "", "creators": [{"creator": "Bi, Xiaojun"}, {"creator": "Wang, Chao"}]}, {"journalid": "12053", "abstract": "AbstractThe Sunburn solar computer system is based on the idea of consuming the excess electricity of photovoltaic energy systems for useful computing. In this way, the proposed approach allows us to move a part of computing capacity from dedicated data centers to individual solar energy systems and thereby reduce the energy consumption of the data centers. In principle, the proposed mechanism converts the excess electrical energy into valuable computation. The results of computation or data processing are easier to store and transfer than produced electricity. In small-scale systems, the benefit from storing or selling the excess electricity is small, whereas the benefit from selling the generated data is potentially larger, as we demonstrate in this study. Finally, the technical feasibility of the solution is illustrated by constructing and evaluating a prototype implementation using excess solar energy for distributed BOINC computing.", "coverDate": "", "number": "1", "publicationName": "Energy Efficiency", "publicationDate": "2017-12-28", "endingPage": "119", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media B.V.", "title": "Sunburn\u2014using excess energy of small-scale production for distributed computing", "identifier": "doi:10.1007/s12053-017-9552-1", "contentType": "Article", "topicalCollection": "", "volume": "11", "startingPage": "97", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s12053-017-9552-1", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s12053-017-9552-1", "format": ""}], "articleCategory": "", "issn": "1570-6478", "issuetype": "", "creators": [{"creator": "Nurminen, J. K."}, {"creator": "Niemi, T."}, {"creator": "Strandman, J."}, {"creator": "Ruokosuo, K."}]}, {"journalid": "12053", "abstract": "AbstractIn the push towards greener ICT, there is a need to examine the energy savings possible in the design of network infrastructure and each network component. The focus of this paper is to evaluate the impact of routing algorithms on the power consumption of network routers. In a comparison of the power consumption of a number of routers implementing different routing protocols on the open source NetFPGA board, our results show the Bloom filter-based zFilter forwarding node is the greenest router. The rate control protocol (RCP) router and the precision time protocol (PTP) router consume almost identical power, which is about 1.1\u20131.6% higher than that of the reference router. This higher power consumption of RCP and PTP routers is attributed to the 3.6%\u20134.7% higher power consumption in the 3.3-V power component of the total power consumption, which is due to the higher router design complexity. Furthermore, we examined the impact of scaling down the core operational clock frequency to demonstrate the merits of energy proportional routing in each of the routers. The zFilter, operating at 62.5\u00a0MHz, outperforms the RCP, PTP, and the reference routers operating at 125\u00a0MHz, in terms of both packet loss and throughput. In addition, the zFilter operating at 62.5\u00a0MHz consumes around 10.9\u201311.2% less power than the reference router operating at 125\u00a0MHz. We analyze the architecture of each router to further explain these results.", "coverDate": "", "number": "1", "publicationName": "Energy Efficiency", "publicationDate": "2017-12-28", "endingPage": "201", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media B.V.", "title": "The impact of networking algorithms on the power consumption of routers", "identifier": "doi:10.1007/s12053-017-9558-8", "contentType": "Article", "topicalCollection": "", "volume": "11", "startingPage": "189", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s12053-017-9558-8", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s12053-017-9558-8", "format": ""}], "articleCategory": "", "issn": "1570-6478", "issuetype": "", "creators": [{"creator": "Zheng, Xing"}, {"creator": "Wang, Xiaojun"}, {"creator": "Sun, Lingling"}, {"creator": "Gao, Haijun"}, {"creator": "Ormond, Olga"}]}, {"journalid": "12859", "abstract": "AbstractBackgroundGenotyping-by-sequencing (GBS), a method to identify genetic variants and quickly genotype samples, reduces genome complexity by using restriction enzymes to divide the genome into fragments whose ends are sequenced on short-read sequencing platforms. While cost-effective, this method produces extensive missing data and requires complex bioinformatics analysis. GBS is most commonly used on crop plant genomes, and because crop plants have highly variable ploidy and repeat content, the performance of GBS analysis software can vary by target organism. Here we focus our analysis on soybean, a polyploid crop with a highly duplicated genome, relatively little public GBS data and few dedicated tools.ResultsWe compared the performance of five GBS pipelines using low-coverage Illumina sequence data from three soybean populations. To address issues identified with existing methods, we developed GB-eaSy, a GBS bioinformatics workflow that incorporates widely used genomics tools, parallelization and automation to increase the accuracy and accessibility of GBS data analysis. Compared to other GBS pipelines, GB-eaSy rapidly and accurately identified the greatest number of SNPs, with SNP calls closely concordant with whole-genome sequencing of selected lines. Across all five GBS analysis platforms, SNP calls showed unexpectedly low convergence but generally high accuracy, indicating that the workflows arrived at largely complementary sets of valid SNP calls on the low-coverage data analyzed.ConclusionsWe show that GB-eaSy is approximately as good as, or better than, other leading software solutions in the accuracy, yield and missing data fraction of variant calling, as tested on low-coverage genomic data from soybean. It also performs well relative to other solutions in terms of the run time and disk space required. In addition, GB-eaSy is built from existing open-source, modular software packages that are regularly updated and commonly used, making it straightforward to install and maintain. While GB-eaSy outperformed other individual methods on the datasets analyzed, our findings suggest that a comprehensive approach integrating the results from multiple GBS bioinformatics pipelines may be the optimal strategy to obtain the largest, most highly accurate SNP yield possible from low-coverage polyploid sequence data.", "coverDate": "", "number": "1", "publicationName": "BMC Bioinformatics", "publicationDate": "2017-12-28", "endingPage": "12", "openaccess": "true", "printDate": "", "copyright": "\u00a92017 The Author(s).", "title": "A comparison of genotyping-by-sequencing analysis methods on low-coverage crop datasets shows advantages of a new workflow, GB-eaSy", "identifier": "doi:10.1186/s12859-017-2000-6", "contentType": "Article", "topicalCollection": "", "volume": "18", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "BioMed Central", "doi": "10.1186/s12859-017-2000-6", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1186/s12859-017-2000-6", "format": ""}], "articleCategory": "", "issn": "1471-2105", "issuetype": "", "creators": [{"creator": "Wickland, Daniel P."}, {"creator": "Battu, Gopal"}, {"creator": "Hudson, Karen A."}, {"creator": "Diers, Brian W."}, {"creator": "Hudson, Matthew E."}]}, {"journalid": "12859", "abstract": "AbstractBackgroundRecent breakthroughs in molecular biology and next generation sequencing technologies have led to the expenential growh of the sequence databases. Researchrs use BLAST for processing these sequences. However traditional software parallelization techniques (threads, message passing interface) applied in newer versios of BLAST are not adequate for processing these sequences in timely manner.MethodsA new method for array job parallelization has been developed which offers O(T) theoretical speed-up in comparison to multi-threading and MPI techniques. Here T is the number of array job tasks. (The number of CPUs that will be used to complete the job equals the product of T multiplied by the number of CPUs used by a single task.) The approach is based on segmentation of both input datasets to the BLAST process, combining partial solutions published earlier (Dhanker and Gupta, Int J Comput Sci Inf Technol_5:4818-4820, 2014), (Grant et al., Bioinformatics_18:765-766, 2002), (Mathog, Bioinformatics_19:1865-1866, 2003). It is accordingly referred to as a \u201cdual segmentation\u201d method. In order to implement the new method, the BLAST source code was modified to allow the researcher to pass to the program the number of records (effective number of sequences) in the original database. The team also developed methods to manage and consolidate the large number of partial results that get produced. Dual segmentation allows for massive parallelization, which lifts the scaling ceiling in exciting ways.ResultsBLAST jobs that hitherto failed or slogged inefficiently to completion now finish with speeds that characteristically reduce wallclock time from 27\u00a0days on 40 CPUs to a single day using 4104 tasks, each task utilizing eight CPUs and taking less than 7 minutes to complete.ConclusionsThe massive increase in the number of tasks when running an analysis job with dual segmentation reduces the size, scope and execution time of each task. Besides significant speed of completion, additional benefits include fine-grained checkpointing and increased flexibility of job submission. \u201cTrickling in\u201d a swarm of individual small tasks tempers competition for CPU time in the shared HPC environment, and jobs submitted during quiet periods can complete in extraordinarily short time frames. The smaller task size also allows the use of older and less powerful hardware. The CDRH workhorse cluster was commissioned in 2010, yet its eight-core CPUs with only 24GB RAM work well in 2017 for these dual segmentation jobs. Finally, these techniques are excitingly friendly to budget conscious scientific research organizations where probabilistic algorithms such as BLAST might discourage attempts at greater certainty because single runs represent a major resource drain. If a job that used to take 24\u00a0days can now be completed in less than an hour or on a space available basis (which is the case at CDRH), repeated runs for more exhaustive analyses can be usefully contemplated.", "coverDate": "", "number": "14", "publicationName": "BMC Bioinformatics", "publicationDate": "2017-12-28", "endingPage": "169", "openaccess": "true", "printDate": "", "copyright": "\u00a92017 The Author(s).", "title": "Scaling bioinformatics applications on HPC", "identifier": "doi:10.1186/s12859-017-1902-7", "contentType": "Article", "topicalCollection": "", "volume": "18", "startingPage": "163", "genre": "OriginalPaper", "eissn": "", "publisher": "BioMed Central", "doi": "10.1186/s12859-017-1902-7", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1186/s12859-017-1902-7", "format": ""}], "articleCategory": "", "issn": "1471-2105", "issuetype": "", "creators": [{"creator": "Mikailov, Mike"}, {"creator": "Luo, Fu-Jyh"}, {"creator": "Barkley, Stuart"}, {"creator": "Valleru, Lohit"}, {"creator": "Whitney, Stephen"}, {"creator": "Liu, Zhichao"}, {"creator": "Thakkar, Shraddha"}, {"creator": "Tong, Weida"}, {"creator": "Petrick, Nicholas"}]}, {"journalid": "12859", "abstract": "AbstractBackgroundCancer constitutes a momentous health burden in our society. Critical information on cancer may be hidden in its signaling pathways. However, even though a large amount of money has been spent on cancer research, some critical information on cancer-related signaling pathways still remains elusive. Hence, new works towards a complete understanding of cancer-related signaling pathways will greatly benefit the prevention, diagnosis, and treatment of cancer.ResultsWe propose the node-weighted Steiner tree approach to identify important elements of cancer-related signaling pathways at the level of proteins. This new approach has advantages over previous approaches since it is fast in processing large protein-protein interaction networks. We apply this new approach to identify important elements of two well-known cancer-related signaling pathways: PI3K/Akt and MAPK. First, we generate a node-weighted protein-protein interaction network using protein and signaling pathway data. Second, we modify and use two preprocessing techniques and a state-of-the-art Steiner tree algorithm to identify a subnetwork in the generated network. Third, we propose two new metrics to select important elements from this subnetwork. On a commonly used personal computer, this new approach takes less than 2 s to identify the important elements of PI3K/Akt and MAPK signaling pathways in a large node-weighted protein-protein interaction network with 16,843 vertices and 1,736,922 edges. We further analyze and demonstrate the significance of these identified elements to cancer signal transduction by exploring previously reported experimental evidences.ConclusionsOur node-weighted Steiner tree approach is shown to be both fast and effective to identify important elements of cancer-related signaling pathways. Furthermore, it may provide new perspectives into the identification of signaling pathways for other human diseases.", "coverDate": "", "number": "16", "publicationName": "BMC Bioinformatics", "publicationDate": "2017-12-28", "endingPage": "65", "openaccess": "true", "printDate": "", "copyright": "\u00a92017 The Author(s)", "title": "The node-weighted Steiner tree approach to identify elements of cancer-related signaling pathways", "identifier": "doi:10.1186/s12859-017-1958-4", "contentType": "Article", "topicalCollection": "", "volume": "18", "startingPage": "53", "genre": "OriginalPaper", "eissn": "", "publisher": "BioMed Central", "doi": "10.1186/s12859-017-1958-4", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1186/s12859-017-1958-4", "format": ""}], "articleCategory": "", "issn": "1471-2105", "issuetype": "", "creators": [{"creator": "Sun, Yahui"}, {"creator": "Ma, Chenkai"}, {"creator": "Halgamuge, Saman"}]}, {"journalid": "10586", "abstract": "AbstractTwin support vector machine and many of its variants proposed recently generate two optimal separating hyperplanes by solving two dual constrained quadratic programming problems (QPPs) independently. However, each dual QPP involves a set of dual variables with its size determined by the number of patterns in the other class. This will lead to prohibitively computational complexity when these models are encountered with large-scale datasets. In this paper, we propose an improved twin support vector machine, termed as fixed-point twin support vector machine, in which each dual QPP in the traditional TWSVM and its variants in high dimensional space is converted into a sequence of successive minimization problems of unimodal functions in one dimension which can be solved by using line search methods like the Fibonacci search method or the golden section rule. Numerical experiments on several benchmark datasets including large-scale datasets are performed to verify the validity of our proposed algorithm. Experimental results indicate that the our model gains faster training speed while maintaining comparable classification accuracy.", "coverDate": "", "number": "", "publicationName": "Cluster Computing", "publicationDate": "2017-12-28", "endingPage": "15", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media, LLC, part of Springer Nature", "title": "Fixed-point twin support vector machine", "identifier": "doi:10.1007/s10586-017-1572-2", "contentType": "Article", "topicalCollection": "", "volume": "", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s10586-017-1572-2", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s10586-017-1572-2", "format": ""}], "articleCategory": "", "issn": "1573-7543", "issuetype": "", "creators": [{"creator": "Fang, Jiayan"}, {"creator": "Liu, Qiao"}, {"creator": "Qin, Zhiguang"}]}, {"journalid": "10586", "abstract": "AbstractCloud computing is the new technology offering services to build new application through virtualization. Virtualization improves the usage of resource utilization in cloud environment. Recently research in Task Scheduling problem has received more attention because the customers want to maximize the utilization of resources in a cheaper way. In this paper an enhanced particle swarm optimization (PSO) algorithm for improving the efficiency in the task scheduling has been proposed. A ranging function and tuning function based PSO (RTPSO) based on data locality is introduced in this paper for solving the inertia weight assignment problem in existing PSO algorithm for task scheduling. The large inertia weight and small inertia weight will assist a global search and local search respectively. In addition, we have combined the RTPSO with Bat algorithm (RTPSO-B) to improve the optimization. Cloudsim is used in this paper to simulate the task scheduling in cloud environment. The proposed RTPSO-B based task scheduling is compared with various existing task scheduling algorithms such as GA, ACO, ordinary PSO. Simulation results proved proposed RTPSO-B based task scheduling method reduces makespan, cost and increases the utilization of resources.", "coverDate": "", "number": "", "publicationName": "Cluster Computing", "publicationDate": "2017-12-28", "endingPage": "14", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media, LLC, part of Springer Nature", "title": "Ranging and tuning based particle swarm optimization with bat algorithm for task scheduling in cloud computing", "identifier": "doi:10.1007/s10586-017-1534-8", "contentType": "Article", "topicalCollection": "", "volume": "", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s10586-017-1534-8", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s10586-017-1534-8", "format": ""}], "articleCategory": "", "issn": "1573-7543", "issuetype": "", "creators": [{"creator": "Valarmathi, R."}, {"creator": "Sheela, T."}]}, {"journalid": "12859", "abstract": "AbstractBackgroundPrediction in high dimensional settings is difficult due to the large number of variables relative to the sample size. We demonstrate how auxiliary \u2018co-data\u2019 can be used to improve the performance of a Random Forest in such a setting.ResultsCo-data are incorporated in the Random Forest by replacing the uniform sampling probabilities that are used to draw candidate variables by co-data moderated sampling probabilities. Co-data here are defined as any type information that is available on the variables of the primary data, but does not use its response labels. These moderated sampling probabilities are, inspired by empirical Bayes, learned from the data at hand. We demonstrate the co-data moderated Random Forest (CoRF) with two examples. In the first example we aim to predict the presence of a lymph node metastasis with gene expression data. We demonstrate how a set of external p-values, a gene signature, and the correlation between gene expression and DNA copy number can improve the predictive performance. In the second example we demonstrate how the prediction of cervical (pre-)cancer with methylation data can be improved by including the location of the probe relative to the known CpG islands, the number of CpG sites targeted by a probe, and a set of p-values from a related study.ConclusionThe proposed method is able to utilize auxiliary co-data to improve the performance of a Random Forest.", "coverDate": "", "number": "1", "publicationName": "BMC Bioinformatics", "publicationDate": "2017-12-28", "endingPage": "11", "openaccess": "true", "printDate": "", "copyright": "\u00a92017 The Author(s)", "title": "Improved high-dimensional prediction with Random Forests by the use of co-data", "identifier": "doi:10.1186/s12859-017-1993-1", "contentType": "Article", "topicalCollection": "", "volume": "18", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "BioMed Central", "doi": "10.1186/s12859-017-1993-1", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1186/s12859-017-1993-1", "format": ""}], "articleCategory": "", "issn": "1471-2105", "issuetype": "", "creators": [{"creator": "te Beest, Dennis E."}, {"creator": "Mes, Steven W."}, {"creator": "Wilting, Saskia M."}, {"creator": "Brakenhoff, Ruud H."}, {"creator": "van de Wiel, Mark A."}]}, {"journalid": "13059", "abstract": "AbstractWe introduce a k-mer-based computational protocol, DE-kupl, for capturing local RNA variation in a set of RNA-seq libraries, independently of a reference genome or transcriptome. DE-kupl extracts all k-mers with differential abundance directly from the raw data files. This enables the retrieval of virtually all variation present in an RNA-seq data set. This variation is subsequently assigned to biological events or entities such as differential long non-coding RNAs, splice and polyadenylation variants, introns, repeats, editing or mutation events, and exogenous RNA. Applying DE-kupl to human RNA-seq data sets identified multiple types of novel events, reproducibly across independent RNA-seq experiments.", "coverDate": "", "number": "1", "publicationName": "Genome Biology", "publicationDate": "2017-12-28", "endingPage": "15", "openaccess": "true", "printDate": "", "copyright": "\u00a92017 The Author(s)", "title": "DE-kupl: exhaustive capture of biological variation in RNA-seq data through k-mer decomposition", "identifier": "doi:10.1186/s13059-017-1372-2", "contentType": "Article", "topicalCollection": "", "volume": "18", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "BioMed Central", "doi": "10.1186/s13059-017-1372-2", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1186/s13059-017-1372-2", "format": ""}], "articleCategory": "", "issn": "1474-760X", "issuetype": "", "creators": [{"creator": "Audoux, J\u00e9r\u00f4me"}, {"creator": "Philippe, Nicolas"}, {"creator": "Chikhi, Rayan"}, {"creator": "Salson, Mika\u00ebl"}, {"creator": "Gallopin, M\u00e9lina"}, {"creator": "Gabriel, Marc"}, {"creator": "Le Coz, J\u00e9r\u00e9my"}, {"creator": "Drouineau, Emilie"}, {"creator": "Commes, Th\u00e9r\u00e8se"}, {"creator": "Gautheret, Daniel"}]}, {"journalid": "12859", "abstract": "AbstractBackgroundClustering methods are becoming widely utilized in biomedical research where the volume and complexity of data is rapidly increasing. Unsupervised clustering of patient information can reveal distinct phenotype groups with different underlying mechanism, risk prognosis and treatment response. However, biological datasets are usually characterized by a combination of low sample number and very high dimensionality, something that is not adequately addressed by current algorithms. While the performance of the methods is satisfactory for low dimensional data, increasing number of features results in either deterioration of accuracy or inability to cluster. To tackle these challenges, new methodologies designed specifically for such data are needed.ResultsWe present 2D\u2013EM, a clustering algorithm approach designed for small sample size and high-dimensional datasets. To employ information corresponding to data distribution and facilitate visualization, the sample is folded into its two-dimension (2D) matrix form (or feature matrix). The maximum likelihood estimate is then estimated using a modified expectation-maximization (EM) algorithm. The 2D\u2013EM methodology was benchmarked against several existing clustering methods using 6 medically-relevant transcriptome datasets. The percentage improvement of Rand score and adjusted Rand index compared to the best performing alternative method is up to 21.9% and 155.6%, respectively. To present the general utility of the 2D\u2013EM method we also employed 2 methylome datasets, again showing superior performance relative to established methods.ConclusionsThe 2D\u2013EM algorithm was able to reproduce the groups in transcriptome and methylome data with high accuracy. This build confidence in the methods ability to uncover novel disease subtypes in new datasets. The design of 2D\u2013EM algorithm enables it to handle a diverse set of challenging biomedical dataset and cluster with higher accuracy than established methods. MATLAB implementation of the tool can be freely accessed online (http://www.riken.jp/en/research/labs/ims/med_sci_math or http://www.alok-ai-lab.com/).", "coverDate": "", "number": "16", "publicationName": "BMC Bioinformatics", "publicationDate": "2017-12-28", "endingPage": "209", "openaccess": "true", "printDate": "", "copyright": "\u00a92017 The Author(s).", "title": "2D\u2013EM clustering approach for high-dimensional data through folding feature vectors", "identifier": "doi:10.1186/s12859-017-1970-8", "contentType": "Article", "topicalCollection": "", "volume": "18", "startingPage": "195", "genre": "OriginalPaper", "eissn": "", "publisher": "BioMed Central", "doi": "10.1186/s12859-017-1970-8", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1186/s12859-017-1970-8", "format": ""}], "articleCategory": "", "issn": "1471-2105", "issuetype": "", "creators": [{"creator": "Sharma, Alok"}, {"creator": "Kamola, Piotr J."}, {"creator": "Tsunoda, Tatsuhiko"}]}, {"journalid": "13321", "abstract": "AbstractDeciphering the structural determinants of protein\u2013protein interactions (PPIs) is essential to gain a deep understanding of many important biological functions in the living cells. Computational approaches for the structural modeling of PPIs, such as protein\u2013protein docking, are quite needed to complement existing experimental techniques. The reliability of a protein\u2013protein docking method is dependent on the ability of the scoring function to accurately distinguish the near-native binding structures from a huge number of decoys. In this study, we developed HawkRank, a novel scoring function designed for the sampling stage of protein\u2013protein docking by summing the contributions from several energy terms, including van der Waals potentials, electrostatic potentials and desolvation potentials. First, based on the solvation free energies predicted by the Generalized Born model for ~\u00a0800 proteins, a SASA (solvent accessible surface area)-based solvation model was developed, which can give the aqueous solvation free energies for proteins by summing the contributions of 21 atom types. Then, the van der Waals potentials and electrostatic potentials based on the Amber ff14SB force field were computed. Finally, the HawkRank scoring function was derived by determining the most optimal weights for five energy terms based on the training set. Here, MSR (modified success rate), a novel protein\u2013protein scoring quality index, was used to assess the performance of HawkRank and three other popular protein\u2013protein scoring functions, including ZRANK, FireDock and dDFIRE. The results show that HawkRank outperformed the other three scoring functions according to the total number of hits and MSR. HawkRank is available at http://cadd.zju.edu.cn/programs/hawkrank.", "coverDate": "", "number": "1", "publicationName": "Journal of Cheminformatics", "publicationDate": "2017-12-28", "endingPage": "15", "openaccess": "true", "printDate": "", "copyright": "\u00a92017 The Author(s)", "title": "HawkRank: a new scoring function for protein\u2013protein docking based on weighted energy terms", "identifier": "doi:10.1186/s13321-017-0254-7", "contentType": "Article", "topicalCollection": "", "volume": "9", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1186/s13321-017-0254-7", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1186/s13321-017-0254-7", "format": ""}], "articleCategory": "", "issn": "1758-2946", "issuetype": "", "creators": [{"creator": "Feng, Ting"}, {"creator": "Chen, Fu"}, {"creator": "Kang, Yu"}, {"creator": "Sun, Huiyong"}, {"creator": "Liu, Hui"}, {"creator": "Li, Dan"}, {"creator": "Zhu, Feng"}, {"creator": "Hou, Tingjun"}]}, {"journalid": "146", "abstract": "AbstractMediation is a process in which two parties agree to resolve their dispute by negotiating over alternative solutions presented by a mediator. In order to construct such solutions, the mediator brings more information and knowledge, and, if possible, resources to the negotiation table. In order to do so, the mediator faces the challenge of determining which information is relevant to the current problem, given a vast database of knowledge. The contribution of this paper is the automated mediation machinery to resolve this issue. We define the concept of a Mediation Problem and show how it can be described in Game Description Language (GDL). Furthermore, we present an algorithm that allows the mediator to efficiently determine which information is relevant to the problem and collect this information from the negotiating agents. We show with several experiments that this algorithm is much more efficient than the naive solution that simply takes all available knowledge into account.", "coverDate": "", "number": "", "publicationName": "AI & SOCIETY", "publicationDate": "2017-12-28", "endingPage": "18", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer-Verlag London Ltd., part of Springer Nature", "title": "Using Game Description Language for mediated dispute resolution", "identifier": "doi:10.1007/s00146-017-0790-8", "contentType": "Article", "topicalCollection": "", "volume": "", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s00146-017-0790-8", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s00146-017-0790-8", "format": ""}], "articleCategory": "", "issn": "1435-5655", "issuetype": "", "creators": [{"creator": "Jonge, Dave"}, {"creator": "Trescak, Tomas"}, {"creator": "Sierra, Carles"}, {"creator": "Simoff, Simeon"}, {"creator": "M\u00e1ntaras, Ramon L\u00f3pez"}]}, {"journalid": "40537", "abstract": "AbstractThe huge variety of NoSQL Big Data has tossed a need for new pathways to store, process and analyze it. The quantum of data created is inconceivable along with a mixed breath of unknown veracity and creative visualization. The new trials of frameworks help to find substantial unidentified values from massive data sets. They have added an exceptional dimension to the pre-processing and contextual conversion of the data sets for needful analysis. In addition, handling of ambitious imbalanced data sets has acknowledged an intimation of alarm. Traditional classifiers are unable to discourse the precise need of grouping for such data sets. Over_sampling of the minority classes help to improve the performance. Updated Class Purity Maximization Over_Sampling Technique (UCPMOT) is a rationalized technique proposed to handle imbalanced data sets using exclusive safe-level based synthetic sample creation. It addresses the multi-class problem in alignment to a newly induced method namely lowest versus highest. The projected technique experiments with several data sets from the UCI repository. The underlying bed of mapreduce environment encompasses the distributed processing approach on Apache Hadoop framework. Several classifiers help to authorize the classification results using parameters like F-measure and AUC values. The experimental conclusions quote the dominance of UCPMOT over the benchmarking techniques.", "coverDate": "", "number": "1", "publicationName": "Journal of Big Data", "publicationDate": "2017-12-28", "endingPage": "32", "openaccess": "true", "printDate": "", "copyright": "\u00a92017 The Author(s)", "title": "Improved classification of large imbalanced data sets using rationalized technique: Updated Class Purity Maximization Over_Sampling Technique (UCPMOT)", "identifier": "doi:10.1186/s40537-017-0108-1", "contentType": "Article", "topicalCollection": "", "volume": "4", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1186/s40537-017-0108-1", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1186/s40537-017-0108-1", "format": ""}], "articleCategory": "", "issn": "2196-1115", "issuetype": "", "creators": [{"creator": "Patil, Sachin S."}, {"creator": "Sonavane, Shefali P."}]}, {"journalid": "40411", "abstract": "AbstractBackgroundCombinatorial Interaction Testing (CIT) approaches have drawn attention of the software testing community to generate sets of smaller, efficient, and effective test cases where they have been successful in detecting faults due to the interaction of several input parameters. Recent empirical studies show that greedy algorithms are still competitive for CIT. It is thus interesting to investigate new approaches to address CIT test case generation via greedy solutions and to perform rigorous evaluations within the greedy context.MethodsWe present a new greedy algorithm for unconstrained CIT, T-TupleReallocation (TTR), to generate CIT test suites specifically via the Mixed-value Covering Array (MCA) technique. The main reasoning behind TTR is to generate an MCA M by creating and reallocating t-tuples into this matrix M, considering a variable called goal (\u03b6). We performed two controlled experiments addressing cost-efficiency and only cost. Considering both experiments, we did 3200 executions related to 8 solutions. In the first controlled experiment, we compared versions 1.1 and 1.2 of TTR in order to check whether there is significant difference between both versions of our algorithm. In such experiment, we jointly considered cost (size of test suites) and efficiency (time to generate the test suites) in a multi-objective perspective. In the second controlled experiment we confronted TTR 1.2 with five other greedy algorithms/tools for unconstrained CIT: IPOG-F, jenny, IPO-TConfig, PICT, and ACTS. We performed two different evaluations within this second experiment where in the first one we addressed cost-efficiency (multi-objective) and in the second only cost (single objective).ResultsResults of the first controlled experiment indicate that TTR 1.2 is more adequate than TTR 1.1 especially for higher strengths (5, 6). In the second controlled experiment, TTR 1.2 also presents better performance for higher strengths (5, 6) where only in one case it is not superior (in the comparison with IPOG-F). We can explain this better performance of TTR 1.2 due to the fact that it no longer generates, at the beginning, the matrix of t-tuples but rather the algorithm works on a t-tuple by t-tuple creation and reallocation into M.ConclusionConsidering the metrics we defined in this work and based on both controlled experiments, TTR 1.2 is a better option if we need to consider higher strengths (5, 6). For lower strengths, other solutions, like IPOG-F, may be better alternatives.", "coverDate": "", "number": "1", "publicationName": "Journal of Software Engineering Research and Development", "publicationDate": "2017-12-28", "endingPage": "41", "openaccess": "true", "printDate": "", "copyright": "\u00a92017 The Author(s)", "title": "An algorithm for combinatorial interaction testing: definitions and rigorous evaluations", "identifier": "doi:10.1186/s40411-017-0043-z", "contentType": "Article", "topicalCollection": "", "volume": "5", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1186/s40411-017-0043-z", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1186/s40411-017-0043-z", "format": ""}], "articleCategory": "", "issn": "2195-1721", "issuetype": "", "creators": [{"creator": "Balera, Juliana M."}, {"creator": "Santiago J\u00fanior, Valdivino A. de"}]}, {"journalid": "11277", "abstract": "AbstractBeing the building block of network infrastructure for Vehicular Ad-hoc Networks (VANETs), Roadside Units (RSUs) can facilitate vehicle-to-vehicle communications and bridge communications between vehicles and the Internet. However, RSUs are expensive and will be in serious shortage state for a relative long time when deploying VANETs gradually. Hence, maximizing expected deployment profit with limited number of RSUs in road network systems is of great importance. In this paper, we create a novel powerful RSU Deployment Problem Model (RDPM) consisting of a road-network model and a profit model. The road-network model in RDPM supports complicated road shapes meanwhile taking into consideration of key influential factors such as lane number, popularity. Since that the optimal RSU deployment solution of a RDPM problem is hard to obtain, we proposed a genetic algorithm based method to solve it heuristically. Simulation results confirm that our proposed method outperforms the exiting typical BEH method.\n", "coverDate": "", "number": "1", "publicationName": "Wireless Personal Communications", "publicationDate": "2017-12-27", "endingPage": "663", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media, LLC", "title": "A Novel Problem Model and Solution Scheme for Roadside Unit Deployment Problem in VANETs", "identifier": "doi:10.1007/s11277-017-4888-6", "contentType": "Article", "topicalCollection": "", "volume": "98", "startingPage": "651", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s11277-017-4888-6", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s11277-017-4888-6", "format": ""}], "articleCategory": "", "issn": "1572-834X", "issuetype": "", "creators": [{"creator": "Gao, Zhenguo"}, {"creator": "Chen, Danjie"}, {"creator": "Yao, Nianmin"}, {"creator": "Lu, Zhimao"}, {"creator": "Chen, Bingcai"}]}, {"journalid": "11277", "abstract": "AbstractThe exponential growth of the number of multihomed mobile devices is changing the way how we can connect to the Internet. Our mobile devices are demanding for more network resources, in terms of traffic volume and QoS requirements. Unfortunately, it is very hard for a multihomed device to be simultaneously connected to the network through multiple links. The current work enhances the network access of multihomed devices, agnostically to the deployed access technologies. This enhancement is achieved by using simultaneously all of the mobile devices interfaces, and by routing each individual data flow through the most convenient access technology. The proposed solution is only deployed at the network side and it extends Proxy Mobile IPv6 with flow mobility in a completely transparent way to mobile nodes. In fact, it gives particular attention to the handover mechanisms, by improving the detection and attachment of nodes in the network, with the inclusion of the IEEE 802.21 standard in the solution. This provides the necessary implementation and integration details to extend a network topology with femtocell devices. Each femtocell is equipped with various network interfaces, supporting a diverse set of access technologies. There is also a decision entity that individually manages each data flow according to its QoS requisites. The proposed solution has been developed and extensively tested with a real prototype. Evaluation results evidence that the overhead for using the solution is negligible as compared to the offered advantages such as: the support of flow mobility, the fulfil of VoIP functional requisites, the session continuity in spite of flows mobility, its low overhead, its high scalability, and the complete transparency of the proposed solution to user terminals.", "coverDate": "", "number": "1", "publicationName": "Wireless Personal Communications", "publicationDate": "2017-12-27", "endingPage": "1082", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media, LLC", "title": "PMIPv6 Integrated with MIH for Flow Mobility Management: A Real Testbed with Simultaneous Multi-Access in Heterogeneous Mobile Networks", "identifier": "doi:10.1007/s11277-017-4908-6", "contentType": "Article", "topicalCollection": "", "volume": "98", "startingPage": "1055", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s11277-017-4908-6", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s11277-017-4908-6", "format": ""}], "articleCategory": "", "issn": "1572-834X", "issuetype": "", "creators": [{"creator": "Alves, Hugo"}, {"creator": "Silva, Lu\u00eds Miguel"}, {"creator": "Neto Marinheiro, Rui"}, {"creator": "Moura, Jos\u00e9 Andr\u00e9 R. S."}]}, {"journalid": "11071", "abstract": "AbstractThere is a few number of optimal fourth-order iterative methods for obtaining the multiple roots of nonlinear equations. But, in most of the earlier studies, scholars gave the flexibility in their proposed schemes only at the second step (not at the first step) in order to explore new schemes. Unlike what happens in existing methods, the main aim of this manuscript is to construct a new fourth-order optimal scheme which will give the flexibility to the researchers at both steps as well as faster convergence, smaller residual errors and asymptotic error constants. The construction of the proposed scheme is based on the mid-point formula and weight function approach. From the computational point of view, the stability of the resulting class of iterative methods is studied by means of the conjugacy maps and the analysis of strange fixed points. Their basins of attractions and parameter planes are also given to show their dynamical behavior around the multiple roots. Finally, we consider a real-life problem and a concrete variety of standard test functions for numerical experiments and relevant results are extensively treated to confirm the theoretical development.", "coverDate": "", "number": "1", "publicationName": "Nonlinear Dynamics", "publicationDate": "2017-12-27", "endingPage": "112", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media B.V.", "title": "Multiplicity anomalies of an optimal fourth-order class of iterative methods for solving nonlinear equations", "identifier": "doi:10.1007/s11071-017-3858-6", "contentType": "Article", "topicalCollection": "", "volume": "91", "startingPage": "81", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s11071-017-3858-6", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s11071-017-3858-6", "format": ""}], "articleCategory": "", "issn": "1573-269X", "issuetype": "", "creators": [{"creator": "Behl, Ramandeep"}, {"creator": "Cordero, Alicia"}, {"creator": "Motsa, Sandile S."}, {"creator": "Torregrosa, Juan R."}]}, {"journalid": "40623", "abstract": "AbstractThis paper summarizes the specifications and initial evaluation results of Wire Probe Antenna (WPT) and Electric Field Detector (EFD), the key components for the electric field measurement of the Plasma Wave Experiment (PWE) aboard the Arase (ERG) satellite. WPT consists of two pairs of dipole antennas with ~\u00a031-m tip-to-tip length. Each antenna element has a spherical probe (60\u00a0mm diameter) at each end of the wire (15\u00a0m length). They are extended orthogonally in the spin plane of the spacecraft, which is roughly perpendicular to the Sun and enables to measure the electric field in the frequency range of DC to 10\u00a0MHz. This system is almost identical to the WPT of Plasma Wave Investigation aboard the BepiColombo Mercury Magnetospheric Orbiter, except for the material of the spherical probe (ERG: Al alloy, MMO: Ti alloy). EFD is a part of the EWO (EFD/WFC/OFA) receiver and measures the 2-ch electric field at a sampling rate of 512\u00a0Hz (dynamic range:\u00a0\u00b1\u00a0200\u00a0mV/m) and the 4-ch spacecraft potential at a sampling rate of 128\u00a0Hz (dynamic range:\u00a0\u00b1\u00a0100\u00a0V and\u00a0\u00b1\u00a03\u00a0V/m), with the bias control capability of WPT. The electric field waveform provides (1) fundamental information about the plasma dynamics and accelerations and (2) the characteristics of MHD and ion waves in various magnetospheric statuses with the magnetic field measured by MGF and PWE\u2013MSC. The spacecraft potential provides information on thermal electron plasma variations and structure combined with the electron density obtained from the upper hybrid resonance frequency provided by PWE\u2013HFA. EFD has two data modes. The continuous (medium-mode) data are provided as (1) 2-ch waveforms at 64\u00a0Hz (in apoapsis mode, L\u00a0>\u00a04) or 256\u00a0Hz (in periapsis mode, L\u00a0<\u00a04), (2) 1-ch spectrum within 1\u2013232\u00a0Hz with 1-s resolution, and (3) 4-ch spacecraft potential at 8\u00a0Hz. The burst (high-mode) data are intermittently obtained as (4) 2-ch waveforms at 512\u00a0Hz and (5) 4-ch spacecraft potential at 128\u00a0Hz and downloaded with the WFC-E/B datasets after the selection. This paper also shows the initial evaluation results in the initial observation phase.", "coverDate": "", "number": "1", "publicationName": "Earth, Planets and Space", "publicationDate": "2017-12-27", "endingPage": "18", "openaccess": "true", "printDate": "", "copyright": "\u00a92017 The Author(s)", "title": "Wire Probe Antenna (WPT) and Electric Field Detector (EFD) of Plasma Wave Experiment (PWE) aboard the Arase satellite: specifications and initial evaluation results", "identifier": "doi:10.1186/s40623-017-0760-x", "contentType": "Article", "topicalCollection": "", "volume": "69", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1186/s40623-017-0760-x", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1186/s40623-017-0760-x", "format": ""}], "articleCategory": "", "issn": "1880-5981", "issuetype": "", "creators": [{"creator": "Kasaba, Yasumasa"}, {"creator": "Ishisaka, Keigo"}, {"creator": "Kasahara, Yoshiya"}, {"creator": "Imachi, Tomohiko"}, {"creator": "Yagitani, Satoshi"}, {"creator": "Kojima, Hirotsugu"}, {"creator": "Matsuda, Shoya"}, {"creator": "Shoji, Masafumi"}, {"creator": "Kurita, Satoshi"}, {"creator": "Hori, Tomoaki"}, {"creator": "Shinbori, Atsuki"}, {"creator": "Teramoto, Mariko"}, {"creator": "Miyoshi, Yoshizumi"}, {"creator": "Nakagawa, Tomoko"}, {"creator": "Takahashi, Naoko"}, {"creator": "Nishimura, Yukitoshi"}, {"creator": "Matsuoka, Ayako"}, {"creator": "Kumamoto, Atsushi"}, {"creator": "Tsuchiya, Fuminori"}, {"creator": "Nomura, Reiko"}]}, {"journalid": "40430", "abstract": "AbstractBoundary Integral simulations are very common to study the microhydrodynamics of viscous drops and predict the rheology of emulsions. The standard boundary integral formulation for the velocity field at the drop surface is given by surface integrals of singular Green\u2019s functions in space and depends on geometric quantities of the mesh, as the local mean curvature and the unitary exterior normal vector. Typically, the drops deform and rotate under the flow action, which leads to a great distortion of the computational mesh and, therefore, might produce several numerical errors in the surface velocity calculation. In this work, we investigate the numerical errors in the calculation of the surface velocity in boundary integral simulations of a viscous drop in simple shear flows. Assuming that the flow is incompressible, such that the total volume of the drops must remain constant, we use the net flow rate across the drop surface as a measure of the numerical errors in the simulations. For both small and moderate regimes of drop surface deformation, we show that the net mass flow rate across the drop surface is positive, and strongly depends on the drop-to-basis fluid viscosity ratio, capillary number of the flow, and mesh refinement. These results indicate that the volume of the drops increases continuously during the simulations. To remove the spurious mass flow rate from the simulations, we present a mesh convergence study based on an extrapolation procedure to an almost continuous mesh, so that the results become independent of mesh refinement and recover the theoretical prediction of a null net flow rate and constant drop volume in incompressible flows. The extrapolation method proposed here can be straightforwardly implemented to improve the accuracy of BI simulations used to study drop microhydrodynamics and emulsion rheology. As a matter of example, we applied the method to predict the surface deformation of a high-viscosity emulsion drop, and the result is compared in very good agreement with asymptotic theories available in the related literature.", "coverDate": "", "number": "1", "publicationName": "Journal of the Brazilian Society of Mechanical Sciences and Engineering", "publicationDate": "2017-12-27", "endingPage": "10", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 The Brazilian Society of Mechanical Sciences and Engineering", "title": "On the volume conservation of emulsion drops in boundary integral simulations", "identifier": "doi:10.1007/s40430-017-0924-4", "contentType": "Article", "topicalCollection": "", "volume": "40", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s40430-017-0924-4", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s40430-017-0924-4", "format": ""}], "articleCategory": "", "issn": "1806-3691", "issuetype": "", "creators": [{"creator": "Siqueira, Ivan Rosa de"}, {"creator": "Rebou\u00e7as, Rodrigo Bento"}, {"creator": "Cunha, Lucas Hildebrand Pires da"}, {"creator": "Oliveira, Taygoara Felamingo de"}]}, {"journalid": "11277", "abstract": "AbstractPrivacy-preserving continuous data aggregation in wireless sensor networks has broad application prospects, such as environmental monitoring, health care, etc. However, the existing secure aggregation algorithms focus on snapshot data aggregation, so they are not suitable for continuous data aggregation in view of traffic and energy consumption. We propose a privacy preserving, energy-efficient and scalable continuous data aggregation (PECDA). PECDA takes advantage of secure channels to ensure data privacy to counter dramatic energy consumption caused by heavy encryption/decryption operations. In addition, PECDA filters data and thus greatly reduces traffic based on the temporal correlation of sensory data. Therefore, PECDA significantly reduces energy consumption and prolongs the lifetime of network. Theoretical analysis and experimental results show that PECDA has low communication overhead, energy-efficiency, high safety and scalability.", "coverDate": "", "number": "1", "publicationName": "Wireless Personal Communications", "publicationDate": "2017-12-27", "endingPage": "684", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media, LLC", "title": "Privacy-Preserving and Energy-Efficient Continuous Data Aggregation Algorithm in Wireless Sensor Networks", "identifier": "doi:10.1007/s11277-017-4889-5", "contentType": "Article", "topicalCollection": "", "volume": "98", "startingPage": "665", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s11277-017-4889-5", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s11277-017-4889-5", "format": ""}], "articleCategory": "", "issn": "1572-834X", "issuetype": "", "creators": [{"creator": "Wang, Taochun"}, {"creator": "Qin, Xiaolin"}, {"creator": "Ding, Youwei"}, {"creator": "Liu, Liang"}, {"creator": "Luo, Yonglong"}]}, {"journalid": "11277", "abstract": "AbstractIn this paper, we study the minimum free distance and error performance of turbo encoders with M\u00f6bius interleavers. In order to be capable of estimating the minimum free distance of these interleavers using binary-fixed point (BFP) algorithm, new deterministic interleavers called \u201ctruncated M\u00f6bius interleavers\u201d are defined and constructed. It is shown how the shifted cycles of these interleavers can be related to the cycle structure of the primary M\u00f6bius transformation and its coefficients. By adjusting some parameters, an upper bound on the number of total tested BFPs for the proposed truncated M\u00f6bius interleavers is found. One distinctive property of M\u00f6bius interleavers is that their inverse can also be represented and computed with M\u00f6bius functions. Simulations are conducted to compare the error performance of the proposed truncated M\u00f6bius interleavers with quadratic permutation polynomial (QPP) interleavers whose inverses are also representable by a quadratic equation\u00a0(Ryu and Takeshita in IEEE Trans Inf Theory 52(3):1254\u20131260, 2006). It is finally shown that the truncated M\u00f6bius interleavers can interleave sequences of information bits faster than QPP interleavers.", "coverDate": "", "number": "1", "publicationName": "Wireless Personal Communications", "publicationDate": "2017-12-27", "endingPage": "291", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media, LLC", "title": "Performance of M\u00f6bius Interleavers for Turbo Codes", "identifier": "doi:10.1007/s11277-017-4869-9", "contentType": "Article", "topicalCollection": "", "volume": "98", "startingPage": "271", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s11277-017-4869-9", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s11277-017-4869-9", "format": ""}], "articleCategory": "", "issn": "1572-834X", "issuetype": "", "creators": [{"creator": "Hosseinalipour, Seyyedali"}, {"creator": "Sakzad, Amin"}, {"creator": "Sadeghi, Mohammad-Reza"}]}, {"journalid": "11277", "abstract": "AbstractIn promising application field such as Vehicular Ad hoc Networks, the ability of the driver to exchange video streams smoothly over the network regardless of his position is one of the most incentive features. However, dynamic nature of mobility and current obstacles in urban areas bring considerable challenges to routing. To give gratifying transmission performances, the vehicular networks must ensure Quality of Experience as well as keeping a tolerable Quality of Service (QoS). Furthermore, studying and comparing the efficiency of existing protocols have been challenging since each protocol is appropriate to a specific environment of application. Additionally, the lack in literature of quantitative comparison between the existing video stream requirements-based study, led us in this survey to analyze ten promising routing protocols focusing on communication challenges. Since the protocol chosen in the industrial world depends on certain metrics including video stream requirements, the paper shows also which protocols are suitable for MPEG-4 video quality by raking merits of QoS.", "coverDate": "", "number": "1", "publicationName": "Wireless Personal Communications", "publicationDate": "2017-12-27", "endingPage": "981", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media, LLC", "title": "An Evaluation of Routing Protocols for Vehicular Ad-Hoc Network Considering the Video Stream", "identifier": "doi:10.1007/s11277-017-4903-y", "contentType": "Article", "topicalCollection": "", "volume": "98", "startingPage": "945", "genre": "ReviewPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s11277-017-4903-y", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s11277-017-4903-y", "format": ""}], "articleCategory": "", "issn": "1572-834X", "issuetype": "", "creators": [{"creator": "Zaimi, Imane"}, {"creator": "Houssaini, Zineb Squalli"}, {"creator": "Boushaba, Abdelali"}, {"creator": "Oumsis, Mohammed"}, {"creator": "Aboutajdine, Driss"}]}, {"journalid": "10404", "abstract": "Abstract\nThe flow characteristics of water through the in-line and staggered pin-fin microchannels with length of 25\u00a0mm, width of 2.4\u00a0mm and height of 0.11\u00a0mm were studied experimentally. The flow transition was identified as a sudden increasing slope in both pressure drop versus mass flow rate curve and friction factor versus Reynolds number curve for in-line pin-fin microchannels, but it did not occur for staggered pin-fin microchannels. The effect of pin-fin arrangements on the flow transition was not reported in the previous literature. With the aid of microparticle image velocimetry (Micro-PIV) technology, the streamlines, velocity fields and velocity fluctuation fields of flow through the pin-fin microchannels were captured to explain the flow transition, and the effect of pin-fin arrangements on the flow transition was analyzed for the first time. It was found that at the critical Reynolds number where the flow transition occurred for the in-line pin-fin microchannels, the steady double-vortex wake flow changed to the unsteady vortex-shedding wake flow. The occurrence of vortex shedding caused an obvious change in main stream from straight flow to wavy flow and further induced significant increases of transversal velocity and velocity fluctuations, which induced strong flow disturbance in transversal directions and large additional pressure drop, and finally caused the flow transition in the in-line pin-fin microchannels. For the staggered pin-fin microchannels, the main stream through the pin-fin arrays was found to be already the wavy flow before the vortex shedding. Thus, the transversal velocity and velocity fluctuations induced by the vortex shedding were relatively small, and therefore, the flow transition with an abrupt pressure drop increase was not observed in the staggered pin-fin microchannels.", "coverDate": "", "number": "1", "publicationName": "Microfluidics and Nanofluidics", "publicationDate": "2017-12-27", "endingPage": "13", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer-Verlag GmbH Germany, part of Springer Nature", "title": "Experimental investigation on the flow transition in different pin-fin arranged microchannels", "identifier": "doi:10.1007/s10404-017-2030-4", "contentType": "Article", "topicalCollection": "", "volume": "22", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s10404-017-2030-4", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s10404-017-2030-4", "format": ""}], "articleCategory": "", "issn": "1613-4990", "issuetype": "", "creators": [{"creator": "Xu, Fayao"}, {"creator": "Pan, Zhenhai"}, {"creator": "Wu, Huiying"}]}, {"journalid": "12711", "abstract": "AbstractBackgroundNon-linear Bayesian genomic prediction models such as BayesA/B/C/R involve iteration and mostly Markov chain Monte Carlo (MCMC) algorithms, which are computationally expensive, especially when whole-genome sequence (WGS) data are analyzed. Singular value decomposition (SVD) of the genotype matrix can facilitate genomic prediction in large datasets, and can be used to estimate marker effects and their prediction error variances (PEV) in a computationally efficient manner. Here, we developed, implemented, and evaluated a direct, non-iterative method for the estimation of marker effects for the BayesC genomic prediction model.\nMethodsThe BayesC model assumes a priori that markers have normally distributed effects with probability $$ \\uppi $$\u03c0 and no effect with probability (1\u00a0\u2212\u00a0$$ \\uppi $$\u03c0). Marker effects and their PEV are estimated by using SVD and the posterior probability of the marker having a non-zero effect is calculated. These posterior probabilities are used to obtain marker-specific effect variances, which are subsequently used to approximate BayesC estimates of marker effects in a linear model. A computer simulation study was conducted to compare alternative genomic prediction methods, where a single reference generation was used to estimate marker effects, which were subsequently used for 10 generations of forward prediction, for which accuracies were evaluated.ResultsSVD-based posterior probabilities of markers having non-zero effects were generally lower than MCMC-based posterior probabilities, but for some regions the opposite occurred, resulting in clear signals for QTL-rich regions. The accuracies of breeding values estimated using SVD- and MCMC-based BayesC analyses were similar across the 10 generations of forward prediction. For an intermediate number of generations (2 to 5) of forward prediction, accuracies obtained with the BayesC model tended to be slightly higher than accuracies obtained using the best linear unbiased prediction of SNP effects (SNP-BLUP model). When reducing marker density from WGS data to 30\u00a0K, SNP-BLUP tended to yield the highest accuracies, at least in the short term.ConclusionsBased on SVD of the genotype matrix, we developed a direct method for the calculation of BayesC estimates of marker effects. Although SVD- and MCMC-based marker effects differed slightly, their prediction accuracies were similar. Assuming that the SVD of the marker genotype matrix is already performed for other reasons (e.g. for SNP-BLUP), computation times for the BayesC predictions were comparable to those of SNP-BLUP.", "coverDate": "", "number": "1", "publicationName": "Genetics Selection Evolution", "publicationDate": "2017-12-27", "endingPage": "9", "openaccess": "true", "printDate": "", "copyright": "\u00a92017 The Author(s)", "title": "Variable selection models for genomic selection using whole-genome sequence data and singular value decomposition", "identifier": "doi:10.1186/s12711-017-0369-3", "contentType": "Article", "topicalCollection": "", "volume": "49", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "BioMed Central", "doi": "10.1186/s12711-017-0369-3", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1186/s12711-017-0369-3", "format": ""}], "articleCategory": "", "issn": "1297-9686", "issuetype": "", "creators": [{"creator": "Meuwissen, Theo H. E."}, {"creator": "Indahl, Ulf G."}, {"creator": "\u00d8deg\u00e5rd, J\u00f8rgen"}]}, {"journalid": "11071", "abstract": "AbstractFor the last 3\u00a0decades, the inclusion of chaos for the encryption of multimedia information is considered as a remarkable aspect of nonlinear dynamics. In this article, chaotic system along with substitution box is used for image encryption. Substitution box is constructed by the group action of the projective general linear group over a finite field. On multiple attempts, using the same security key the host image gives a different encrypted image which is the main feature of this scheme. For secure communication, this idea of chaos-based image encryption along with substitution box shows relatively improved results as depicted in simulation and security analysis. The scheme demonstrates resistance against image processing attacks.", "coverDate": "", "number": "1", "publicationName": "Nonlinear Dynamics", "publicationDate": "2017-12-27", "endingPage": "370", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media B.V.", "title": "A novel scheme for image encryption using substitution box and chaotic system", "identifier": "doi:10.1007/s11071-017-3874-6", "contentType": "Article", "topicalCollection": "", "volume": "91", "startingPage": "359", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s11071-017-3874-6", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s11071-017-3874-6", "format": ""}], "articleCategory": "", "issn": "1573-269X", "issuetype": "", "creators": [{"creator": "Ullah, Atta"}, {"creator": "Jamal, Sajjad Shaukat"}, {"creator": "Shah, Tariq"}]}, {"journalid": "11277", "abstract": "AbstractMobile crowd-sensing applications produce useful knowledge of the surrounding environment, which makes our life more predictable. However, these applications often require users to contribute, consciously or unconsciously, location-related data for analysis, which gravely encroaches users\u2019 location privacy. Aggregate processing is a feasible way for preserving user privacy to some extent, and based on the mode, some privacy-preserving schemes have been proposed. However, existing schemes still cannot guarantee users\u2019 location privacy in the scenarios with low density participants. Meanwhile, user accountability also needs to be considered comprehensively to protect the system against malicious users. In this paper, we propose data aggregate statistics schemes with participant density-independent location privacy-protection for mobile crowd-sensing applications. First, we make use of multi-pseudonym mechanism to overcome the vulnerability due to low participant density. Then, to further handle sybil attacks, we propose two schemes based on the Paillier cryptosystem. In the basic scheme, we leverage non-interactive zero-knowledge proof technology to verify users\u2019 sensing data. In the advanced scheme, we present a novel verification framework, which also addresses the problem of user accountability, but at the cost of introducing a new entity. Finally, the theoretical analysis indicates that our scheme achieves the desired properties, and the performance experiments demonstrate that our scheme can achieve a balance among accuracy, privacy-protection and computational overhead.", "coverDate": "", "number": "1", "publicationName": "Wireless Personal Communications", "publicationDate": "2017-12-27", "endingPage": "723", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media, LLC", "title": "Participant Density-Independent Location Privacy Protection for Data Aggregation in Mobile Crowd-Sensing", "identifier": "doi:10.1007/s11277-017-4891-y", "contentType": "Article", "topicalCollection": "", "volume": "98", "startingPage": "699", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s11277-017-4891-y", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s11277-017-4891-y", "format": ""}], "articleCategory": "", "issn": "1572-834X", "issuetype": "", "creators": [{"creator": "Chen, Jianwei"}, {"creator": "Ma, Huadong"}, {"creator": "Zhao, Dong"}, {"creator": "Wei, David S. L."}]}, {"journalid": "11554", "abstract": "AbstractWe present a segmentation software package primarily targeting medical and biological applications, with a high level of visual feedback and several usability enhancements over existing packages. Specifically, we provide a substantially faster GPU implementation of the local Gaussian distribution fitting energy model, which can segment inhomogeneous objects with poorly defined boundaries as often encountered in biomedical images. We also provide interactive brushes to guide the segmentation process in a semiautomated framework.\n The speed of our implementation allows us to visualize the active surface in real time with a built-in ray tracer, where users may halt evolution at any time step to correct implausible segmentation by painting new blocking regions or new seeds. Quantitative and qualitative validation is presented, demonstrating the practical efficacy of our interactive elements for a variety of real-world datasets.", "coverDate": "", "number": "", "publicationName": "Journal of Real-Time Image Processing", "publicationDate": "2017-12-26", "endingPage": "14", "openaccess": "true", "printDate": "", "copyright": "\u00a92018 The Author(s)", "title": "Interactive GPU active contours for segmenting inhomogeneous objects", "identifier": "doi:10.1007/s11554-017-0740-1", "contentType": "Article", "topicalCollection": "", "volume": "", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s11554-017-0740-1", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s11554-017-0740-1", "format": ""}], "articleCategory": "", "issn": "1861-8219", "issuetype": "", "creators": [{"creator": "Willcocks, Chris G."}, {"creator": "Jackson, Philip T. G."}, {"creator": "Nelson, Carl J."}, {"creator": "Nasrulloh, Amar V."}, {"creator": "Obara, Boguslaw"}]}, {"journalid": "12149", "abstract": "AbstractPurposeIn targeted radionuclide therapy (TRT), a prior knowledge of the absorbed dose biodistribution is essential for pre-therapy treatment planning. Previously, we showed that non-rigid organ-by-organ registration in sequential quantitative SPECT images improved dose estimation. This study aims to investigate if sequential CT can further improve TRT dosimetric accuracy.MethodsWe simulated SPECT/CT acquisitions at 1, 12, 24, 72 and 144\u00a0h In-111 Zevalin post-injection using an analytical MEGP projector, modeling attenuation, scatter and collimator-detector response. We later recruited a patient injected with 222\u00a0MBq In-111 DTPAOC imaged at 3 SPECT/CT sessions for clinical evaluations. Four registration schemes were evaluated: whole-body-based registration performed on sequential (1) SPECT (WB-SPECT) or (2) CT (WB-CT) images; organ-based registration applied on organs individually segmented from sequential (3) SPECT (O-SPECT) or (4) CT (O-CT) images. Voxel-by-voxel integration was performed followed by Y-90 voxel-S-kernel convolution. Organ-absorbed doses, iso-dose curves, dose\u2013volume histograms (DVHs) were generated for targeted organs for analysis.Results:In simulation study, organ-absorbed dose errors were (\u2212\u20098.66\u2009\u00b1\u20092.83)%, (\u2212\u20092.51\u2009\u00b1\u20093.69)%, (\u2212\u20099.23\u2009\u00b1\u20093.28)%, (\u2212\u20097.17\u2009\u00b1\u20092.53)% for liver, (\u2212\u200914.81\u2009\u00b1\u20094.91)%, (\u2212\u20093.60\u2009\u00b1\u20094.37)%, (\u2212\u200918.13\u2009\u00b1\u20094.44)%, (\u2212\u200911.34\u2009\u00b1\u20094.22)% for spleen, for O-SPECT, O-CT, WB-SPECT and WB-CT registrations, respectively. For all organs, O-CT showed superior results. Results of iso-dose contour, DVHs were in accordance with the organ-absorbed doses. In clinical studies, the results were also consistent which showed O-CT method deviated the most from the result with no registration.Conclusions:We conclude that if both sequential SPECT/CT scans are available, CT organ-based registration method can more effectively improve the 3D dose estimation. Sequential low-dose CT scans might be considered to be included in the standard TRT protocol.", "coverDate": "", "number": "1", "publicationName": "Annals of Nuclear Medicine", "publicationDate": "2017-12-26", "endingPage": "43", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 The Japanese Society of Nuclear Medicine", "title": "Evaluation of sequential SPECT and CT for targeted radionuclide therapy dosimetry", "identifier": "doi:10.1007/s12149-017-1218-8", "contentType": "Article", "topicalCollection": "", "volume": "32", "startingPage": "34", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s12149-017-1218-8", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s12149-017-1218-8", "format": ""}], "articleCategory": "", "issn": "1864-6433", "issuetype": "", "creators": [{"creator": "Li, Tiantian"}, {"creator": "Wu, Nien-Yun"}, {"creator": "Song, Na"}, {"creator": "Mok, Greta S. P."}]}, {"journalid": "158", "abstract": "AbstractIn this work we present LSEGO, an approach to drive efficient global optimization (EGO), based on LS (least squares) ensemble of metamodels. By means of LS ensemble of metamodels it is possible to estimate the uncertainty of the prediction with any kind of model (not only kriging) and provide an estimate for the expected improvement function. For the problems studied, the proposed LSEGO algorithm has shown to be able to find the global optimum with less number of optimization cycles than required by the classical EGO approach. As more infill points are added per cycle, the faster is the convergence to the global optimum (exploitation) and also the quality improvement of the metamodel in the design space (exploration), specially as the number of variables increases, when the standard single point EGO can be quite slow to reach the optimum. LSEGO has shown to be a feasible option to drive EGO with ensemble of metamodels as well as for constrained problems, and it is not restricted to kriging and to a single infill point per optimization cycle.", "coverDate": "", "number": "1", "publicationName": "Structural and Multidisciplinary Optimization", "publicationDate": "2017-12-26", "endingPage": "159", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer-Verlag GmbH Germany", "title": "Ensemble of metamodels: extensions of the least squares approach to efficient global optimization", "identifier": "doi:10.1007/s00158-017-1745-x", "contentType": "Article", "topicalCollection": "", "volume": "57", "startingPage": "131", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s00158-017-1745-x", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s00158-017-1745-x", "format": ""}], "articleCategory": "", "issn": "1615-1488", "issuetype": "", "creators": [{"creator": "Ferreira, Wallace G."}, {"creator": "Serpa, Alberto L."}]}, {"journalid": "158", "abstract": "AbstractIn this brief note we show that the number of design variables in density-based topology optimization can be phenomenally reduced using discrete cosine transform (DCT), which is one of the most frequently used transforms in digital image compression. Only quite a few nonzero DCT coefficients corresponding to low frequency components are needed to generate optimized topology with high resolution. Through two examples, one for compliance minimization and the other for heat conduction, we show that the density method can be surprisingly efficient than people have thought. Moreover, there is no need to use additional density filter or sensitivity filter since high frequency components are inherently filtered by the DCT-based compression.", "coverDate": "", "number": "1", "publicationName": "Structural and Multidisciplinary Optimization", "publicationDate": "2017-12-26", "endingPage": "467", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer-Verlag GmbH Germany", "title": "Highly efficient density-based topology optimization using DCT-based digital image compression", "identifier": "doi:10.1007/s00158-017-1840-z", "contentType": "Article", "topicalCollection": "", "volume": "57", "startingPage": "463", "genre": "BriefCommunication", "eissn": "", "publisher": "Springer", "doi": "10.1007/s00158-017-1840-z", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s00158-017-1840-z", "format": ""}], "articleCategory": "", "issn": "1615-1488", "issuetype": "", "creators": [{"creator": "Zhou, Pingzhang"}, {"creator": "Du, Jianbin"}, {"creator": "L\u00fc, Zhenhua"}]}, {"journalid": "158", "abstract": "AbstractA vibration isolation system is designed using novel hybrid optimization techniques, where locations of machines, locations of isolators and layout of supporting structure are all taken as design variables. Instead of conventional parametric optimization model, the 0-1 programming model is established to optimize the locations of machines and isolators so that the time-consuming remeshing procedure and the complicated sensitivity analysis with respect to position parameters can be circumvented. The 0-1 sequence for position design variables is treated as binary bits so as to reduce the actual number of design variables to a great extent. This way the 0-1 programming can be solved in a quite efficient manner using a special version of genetic algorithm(GA) that has been published by the authors. The layout of supporting structure is optimized using SIMP based topology optimization method, where the fictitious elemental densities are taken as design variables ranging from 0 to 1. Influence of different design variables is firstly investigated by numerical examples. Then a hybrid multilevel optimization method is proposed and implemented to simultaneously take all design variables into account.", "coverDate": "", "number": "1", "publicationName": "Structural and Multidisciplinary Optimization", "publicationDate": "2017-12-26", "endingPage": "15", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer-Verlag GmbH Germany", "title": "Hybrid optimization of a vibration isolation system considering layout of structure and locations of components", "identifier": "doi:10.1007/s00158-017-1828-8", "contentType": "Article", "topicalCollection": "", "volume": "57", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s00158-017-1828-8", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s00158-017-1828-8", "format": ""}], "articleCategory": "", "issn": "1615-1488", "issuetype": "", "creators": [{"creator": "Zhou, Pingzhang"}, {"creator": "Du, Jianbin"}, {"creator": "L\u00fc, Zhenhua"}]}, {"journalid": "158", "abstract": "AbstractAiming at improving the optimization efficiency, a theoretical optimization method for drawbead is proposed based on plastic flow principles. Essentially different from the existing common optimization methods which are on the basis of mathematics or statistics, this method, which can accurately reflect the relationship between the forming quality and the drawbead restraining force from the perspective of plastic flow theory, is a professional optimization method with higher efficiency. Plastic flow principles are first established to determine the influence degree of the drawbead restraining force to the forming quality. Then an evaluation model of the forming quality near a drawbead segment can be established based on the plastic flow principles to qualify the forming quality near the drawbead segment. Finally, a theoretical optimization method for drawbead is proposed based on the evaluation model, according to which the restraining force of each drawbead segment can be directly adjusted. By using the method, the optimal drawbead scheme in automotive panel forming can be obtained with only 3\u20135 iterations. The efficiency and accuracy of the optimization method are verified by a numerical example of a fender panel.", "coverDate": "", "number": "1", "publicationName": "Structural and Multidisciplinary Optimization", "publicationDate": "2017-12-26", "endingPage": "278", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer-Verlag GmbH Germany", "title": "A theoretical optimization method for drawbead restraining forces in automotive panel forming based on plastic flow principles", "identifier": "doi:10.1007/s00158-017-1752-y", "contentType": "Article", "topicalCollection": "", "volume": "57", "startingPage": "267", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s00158-017-1752-y", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s00158-017-1752-y", "format": ""}], "articleCategory": "", "issn": "1615-1488", "issuetype": "", "creators": [{"creator": "Zhang, Qiuchong"}, {"creator": "Liu, Yuqi"}, {"creator": "Zhang, Zhibing"}]}, {"journalid": "10586", "abstract": "AbstractApache Spark is one of the recently popularized open-source platforms that is increasingly being used for large-scale data analytic applications. However, while performance prediction in such systems is important for efficient job scheduling and optimizing resource allocation, interference among multiple Apache Spark jobs running concurrently in a virtualized environment makes it extremely difficult, which is addressed in this paper. Towards that, first, we develop data-driven analytical models to estimate the effect of interference among multiple Apache Spark jobs on job execution time in virtualized cloud environments. Next, we present the design of an interference aware job scheduling algorithm leveraging the developed analytical framework. We evaluated the accuracy of our models using four real-life applications (e.g., Page rank, K-means, Logistic regression, and Word count) on a 6 node cluster while running up to four jobs concurrently. Our experimental results show that the scheduling algorithm reduces the average execution time of individual jobs and the total execution time significantly, and ranges between 47 and 26% for individual jobs and 2\u201313% for total execution time respectively.", "coverDate": "", "number": "", "publicationName": "Cluster Computing", "publicationDate": "2017-12-23", "endingPage": "15", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media, LLC, part of Springer Nature", "title": "Design and implementation of an analytical framework for interference aware job scheduling on Apache Spark platform", "identifier": "doi:10.1007/s10586-017-1466-3", "contentType": "Article", "topicalCollection": "", "volume": "", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s10586-017-1466-3", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s10586-017-1466-3", "format": ""}], "articleCategory": "", "issn": "1573-7543", "issuetype": "", "creators": [{"creator": "Wang, Kewen"}, {"creator": "Khan, Mohammad Maifi Hasan"}, {"creator": "Nguyen, Nhan"}, {"creator": "Gokhale, Swapna"}]}, {"journalid": "10732", "abstract": "AbstractThe multi-objective open vehicle routing problem (MO-OVRP) is a variant of the classic vehicle routing problem in which routes are not required to return to the depot after completing their service and where more than one objective is optimized. This work is intended to solve a more realistic and general version of the problem by considering three different objective functions. MO-OVRP seeks solutions that minimize the total number of routes, the total travel cost, and the longest route. For this purpose, we present a general variable neighborhood search algorithm to approximate the efficient set. The performance of the proposal is supported by an extensive computational experimentation which includes the comparison with the well-known multi-objective genetic algorithm NSGA-II.", "coverDate": "", "number": "", "publicationName": "Journal of Heuristics", "publicationDate": "2017-12-23", "endingPage": "30", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media, LLC, part of Springer Nature", "title": "A general variable neighborhood search for solving the multi-objective open vehicle routing problem", "identifier": "doi:10.1007/s10732-017-9363-8", "contentType": "Article", "topicalCollection": "", "volume": "", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s10732-017-9363-8", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s10732-017-9363-8", "format": ""}], "articleCategory": "", "issn": "1572-9397", "issuetype": "", "creators": [{"creator": "S\u00e1nchez-Oro, Jes\u00fas"}, {"creator": "L\u00f3pez-S\u00e1nchez, Ana D."}, {"creator": "Colmenar, J. Manuel"}]}, {"journalid": "10586", "abstract": "AbstractGPU\u2019s powerful parallel processing capability has been highly recognized throughout the industry; however, GPU computing environments have not yet been widely used in the field of parallel computing. In this study, we develop a method of parallelization of serial programs for GPU computing. In particular, we propose an approach called PRODA to speedup parallel programs on GPUs through dependency analysis. PRODA provides theoretical underpins of task partitioning in parallel programs running in GPU computing environments. At the heart of PRODA is an analyzer for program workflows as well as data and function dependencies in a GPU program. With the dependency analysis in place, PRODA assigns computing tasks to multiple GPU cores in a way to speedup the performance of parallel program on GPUs. An overarching goal of PRODA is to minimize data communication cost between GPUs and main memory of a host CPU. PRODA achieves this goal by apply deploying two strategies. First, PRODA assigns functions processing the same data to a GPU core. Second, PRODA runs multiple independent functions on separate GPU cores. In doing so, PRODA improves the parallelism of parallel programs. We evaluate the performance of PRODA by running two popular benchmarks (i.e., AES and T26) on an 256-core system, where key length is set to 256 bits. The experimental results show that the speedup ratio of AES governed by PRODA is 5.2. Specifically, PRODA improves the performance of the existing CFM scheme by a factor of 1.39. To measure cost of parallel computing, we test PRODA and the alternative solutions by running AES under the 256-bit key length on 128 cores. The cost of parallel computing in PRODA is 524.8ms, which is 61.2% lower than that of the existing SA solution. The parallel efficiency of PRODA is 2.08, which represents an improvement of the PDM algorithm by a factor of 2.08.", "coverDate": "", "number": "", "publicationName": "Cluster Computing", "publicationDate": "2017-12-22", "endingPage": "16", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media, LLC", "title": "PRODA: improving parallel programs on GPUs through dependency analysis", "identifier": "doi:10.1007/s10586-017-1295-4", "contentType": "Article", "topicalCollection": "", "volume": "", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s10586-017-1295-4", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s10586-017-1295-4", "format": ""}], "articleCategory": "", "issn": "1573-7543", "issuetype": "", "creators": [{"creator": "Wei, Xiong"}, {"creator": "Hu, Ming"}, {"creator": "Peng, Tao"}, {"creator": "Jiang, Minghua"}, {"creator": "Wang, Zhiying"}, {"creator": "Qin, Xiao"}]}, {"journalid": "10470", "abstract": "AbstractIn this work, we show how a distributed sensing network consisting of very low-cost nodes can also be used to locate radio transmitters without prior knowledge of which waveform is used. This information can aid in increasing location awareness among cognitive radios, as well as provide assistance in locating offending transmitters. The low accuracy of the internal clocks of these low-cost receivers as well as the geographical distribution of the nodes result in significant challenges regarding the synchronization of the receivers in order to position the source with adequate accuracy. In this article, we synchronize the nodes to an arbitrary modulated RF signal, after which we calculate estimated time differences of arrival to an unknown transmitter. We describe the implementation as well as give results on measurement accuracy in various scenarios using a prototype network of nodes spread out in the city of Turku, Finland. In the individual distance measurements of receiver pairs, the errors in distances vary between 30 and 900\u00a0m, depending on channel conditions.", "coverDate": "", "number": "", "publicationName": "Analog Integrated Circuits and Signal Processing", "publicationDate": "2017-12-22", "endingPage": "10", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Springer Science+Business Media, LLC, part of Springer Nature", "title": "Synchronization of low-cost distributed spectrum sensing nodes for multilateration-based geolocation", "identifier": "doi:10.1007/s10470-017-1094-0", "contentType": "Article", "topicalCollection": "", "volume": "", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s10470-017-1094-0", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s10470-017-1094-0", "format": ""}], "articleCategory": "", "issn": "1573-1979", "issuetype": "", "creators": [{"creator": "Gr\u00f6nroos, Stefan"}, {"creator": "Nybom, Kristian"}, {"creator": "Bj\u00f6rkqvist, Jerker"}]}, {"journalid": "13007", "abstract": "AbstractBackgroundPlants demonstrate dynamic growth phenotypes that are determined by genetic and environmental factors. Phenotypic analysis of growth features over time is a key approach to understand how plants interact with environmental change as well as respond to different treatments. Although the importance of measuring dynamic growth traits is widely recognised, available open software tools are limited in terms of batch image processing, multiple traits analyses, software usability and cross-referencing results between experiments, making automated phenotypic analysis problematic.ResultsHere, we present Leaf-GP (Growth Phenotypes), an easy-to-use and open software application that can be executed on different computing platforms. To facilitate diverse scientific communities, we provide three software versions, including a graphic user interface (GUI) for personal computer (PC) users, a command-line interface for high-performance computer (HPC) users, and a well-commented interactive Jupyter Notebook (also known as the iPython Notebook) for computational biologists and computer scientists. The software is capable of extracting multiple growth traits automatically from large image datasets. We have utilised it in Arabidopsis thaliana and wheat (Triticum aestivum) growth studies at the Norwich Research Park (NRP, UK). By quantifying a number of growth phenotypes over time, we have identified diverse plant growth patterns between different genotypes under several experimental conditions. As Leaf-GP has been evaluated with noisy image series acquired by different imaging devices (e.g. smartphones and digital cameras) and still produced reliable biological outputs, we therefore believe that our automated analysis workflow and customised computer vision based feature extraction software implementation can facilitate a broader plant research community for their growth and development studies. Furthermore, because we implemented Leaf-GP based on open Python-based computer vision, image analysis and machine learning libraries, we believe that our software not only can contribute to biological research, but also demonstrates how to utilise existing open numeric and scientific libraries (e.g. Scikit-image, OpenCV, SciPy and Scikit-learn) to build sound plant phenomics analytic solutions, in a efficient and effective way.ConclusionsLeaf-GP is a sophisticated software application that provides three approaches to quantify growth phenotypes from large image series. We demonstrate its usefulness and high accuracy based on two biological applications: (1) the quantification of growth traits for Arabidopsis genotypes under two temperature conditions; and (2) measuring wheat growth in the glasshouse over time. The software is easy-to-use and cross-platform, which can be executed on Mac OS, Windows and HPC, with open Python-based scientific libraries preinstalled. Our work presents the advancement of how to integrate computer vision, image analysis, machine learning and software engineering in plant phenomics software implementation. To serve the plant research community, our modulated source code, detailed comments, executables (.exe for Windows; .app for Mac), and experimental results are freely available at https://github.com/Crop-Phenomics-Group/Leaf-GP/releases.", "coverDate": "", "number": "1", "publicationName": "Plant Methods", "publicationDate": "2017-12-22", "endingPage": "17", "openaccess": "true", "printDate": "", "copyright": "\u00a92017 The Author(s)", "title": "Leaf-GP: an open and automated software application for measuring growth phenotypes for arabidopsis and wheat", "identifier": "doi:10.1186/s13007-017-0266-3", "contentType": "Article", "topicalCollection": "", "volume": "13", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "BioMed Central", "doi": "10.1186/s13007-017-0266-3", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1186/s13007-017-0266-3", "format": ""}], "articleCategory": "", "issn": "1746-4811", "issuetype": "", "creators": [{"creator": "Zhou, Ji"}, {"creator": "Applegate, Christopher"}, {"creator": "Alonso, Albor Dobon"}, {"creator": "Reynolds, Daniel"}, {"creator": "Orford, Simon"}, {"creator": "Mackiewicz, Michal"}, {"creator": "Griffiths, Simon"}, {"creator": "Penfield, Steven"}, {"creator": "Pullen, Nick"}]}, {"journalid": "12517", "abstract": "AbstractA systematic approach to numerically simulating an island longwall panel operation is proposed: it aims to investigate the evolution of overlying strata, static stress and displacement response and dynamic load arising from roof fracturing and fault slip. The results show that due to a small gob width (70\u00a0m) on both sides, the evolution height of overlying strata is limited, i.e. the heights of the cave-in zone and fracture zone are 30.98 and 66.91\u00a0m, respectively. The numerical model matches the theoretical analysis and field observations. Dynamic analysis reveals that the envelope of mine tremors confirms the good correlation with the evolution of the fracture zone. As the mining panel is far from the fault, fault slip does not occur; at this time, the dynamic load mainly comes from roof fracturing. When mining activities approach the fault, the calculation of the dynamic response of fault slip is performed over the area where the increase of relative shear displacement during dynamic analysis exceeds 0.05\u00a0m and where the shear stress along the fault decreases. It is shown that during the initial stage of the mining process, and in a square mining panel, fault slip is more likely to occur, leading to strong tremors and rock bursts, which become more notable in the later stages of the mining of the island panel.", "coverDate": "", "number": "24", "publicationName": "Arabian Journal of Geosciences", "publicationDate": "2017-12-22", "endingPage": "22", "openaccess": "false", "printDate": "", "copyright": "\u00a92017 Saudi Society for Geosciences", "title": "Numerical investigation of the evolution of overlying strata and distribution of static and dynamic loads in a deep island coal panel", "identifier": "doi:10.1007/s12517-017-3300-x", "contentType": "Article", "topicalCollection": "", "volume": "10", "startingPage": "1", "genre": "OriginalPaper", "eissn": "", "publisher": "Springer", "doi": "10.1007/s12517-017-3300-x", "onlineDate": "", "url": [{"platform": "", "value": "http://dx.doi.org/10.1007/s12517-017-3300-x", "format": ""}], "articleCategory": "", "issn": "1866-7538", "issuetype": "", "creators": [{"creator": "Zhu, Guang-an"}, {"creator": "Dou, Lin-ming"}, {"creator": "Wang, Chang-bin"}, {"creator": "Li, Jing"}, {"creator": "Cai, Wu"}, {"creator": "Ding, Zi-wei"}]}], "result": [{"recordsDisplayed": "50", "start": "1", "total": "10670", "pageLength": "50"}], "query": "(\"cpu\" AND year:2017)"}