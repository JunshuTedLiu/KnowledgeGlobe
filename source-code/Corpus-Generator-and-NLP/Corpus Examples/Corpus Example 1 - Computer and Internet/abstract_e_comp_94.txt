CNRS agrees to pay up over destruction of computer files  Structure of the human ADP-ribosylation factor 1 complexed with GDP ADP-ribosylation factors (ARFs) are essential and ubiquitous in eukaryotes, being involved in vesicular transport and functioning as an activator of phospholipase D (refs 1,2) and cholera toxin3,4. The functions of ARF proteins in membrane traffic and organelle integrity5,6 are intimately tied to its reversible association with membranes and specific interactions with membrane phospholipids. One common feature of these functions is their regulation by the binding and hydrolysis of GTP. Here we report the three-dimensional structure of full-length human ARF1 (Mr 21,000) in its GDP-bound non-myristoylated form. The presence of a unique amino-terminal α-helix and loop, together with differences in Mg2+ ligation and the existence of a non-crystallographic dimer, set this structure apart from other GTP-binding proteins. These features provide a structural basis for the GTP-dependent modulation of membrane affinity, the lack of intrinsic GTPase activity, and the nature of effector binding surfaces. Computer-administered versus paper-and-pencil surveys and the effect of sample selection Airport patrons answered a self-administered questionnaire regarding their satisfaction with various airport facilities and operations. The questionnaire was administered either by a computer touch-sensitive screen or by a contextually identical paper-and-pencil version. For the latter method, respondents were selected randomly, and for the former, they were either randomly selected or self-selected. The effect of the method of questionnaire administration on the rating scales was very small when the samples were selected at random. However, there were substantial differences in ratings between self-selected and randomly selected respondents: The former gave consistently more negative ratings. These differences are probably due to psychological factors such as motivation to participate. Also, it was found that self-selected persons using the computer were more likely to make comments. The findings of this study are discussed with emphasis on their implications for computer interactive surveys. Depth from defocus: A spatial domain approach A new method named STM is described for determining distance of objects and rapid autofocusing of camera systems. STM uses image defocus information and is based on a new Spatial-Domain Convolution/Deconvolution Transform. The method requires only two images taken with different camera parameters such as lens position, focal length, and aperture diameter. Both images can be arbitrarily blurred and neither of them needs to be a focused image. Therefore STM is very fast in comparison with Depth-from-Focus methods which search for the lens position or focal length of best focus. The method involves simple local operations and can be easily implemented in parallel to obtain the depth-map of a scene. STM has been implemented on an actual camera system named SPARCS. Experiments on the performance of STM and their results on real-world planar objects are presented. The results indicate that the accuracy of STM compares well with Depth-from-Focus methods and is useful in practical applications. The utility of the method is demonstrated for rapid autofocusing of electronic cameras. Validation, verification, and testing techniques throughout the life cycle of a simulation study Life cycle validation, verification, and testing (VV&T) is extremely important for the success of a simulation study. This paper surveys current software VV&T techniques and current simulation model VV&T techniques and describes how they can all be applied throughout the life cycle of a simulation study. The processes and credibility assessment stages of the life cycle are described and the applicability of the VV&T techniques for each stage is stated. A glossary is provided to explicitly define important terms and VV&T techniques. Artificial intelligence in simulation The aims are to clarify some basic concepts on cognizant simulation as well as on the similarity and the relationships of AI and simulation; provide an inventory and a taxonomy of AI-based (cognizant) simulation and AI-assisted simulation (cognizant simulation environments); and to indicate some desirable research directions. Ninth graders' attitudes towards selected uses of technology Technology has become affordable and available for science teachers of all grade levels. This study presents the results of collecting attitudinal data from over 240 ninth grade physical science students prior to the integration of technology into their science curriculum. The data were evaluated utilizing a probabilistic model, which corrects for the nonlinearity of rating scale responses. Results indicate that surveyed students, on average, were supportive of the integration of a wide range of technology into their classroom. Although students were generally positive, they were statistically more supportive of activities that involved the sending of electronic messages to other students when a comparison was made to their attitudes toward the collecting of data with computer probes. The results suggest that physical science teachers may want to initiate the integration of technology into classes by first requiring students to use electronic mail to share data collected without the aid of a computer. This activity could, in turn, be followed by labs that require the collecting of data with computer probes and subsequent sharing of that data with other students through electronic mail. Teacher and technologist beliefs about educational technology This study investigated perceptions of teachers and educational technologists about the following areas: instructional design, cooperative learning, learner control, school reform, computers and media, and implementation of a key practice in each area in their teaching. Subjects were 477 individuals representing four respondent groups: educational technology faculty, educational technology graduate students, K-8 teachers, and 9–12 teachers. Data were collected using a 30-item, five-choice Likert-type questionnaire containing five items per topic area. Significant differences between groups were found on 16 items, with an overall total of 32 significant between-group differences. Nineteen of the 32 differences were between K-8 teachers and either the educational technology faculty or the graduate students. K-8 teachers had significantly more positive perceptions than one or both of the educational technology groups on all five of the cooperative learning items and on three of the five learner control items. The results suggest approaches that educational technologists can use in teaching instructional design courses and in designing instructional programs for the schools. Image sensing model and computer simulation for CCD camera systems A computational model and a computer simulation system are presented for image sensing in a typical CCD camera system. The computational model makes explicit the sequence of transformations that the light incident on the camera system undergoes before being sensed and recorded. The model is based on a precise definition of input to the camera system that decouples the photometric properties of a scene from the geometric properties of the scene. Based on this model, an interactive research software, the Image Defocus Simulator, has been developed. Application of this software in machine vision research and development is described with examples. The Conical Methodology and the evolution of simulation model development Originating with ideas generated in the mid-1970s, the Conical Methodology (CM) is the oldest procedural approach to simulation model development. This evolutionary overview describes the principles underlying the CM, the environment structured according to these principles, and the capabilities for large complex simulation modeling tasks not provided in textbook descriptions. The CM is an object-oriented, hierarchical specification language that iteratively prescribes object attributes in a definitional phase that is topdown, followed by a specification phase that is bottom-up. The intent is to develop successive model representations at various levels of abstraction that can be diagnosed for correctness, completeness, consistency, and other characteristics prior to implementation as an executable program. Related or competitive approaches, throughout the evolutionary period are categorized as emanating from: artificial intelligence, mathematical programming, software engineering, conceptual modeling, systems theory, logic-based theory, or graph theory. Work in each category is briefly described. Algebraic nonlinearity and its applications to cryptography The algebraic nonlinearity of an n-bit boolean function is defined as the degree of the polynomial f(X) ε Z2[x1, x2,..., xn] that represents f. We prove that the average degree of an ANF polynomial for an n-bit function is n+o(1). Further, for a balanced n-bit function, any subfunction obtained by holding less than n-[log n]- 1 bits constant is also expected to be nonaffine. A function is partially linear if f(X) has some indeterminates that only occur in terms bounded by degree 1. Boolean functions which can be mapped to partially linear functions via a linear transformation are said to have a linear structure, and are a potentially weak class of functions for cryptography. We prove that the number of n-bit functions that have a linear structure is asymptotic 
$$\left( {2^n  - 1} \right) \bullet 2^{2n - 1 + 1} $$
. Rectilinear steiner tree heuristics and minimum spanning tree algorithms using geographic nearest neighbors We study the application of the geographic nearest neighbor approach to two problems. The first problem is the construction of an approximately minimum length rectilinear Steiner tree for a set ofn points in the plane. For this problem, we introduce a variation of a subgraph of sizeO(n) used by YaO [31] for constructing minimum spanning trees. Using this subgraph, we improve the running times of the heuristics discussed by Bern [6] fromO(n2logn) toO(n log2n). The second problem is the construction of a rectilinear minimum spanning tree for a set ofn noncrossing line segments in the plane. We present an optimalO(n logn) algorithm for this problem. The rectilinear minimum spanning tree for a set of points can thus be computed optimally without using the Voronoi diagram. This algorithm can also be extended to obtain a rectilinear minimum spanning tree for a set of nonintersecting simple polygons. New results for the minimum weight triangulation problem Given a finite set of points in a plane, a triangulation is a maximal set of nonintersecting line segments connecting the points. The weight of a triangulation is the sum of the Euclidean lengths of its line segments. No polynomial-time algorithm is known to find a triangulation of minimum weight, nor is the minimum weight triangulation problem known to be NP-hard. This paper proposes a new heuristic algorithm that triangulates a set ofn points inO(n3) time and that never produces a triangulation whose weight is greater than that of a greedy triangulation. The algorithm produces an optimal triangulation if the points are the vertices of a convex polygon. Experimental results indicate that this algorithm rarely produces a nonoptimal triangulation and performs much better than a seemingly similar heuristic of Lingas. In the direction of showing the minimum weight triangulation problem is NP-hard, two generalizations that are quite close to the minimum weight triangulation problem are shown to be NP-hard. Integrating qualitative and quantitative shape recovery Recent work in qualitative shape recovery and object recognition has focused on solving the “what is it” problem, while avoiding the “where is it” problem. In contrast, typical CAD-based recognition systems have focused on the “where is it” problem, while assuming they know what the object is. Although each approach addresses an important aspect of the 3-D object recognition problem, each falls short in addressing the complete problem of recognizing and localizing 3-D objects from a large database. In this paper, we first synthesize a new approach to shape recovery for 3-D object recognition that decouples recognition from localization by combining basic elements from these two approaches. Specifically, we use qualitative shape recovery and recognition techniques to provide strong fitting constraints on physics-based deformable model recovery techniques. Secondly, we extend our previously developed technique of fitting deformable models to occluding image contours to the case of image data captured under general orthographic, perspective, and stereo projections. On one hand, integrating qualitative knowledge of the object being fitted to the data, along with knowledge of occlusion supports a much more robust and accurate quantitative fitting. On the other hand, recovering object pose and quantitative surface shape not only provides a richer description for indexing, but supports interaction with the world when object manipulation is required. This paper presents the approach in detail and applies it to real imagery. Safe self-scheduling: A parallel loop scheduling scheme for shared-memory multiprocessors In this papaer was present Safe Self-Scheduling (SSS), a new scheduling scheme that schedules parallel loops with variable length iteration execution times not known at compile time. The scheme assumes a shared memory space. SSS combines static scheduling with dynamic scheduling and draws favorable advantages from each. First, it reduces the dynamic scheduling overhead by statically scheduling a major portion of loop iterations. Second, the workload is balanced with a simple and efficient self-scheduling scheme by applying a new measure, thesmallest critical chore size. Experimental results comparing SSS with other scheduling schemes indicate that SSS surpasses other scheduling schemes. In the experiment on Gauss-Jordan, an application that is suitable for static scheduling schemes, SSS is the only self-scheduling scheme that outperforms the static scheduling scheme. This indicates that SSS achieves a balanced workload with a very small amount of overhead. Automatizing Parametric Reasoning on Distributed Concurrent Systems There can be different views of a concurrent distributed system, depending on who observes it. The final user may just want to know how the system behaves in terms of its possible sequences of actions, while the designer may want to know what are the sequential components of a system or how are they distributed in space. In other words, there is not a widely accepted single semantic model for concurrent systems. This paper describes a parametric verification tool for process description languages. It performs a symbolic execution of processes at different levels of abstraction and verifies deadlock and reachability properties. The tool also provides facilities to check behavioural equivalences. In addition to the classical interleaving semantics, truly concurrent approaches based on the notion of causality and locality are considered. This allows us to compare the expressive power of different models within the same environment. In this respect, we have verified many of the examples given in the literature using our tool. New types of cryptanalytic attacks using related keys In this paper we study the influence of key-scheduling algorithms on the strength of blockciphers. We show that the key-scheduling algorithms of many blockciphers inherit obvious relationships between keys, and use these key relations to attack the blockciphers. Two new types of attacks are described: New chosen plaintext reductions of the complexity of exhaustive search attacks (and the faster variants based on complementation properties), and new low-complexity chosen key attacks. These attacks are independent of the number of rounds of the cryptosystems and of the details of the F-function and may have very small complexities. These attacks show that the key-scheduling algorithm should be carefully designed and that its structure should not be too simple. These attacks are applicable to both variants of LOKI and to Lucifer. DES is not vulnerable to the related keys attacks since the shift pattern in the key-scheduling algorithm is not the same in all the rounds. A quantitative solution to Constraint Satisfaction Problem (CSP) Constraint Satisfaction Problem (CSP) is an important problem in artificial intelligence and operations research. Many practical problems can be formulated as CSP, i.e., finding a consistent value assignment to variables subject to a set of constraints. In this paper, we give a quantitative approach to solve the CSPs which deals uniformly with binary constraints as well as high order,k-ary (k ≥ 2) constraints. In this quantitative approach, using variable transformation and constraint transformation, a CSP is transformed into a satisfiability (SAT) problem. The SAT problem is then solved within a continuous search space. We will evaluate the performance of this method based on randomly generated SAT problem instances and regularly generatedk-ary (k ≥ 2) CSP problem instances. Moded flat GHC and its message-oriented implementation technique Concurrent processes can be used both for programming computation and for programming storage. Previous implementations of Flat GHC, however, have been tuned for computation-intensive programs, and perform poorly for storage-intensive programs (such as programs implementing reconfigurable data structures using processes and streams) and demand-driven programs. This paper proposes an optimization technique for programs in which processes are almost always suspended. The technique compiles unification for data transfer into message passing. Instead of reducing the number of process switching operations, the technique optimizes the cost of each process switching operation and reduces the number ofcons operations for data buffering.The technique is based on a mode system which is powerful enough to analyze bidirectional communication and streams of streams. The mode system is based on mode constraints imposed by individual clauses rather than on global dataflow analysis, enabling separate analysis of program modules. The introduction of a mode system into Flat GHC effectively subsets Flat GHC; the resulting language is calledModed Flat GHC. Moded Flat GHC programs enjoy two important properties under certain conditions: (1) reduction of a goal clause retains the well-modedness of the clause, and (2) when execution terminates, all the variables in an initial goal clause will be bound to ground terms. Practically, the computational complexity of all-at-once mode analysis can be made almost linear with respect to the sizen of the program and the complexity of the data structures used, and the complexity of separate analysis is higher only by O(logn) times. Mode analysis provides useful information for debugging as well.Benchmark results show that the proposed technique well improves the performance of storage-intensive programs and demand-driven programs compared with a conventional native-code implementation. It also improves the performance of some computation-intensive programs. We expect that the proposed technique will expand the application areas of concurrent logic languages. Representation of geometric variations using matrix transforms for statistical tolerance analysis in assemblies The goal of this article is to develop a tolerance representation for assemblies compatible with tolerance analysis based on a closed-form algorithm used in robotic applications. A methodology is described that represents standard Y14.5M-1982 tolerances using homogeneous 4×4 matrix transforms. Transforms represent both the nominal relations between parts and the variations caused by geometric deviations allowed by the tolerances. The analysis calculates a statistical estimate of the location of theNth part in an assembly starting from the first part or a fixture. Except forform tolerances, most types of tolerance specifications are compatible with the proposed representation. This approach is well suited to integration with CAD systems and feature-based design. Since assembly apparatus errors can be calculated using the same methodology, one can predict the relative position and angle errors between two parts about to be mated. This permits useful evaluation of assembly equipment errors, comparison of different product tolerance assignments, and calculations of assembly process capability. An efficient parallel algorithm for geometrically characterising drawings of a class of 3-D objects Labelling the lines of a planar line drawing of a 3-D object in a way that reflects the geometric properties of the object is a much studied problem in computer vision, considered to be an important step towards understanding the object from its 2-D drawing. Combinatorially, the labellability problem is a Constraint Satisfaction Problem and has been shown to be NP-complete even for images of polyhedral scenes. In this paper, we examine scenes that consist of a set of objects each obtained by rotating a polygon around an arbitrary axis. The objects are allowed to arbitrarily intersect or overlay. We show that for these scenes, there is a sequential lineartime labelling algorithm. Moreover, we show that the algorithm has a fast parallel version that executes inO(log3n) time on an Exclusive-Read-Exclusive-Write Parallel Random Access Machine withO(n3/log3n) processors. The algorithm not only answers the decision problem of labellability, but also produces a legal labelling, if there is one. This parallel algorithm should be contrasted with the techniques of dealing with special cases of the constraint satisfaction problem. These techniques employ an effective, but inherently sequential, relaxation procedure in order to restrict the domains of the variables. Effects of orienting activities and practice on achievement, continuing motivation, and student behaviors in a cooperative learning environment The purpose of this study was to investigate the effect of orienting activities and type of practice on achievement, continuing motivation, and student behaviors in a cooperative learning environment. Eighty graduate education majors were assigned to cooperative groups and required to learn instructional design principles from three instructional television lessons. Each lesson included specific orienting activities (advance organizers or objectives) and different types of practice (verbal information or intellectual skills). Results indicated that subjects who worked in groups that received intellectual skills practice performed better on the application portion of the posttest than those who received verbal information practice. Knowledge acquisition and student behaviors were affected by a combination of type of practice and orienting activity. Groups that received intellectual skills practice discussed more content, gave more help to their fellow group members, and exhibited less individual behavior than groups that received verbal information practice. Groups given objectives discussed significantly more content than groups given advance organizers. Knowledge-based facility planning: a review and a framework ledge-based facility planning (KBFP) problem is reviewed. The aim of KBFP is to provide a more comprehensive planning package for users so that their expertise can be augmented with proven knowledge, and yield significantly better plans. The categories reviewed include facilities equipment selection, software model selection, and the generative task of creating a facility planning solution. The employed problem representation and problem-solving techniques are reviewed. Finally, the development of an integrated framework for KBFP is discussed. Parallel simulation today This paper surveys topics that presently define the state of the art in parallel simulation. Included in the tutorial are discussions on new protocols, mathematical performance analysis, time parallelism, hardware support for parallel simulation, load balancing algorithms, and dynamic memory management for optimistic snchronization. Grammar formalisms viewed as evolving algebras We consider the use ofevolving algebra methods of specifying grammars for natural languages. We are especially interested in distributed evolving algebras. We provide the motivation for doing this, and we give a reconstruction of some classic grammar formalisms in directly dynamic terms. Finally, we consider some technical questions arising from the use of direct dynamism in grammar formalisms. External segment trees The segment tree is a well-known internal data structure with numerous applications in computational geometry. It allows the dynamical maintenance of a set of intervals such that the intervals enclosing a query point can be found efficiently (point enclosure search).In this paper we transfer the underlying principle of the segment tree in a nontrivial way to secondary storage and arrive at the EST-an external file structure with the same functionality and the following properties: (1) Point enclosure searches are very efficient—only very few pages are accessed that are not filled to more than 50% with result intervals. (2) A page filling of 50% is guaranteed—on the average it will be around 70%. Although the segment tree represents, in the worst case, each interval by a logarithmic number offragments, in practical cases fragmentation remains low and the storage requirements about linear. (3) The EST is balanced and the update algorithms are efficient. (4) Unlike many other file structures for spatial objects the EST has no problems with an arbitrarydensity, that is, an arbitrarily large number of intervals covering any point of the line.Furthermore, the EST can be used as a file structureconstructor in the following sense: Let there be a file structureX supporting searches for objects with propertyx and suppose it is necessary to maintain a collection of objects with associated (e.g., time) intervals. Then an EST-X structure that supports searches for objects with propertyx present at timet can be built. This suggests using the EST as a building block in the implementation of temporal database systems. Other applications include the one-dimensional indexing of collections of spatial objects in two or more dimensions.More generally, this paper shows techniques for mapping internal tree structures with node lists (other examples: range tree, interval tree) to secondary memory. In this context an intriguing theoretical problem, thecover-balancing problem, is solved: Given a tree whose nodes have associatedweights partitioned into subtrees whose weights must lie in a certain range, maintain this partition under weight changes at arbitrary nodes. This is in contrast to classical balancing problems where updates occur only at the leaves. Visual interactive simulation: A methodological perspective The ideas and methods of Visual Interactive Simulation (VIS) are nearly fifteen years old, yet are still often misunderstood or misrepresented. It is argued that VIS is primarily a methodology, a way of solving problems with simulation modeling, not necessarily a way of building simulation models. Two distinct methodologies are identified, namely active and passive VIS. It is shown that simulation languages and packages tend to implicitly employ a particular methodology, and that each approach requires a different problem-solving life cycle. Present research, and necessary future research, are reviewed from our methodological perspective. Calendar  A cognitive load application in tutoring Research on intelligent tutoring systems has mainly concentrated on how to reduce a cognitive load which a student will bear in learning a domain. This load reduction approach contributes to facilitating his/her learning. However the approach often fails to reinforce the student's comprehension and retention. Another approach to tutoring is to apply a load to him/her purposefully. In this paper, we present a framework for cognitive load application and describe a demonstration system. The framework imposes a load on a student who tries to understand an explanation. The important point toward the load application is to provide the student with an optimal load that does not go beyond his/her capacity for understanding. This requires controlling the student's load by means of explanations. In order to implement such load control, it is necessary to estimate how much load the explanation imposes on his/her understanding process. The load estimate depends on his/her understanding capability since the same explanation imposes a different load according to the capability. Therefore a student model representing his/her capability is required. This paper shows how our system accomplishes a proper load application by generating explanations with the load estimate. Infinite Concurrent Systems — I. The Relationship between Metric and Order Convergence In recent years, Mazurkiewicz trace theory has become increasingly popular as a model of semantics for non-interleaving concurrency, and the theory is here further extended to allow descriptions of infinite behaviours based on alphabets of arbitrary cardinality. We describe an unconventional but nonetheless natural, ordering on trace space, and show that under this ordering, every nonempty set of traces has a greatest lower bound. Consequently, the ordering is consistently complete, i.e. if a set of traces has an upper bound, it has a least such bound. This parallels the result that traces over finite alphabets form a domain.Kwiatkowska has demonstrated an ultrametric, definable on (standard) trace spaces, under which they become compact, and hence complete, topological spaces. For finite alphabets, thus ultrametric essentially agrees with that of Comyn and Dauchet, but for infinite alphabets they differ in behaviour. We investigate the structure of trace space under various topologies, demonstrate the relationship between order convergence and the various metric convergence regimes, and thereby explain apparent behaviourial anomalies. Fast computation of unbiased intensity derivatives in images using separable filters In this paper we prove that all high order non-biased spatial intensity derivative operators in images can be computed using linear combinations of separable filters. The separable filters are the same as those used by Haralick (1984), but different linear combinations are taken. A comparison of the number of operations necessary to compute the derivatives using separable and non-separable filters is made. The conclusion of our analysis is that the optimal way to compute the needed derivatives depends on which derivatives we have to compute, on the size of the window and on the order of expansion. Finally, we discuss the performance of an edge detector using these derivatives for unsmoothed and smoothed step edges. Heuristic sampling on DAGs In theory, many problems in computer applications can be solved by searching through a directed-acyclic graph (DAG). In practice, however, this approach has been hampered by our analytical inability to predict the search cost accurately without actually implementing and executing the program. To overcome this inability, a simple and quick heuristic procedure based on a stratified sampling approach is presented. It generalizes a tree sampling technique already shown to be useful in predicting the performance of tree-searching programs. With the addition of this DAG sampling procedure, we should be able to forecast the complexity and feasibility of alternative tree or DAG searching algorithms so that we may utilize our computational resources more effectively. Design of software for safety critical systems In this paper, we provide an overview of the use of formal methods in the development of safety critical systems and the notion ofsafety in the context. Our attempt would be to draw lessons from the various research efforts that have gone in towards the development of robust/reliable software for safety-critical systems. In the context of India leaping into hi-tech areas, we argue for the need of a thrust in the development of quality software and also discuss the steps to be initiated towards such a goal. Policy driven management for distributed systems Separating management policy from the automated managers which interpret the policy facilitates the dynamic change of behavior of a distributed management system. This permits it to adapt to evolutionary changes in the system being managed and to new application requirements. Changing the behavior of automated managers can be achieved by changing the policy without having to reimplement them—this permits the reuse of the managers in different environments. It is also useful to have a clear specification of the policy applying to human managers in an enterprise. This paper describes the work on policy which has come out of two related ESPRIT funded projects, SysMan and IDSM. Two classes of policy are elaborated—authorization policies define what a manager is permitted to do and obligation policies define what a manager must do. Policies are specified as objects which define a relationship between subjects (managers) and targets (managed objects). Domains are used to group the objects to which a policy applies. Policy objects also have attributes specifying the action to be performed and constraints limiting the applicability of the policy. We show how a number of example policies can be modeled using these objects and briefly mention issues relating to policy hierarchy and conflicts between overlapping policies. Book reviews  Sharing Analysis Based on Type Inference This paper presents a framework for sharing analysis based on type inference. The type system used in this paper is a modified version of Milner’s type system. The soundness of the analysis is defined by the semantics of types extended for sharing information and proved with respect to an operational semantics for a strict language. It is also shown that the information obtained by the analysis can be applied to compile-time garbage collection. A parallel algorithm for surface-based object reconstruction This paper presents a parallel algorithm that approximates the surface of an object from a collection of its planar contours. Such a reconstruction has wide applications in such diverse fields as biological research, medical diagnosis and therapy, architecture, automobile and ship design, and solid modeling. The surface reconstruction problem is transformed into the problem of finding a minimum-cost acceptable path on a toroidal grid graph, where each horizontal and each vertical edge have the same orientation. An acceptable path is closed path that makes a complete horizontal and vertical circuit. We exploit the structure of this graph to develop efficient parallel algorithms for a message-passing computer. Givenp processors and anm byn toroidal graph, our algorithm will find the minimum cost acceptable path inO(mn log(m)/p) steps, ifp =O(mn/((m + n) log(mn/(m + n)))), which is an optimal speedup. We also show that the algorithm will sendO(p2(m + n)) messages. The algorithm has a linear topology, so it is easy to embed the algorithm in common multiprocessor architectures. Assessing the effect on high school teachers of direct and unrestricted access to the internet: A case study of an east central florida high school The purpose of this study was to investigate the effect direct and unrestricted access to the Internet had on a group of high school teachers. Based on the naturalistic inquiry paradigm, this study explored the barriers these teachers encountered when using the Internet, how and when they elected to use the Internet, the factors that influenced their continued use of the Internet, and the transitions they experienced from using the Internet. Data collection was based on Patton's (1990) three approaches to interviewing; data analysis was based in part on Miles and Huberman's (1984) model of data reduction and display and on Spradley's (1979) task of domain analysis. Findings suggested: that teachers require ongoing Internet training, technical support, home Internet access, and time in which to learn and incorporate the Internet into their classes; that Internet use can increase teachers' self-esteem and improve their attitudes toward computers and education; and that use of the Internet by teachers encourages them to restructure their classes and schedules to accommodate Internet resources within their classrooms. Vector-quantization-based video codec for software-only playback on personal computers This paper discusses a video compression and decompression method based on vector quantization (VQ) for use on general purpose computer systems without specialized hardware. After describing basic VQ coding, we survey common VQ variations and discuss their impediments in light of the target application. We discuss how the proposed video codec was designed to reduce computational complexity in every principal task of the video codec process. We propose a classified VQ scheme that satisfies the data rate, image quality, decoding speed, and encoding speed objectives for software-only video playback. The functional components of the proposed VQ method are covered in detail. The method employs a pseudo-YUV color space and criteria to detect temporal redundancy and low spatial frequency regions. A treestructured-codebook generation algorithm is proposed to reduce encoding execution time while preserving image quality. Two separate vector codebooks, each generated with the treestructured search, are employed for detail and low spatial frequency blocks. Codebook updating and sharing are proposed to further improve encoder speed and compression. A computerised technique for morphometry and 3D reconstruction of embryological structures n the last decade we have witnessed the development of software technology capable of image analysis and morphometry [1, 8–12]. Although these methods are sometimes difficult to master in a practical sense, they are tremendously efficient and precise when applied to the study and measurement of developing biological structures, particularly in the field of embryology. In this study we describe the application on human embryos of an image analyzing system that enables one to perform quantitative analyses of the morphology and size of developing organs and structures as well as their ultimate three-dimensional reconstruction (3DR).RésuméAu cours de cette dernière décennie nous avons assisté au développement de logiciels adaptés à l'analyse de l'image et à la morphométrie. Bien que ces techniques soient parfois difficiles à maîtriser, elles sont particulièrement efficaces et précises dans l'étude du développement des structures biologiques, et particulièrement dans le domaine de l'embryologie. Dans ce travail, nous rapportons l'application aux embryons humains d'un système d'analyse d'image, qui autorise aussi bien une analyse morphologique quantitative des organes et des structures en développement, que leur reconstruction tridimensionnelle ultérieure. Implementing software metrics — the critical success factors This paper reviews the current state of industry with regard to the introduction of software metrics. It discusses the benefits that organizations have derived from metrication, it looks at why organizations have sought to introduce metrics programmes, how they have gone about introducing those programmes and the problems that they have encountered during implementation. The review found that, on the whole, only the sanitized aspects of metrics experiences have actually been published. This seems to be especially the case with respect to practitioner resistance. Very few organizations admit to having encountered resistance during the introduction of a metrics programme. The paper also includes the results of a pilot study, conducted by the first author, which examines the attitudes that developers hold towards the introduction of software metrics. The key findings of this pilot study are that positive attitudes to metrics correlate highly with levels of education and to job satisfaction. Important announcement  The bigger picture: Video pulls together bit and byte interests  Multiple-type, two-dimensional bin packing problems: Applications and algorithms In this paper we consider a class of bin selection and packing problems (BPP) in which potential bins are of various types, have two resource constraints, and the resource requirement for each object differs for each bin type. The problem is to select bins and assign the objects to bins so as to minimize the sum of bin costs while meeting the two resource constraints. This problem represents an extension of the classical two-dimensional BPP in which bins are homogeneous. Typical applications of this research include computer storage device selection with file assignment, robot selection with work station assignment, and computer processor selection with task assignment. Three solution algorithms have been developed and tested: a simple greedy heuristic, a method based onsimulated annealing (SA) and an exact algorithm based onColumn Generation with Branch and Bound (CG). An LP-based method for generating tight lower bounds was also developed (LB). Several hundred test problems based on computer storage device selection and file assignment were generated and solved. The heuristic solved problems up to 100 objects in less than a second; average solution value was within about 3% of the optimum. SA improved solutions to an average gap of less than 1% but a significant increase in computing time. LB produced average lower bounds within 3% of optimum within a few seconds. CG is practical for small to moderately-sized problems — possibly as many as 50 objects. Derivation of algorithms by introduction of generation functions A specification is a mathematical description of the intent we want a program to behave, while an algorithm is a formal description of a program that satisfies the specification. When we want to write an algorithm by defining data domains and functions over these domains, it is common to use recursion equations for function definitions. However, recursion equations lack imaginability such as existential quantifiers in predicate calculus. In this paper we show how executable algorithms are derived from specifications written in ordinary set-theoretic formulas, and illustrate it by deriving several kinds of sorting algorithms as well as a graph algorithm. Modelling the student in Pitagora 2.0 With the aim to individualise human-computer interaction, an Intelligent Tutoring System (ITS) has to keep track of what and how the student has learned. Hence, it is necessary to maintain a Student Model (SM) dealing with complex knowledge representation, such as incomplete and inconsistent knowledge and belief revision. With this in view, the main objective of this paper is to present and discuss the student modelling approach we have adopted to implement Pitagora 2.0, an ITS based on a co-operative learning model, and designed to support teaching-learning activities in a Euclidean Geometry context. In particular, this approach has led us to develop two distinct modules that cooperate to implement the SM of Pitagora 2.0. The first module resembles a “classical” student model, in the sense that it maintains a representation of the current student knowledge level, which can be used by the teacher in order to tune its teaching strategies to the specific student needs. In addition, our system contains a second module that implements a virtual partner, called companion. This module consists of a computational model of an “average student” which cooperates with the student during the learning process. The above mentioned module calls for the use of machine learning algorithms that allow the companion to improve in parallel with the real student. Computational results obtained when testing this module in simulation experiments are also presented. Polynomial algorithms for linear programming over the algebraic numbers We derive a bound on the computational complexity of linear programs whose coefficients are real algebraic numbers. Key to this result is a notion of problem size that is analogous in function to the binary size of a rational-number problem. We also view the coefficients of a linear program as members of a finite algebraic extension of the rational numbers. The degree of this extension is an upper bound on the degree of any algebraic number that can occur during the course of the algorithm, and in this sense can be viewed as a supplementary measure of problem dimension. Working under an arithmetic model of computation, and making use of a tool for obtaining upper and lower bounds on polynomial functions of algebraic numbers, we derive an algorithm based on the ellipsoid method that runs in time bounded by a polynomial in the dimension, degree, and size of the linear program. Similar results hold under a rational number model of computation, given a suitable binary encoding of the problem input. Stability, Sequentiality and Demand Driven Evaluation in Dataflow We show that a given dataflow language l has the property that for any program P and any demand for outputs D (which can be satisfied) there exists a least partial computation of P which satisfies D, iff all the operators of l are stable. This minimal computation is the demand-driven evaluation of P. We also argue that in order to actually implement this mode of evaluation, the operators of l should be further restricted to be effectively sequential ones. Report on the computer software contest at the Forty-First Congress of the Japan Society of Anesthesiology  A system for describing design artifacts using the knowledge representation technique of frames Any attempt to utilize computer aids to automate or partially automate the design process must resolve the problem of representing the design artifact in a manner that is suited both to computer manipulation and to engineering analysis, and subsequently to design. Recognizing these needs, we have devised a model, referred to as the description system, that is based on the frame technique of description or knowledge representation. Our goals were to explore the suitability of frames for modeling design artifacts, evaluate their characteristics, explore their robustness and assess their applicability in the structural engineering domain. This paper reports on the development of a computer implementation of a system, referred to as a description system, designed to achieve these goals. The description system consists of three levels: the system shell; the knowledge base; and instantiations of descriptions. This system implements various facilities which are useful to designers, including the computation of design values, integrity maintenance, providing default values, and supporting explanation. This paper presents an overview of the three levels of the description system, briefly discusses its main features and illustrates a civil engineering application involving the design of a two-storey structure.