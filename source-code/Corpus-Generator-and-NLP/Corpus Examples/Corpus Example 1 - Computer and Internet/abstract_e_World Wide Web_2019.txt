Research and implementation of event extraction from twitter using LDA and scoring function With the fast growth of social media, interest is increasing in detecting popular events from tweets. Event extraction is a work which identifies events from tweets or database of tweets. Each and every day, hundreds of megabytes of current stories are being added into the news archives of the major news agencies, containing much important and interesting news. Aim of this event extraction strategy is to extract and retrieve major life events from twitter data. Example of events extraction include seminar presentation, Job opening, Admission in Top universities, new technology etc. The role of this extraction is to collect major life events in the form of retrievable entries that include structured data about major life event name, location and time. Most of previous research on event extraction was mainly on textual level extraction such as News, medical systems, text summarization, whereas less work has been done on event extraction from noisy text such as tweets. For instance, tweets are short and self-contained which make them lack useful information. The target of this research is to develop algorithm and methodology that extract and efficiently conclude major life events extracted from social media. Optimizing semantic LSTM for spam detection Classifying spam is a topic of ongoing research in the area of natural language processing, especially with the increase in the usage of the Internet for social networking. This has given rise to the increase in spam activity by the spammers who try to take commercial or non-commercial advantage by sending the spam messages. In this paper, we have implemented an evolving area of technique known as deep learning technique. A special architecture known as Long Short Term Memory (LSTM), a variant of the Recursive Neural Network (RNN) is used for spam classification. It has an ability to learn abstract features unlike traditional classifiers, where the features are hand-crafted. Before using the LSTM for classification task, the text is converted into semantic word vectors with the help of word2vec, WordNet and ConceptNet. The classification results are compared with the benchmark classifiers like SVM, Naïve Bayes, ANN, k-NN and Random Forest. Two corpuses are used for comparison of results: SMS Spam Collection dataset and Twitter dataset. The results are evaluated using metrics like Accuracy and F measure. The evaluation of the results shows that LSTM is able to outperform traditional machine learning methods for detection of spam with a considerable margin. Self-learning and embedding based entity alignment Entity alignment aims to identify semantical matchings between entities from different groups. Traditional methods (e.g., attribute comparison-based methods, graph operation-based methods and active learning ones) are usually supervised by labeled data as prior knowledge. Since it is not trivial to label data for training, researchers have then turned to unsupervised methods, and have thus developed similarity-based methods, probabilistic methods, graphical model-based methods, etc. In addition, structure or class information is further explored. As an important part of a knowledge graph, entities contain rich semantical information that can be well learned by knowledge graph embedding methods in low-dimensional vector spaces. However, existing methods for entity alignment have paid little attention to knowledge graph embedding. In this paper, we propose a self-learning and embedding based method for entity alignment, thus called SEEA, to iteratively find semantically aligned entity pairs, which makes full use of semantical information contained in the attributes of entities. Experiments on three realistic datasets and comparison with a few baseline methods validate the effectiveness and merits of the proposed method. $$\hbox {NE}^2$$NE2: named event extraction engine Named event discovery using news headlines is an important problem with various applications in story telling, news event exploration, social media information fusion, etc. Named events are short phrases that represent the name of events like 2016 Rio Olympic Games, 2G Case, and Adarsh Society Scam. Existing work has largely focused on discovering events of named events using data mining and text mining techniques. However, the problem of discovering named event has not been addressed yet. In this paper, we present a system $$\hbox {NE}^{2}$$NE2 that uses pattern- based method to discover named events using news headlines. Along with named event, we also discover its categories, popular durations, popularity, and type of named events. Named events are categorized into candidate-level and high-level categories using URL information, and popular durations of named events are extracted using temporal information of news headlines. Our system generates 75,689 number of named events by analyzing 6.5 million news headlines. Out of 75,689 named events, 62,950 (82%) are categorized and popular duration are extracted for 73,288 (96.8%) number of named events. Based on performed experiments, our proposed system $$\hbox {NE}^{2}$$NE2 has 68% of accuracy for named events, 71.6% for named event’s category, and 78.4% for named event’s popular duration. A probabilistic model for semantic advertising Contextual advertising focuses on placing suitable advertisements on web pages. To attract user’s intention, the advertisements should be highly related to the target web page. The most effective way to do contextual advertising is ontology-based matching algorithms. The main problem of such algorithms is the difficulty in constructing and populating the ontology for matching advertisements. In this paper, we propose an automatic construction method for advertisement ontology. The construction method searches related documents from Web, extracts keywords and weights keywords for concepts. The weighted keywords are treated as instances of concepts and used to generate centroid vectors for concepts. In order to weight keywords in a proper way, we raise a formula WebSSR (Super-Subordinate Relation by Web). WebSSR weights words based on the probabilities that they have Specific Relations with the target concept. We compare our formula with LDA, NGD, WebJaccard, WebOverlap, WebDice and WebPMI, and our formula outperforms all of them. Experiment results also show that our method is more effective than five baseline methods: Bayesian, SVM, SLSA, LDA and Paragraph2Vec. User interest prediction over future unobserved topics on social networks The accurate prediction of users’ future interests on social networks allows one to perform future planning by studying how users will react if certain topics emerge in the future. It can improve areas such as targeted advertising and the efficient delivery of services. Despite the importance of predicting user future interests on social networks, existing works mainly focus on identifying user current interests and little work has been done on the prediction of user potential interests in the future. There have been work that attempt to identify a user future interests, however they cannot predict user interests with regard to new topics since these topics have never received any feedback from users in the past. In this paper, we propose a framework that works on the basis of temporal evolution of user interests and utilizes semantic information from knowledge bases such as Wikipedia to predict user future interests and overcome the cold item problem. Through extensive experiments on a real-world Twitter dataset, we demonstrate the effectiveness of our approach in predicting future interests of users compared to state-of-the-art baselines. Moreover, we further show that the impact of our work is especially meaningful when considered in case of cold items. Search bias quantification: investigating political bias in social media and web search Users frequently use search systems on the Web as well as online social media to learn about ongoing events and public opinion on personalities. Prior studies have shown that the top-ranked results returned by these search engines can shape user opinion about the topic (e.g., event or person) being searched. In case of polarizing topics like politics, where multiple competing perspectives exist, the political bias in the top search results can play a significant role in shaping public opinion towards (or away from) certain perspectives. Given the considerable impact that search bias can have on the user, we propose a generalizable search bias quantification framework that not only measures the political bias in ranked list output by the search system but also decouples the bias introduced by the different sources—input data and ranking system. We apply our framework to study the political bias in searches related to 2016 US Presidential primaries in Twitter social media search and find that both input data and ranking system matter in determining the final search output bias seen by the users. And finally, we use the framework to compare the relative bias for two popular search systems—Twitter social media search and Google web search—for queries related to politicians and political events. We end by discussing some potential solutions to signal the bias in the search results to make the users more aware of them. Predicting trading interactions in an online marketplace through location-based and online social networks Link prediction is a prominent research direction e.g., for inferring upcoming interactions to be used in recommender systems. Although this problem of predicting links between users has been extensively studied in the past, research investigating this issue simultaneously in multiplex networks is rather rare so far. This is the focus of this paper. We investigate the extent to which trading interactions between sellers and buyers within an online marketplace platform can be predicted based on three different but overlapping networks—an online social network, a location-based social network and a trading network. In particular, we conducted the study in the context of the virtual world Second Life. For that, we crawled according data of the online social network, user information of the location-based social network obtained by specialized bots, and we extracted purchases of the trading network. Overall, we generated and used 57 topological and homophilic features in different constellations to predict trading interactions between user pairs. We focused on both unsupervised as well as supervised learning methods. For supervised learning, we achieved accuracy values up to $$92.5\%$$92.5%, for unsupervised learning we obtained nDCG values up to over $$97\%$$97% and MAP values up to $$75\%$$75%. Critical Philosophy of the Postdigital This paper draws on authors’ recent works on cybernetics, complexity theory, quantum computing, Artificial Intelligence, deep learning, and algorithmic capitalism, and these ideas are brought together to develop a critical philosophy of the postdigital. Quantum computing is based on quantum mechanics and offers a radically different approach from classical comouting based on classical mechanics. Cybernetics, and complexity theory, provide insight into systems that are too complex to predict their future. Artificial Intelligence and deep learning are promising the final stage of automation which is not compatible with the welfare state based on full employment. We have thus arrived into the age of algorithmic capitalism, and its current phase, ‘biologization of digital reason’ is a distinct phenomenon that is at an early emergent form that springs from the application of digital reason to biology and the biologization of digital processes. Rejecting a fully mechanical universe, therefore, a critical pedagogy of the postdigital is closely related to Whitehead’s process philosophy, which is a form of speculative metaphysics that privileges the event and processes over and above substance. A critical philosophy of the postdigital is dialectically interrelated with the theories such as cybernetics and complexity theory, and also processes such as quantum computing, complexity science, and deep learning. These processes constitute the emerging techno-science global system, perpetuate (algorithmic) capitalism, and offer an opportunity for techno-social change. Influence me! Predicting links to influential users In addition to being in contact with friends, online social networks are commonly used as a source of information, suggestions and recommendations from members of the community. Whenever we accept a suggestion or perform any action because it was recommended by a “friend”, we are being influenced by him/her. For this reason, it is useful for users seeking for interesting information to identify and connect to this kind of influential users.
 In this context, we propose an approach to predict links to influential users. Compared to approaches that identify general influential users in a network, our approach seeks to identify users who might have some kind of influence to individual (target) users. To carry out this goal, we adapted an influence maximization algorithm to find new influential users from the set of current influential users of the target user. Moreover, we compared the results obtained with different metrics for link prediction and analyzed in which context these metrics obtained better results.
 Those were the days: learning to rank social media posts for reminiscence Social media posts are a great source for life summaries aggregating activities, events, interactions and thoughts of the last months or years. 
They can be used for personal reminiscence as well as for keeping track with developments in the lives of not-so-close friends. One of the core challenges of automatically creating such summaries is to decide which posts are memorable, i.e., should be considered for retention and which ones to forget. To address this challenge, we design and conduct user evaluation studies and construct a corpus that captures human expectations towards content retention. We analyze this corpus to identify a small set of seed features that are most likely to characterize memorable posts. Next, we compile a broader set of features that are leveraged to build general and personalized machine-learning models to rank posts for retention. By applying feature selection, we identify a compact yet effective subset of these features. The models trained with the presented feature sets outperform the baseline models exploiting an intuitive set of temporal and social features. Improved network community detection using meta-heuristic based label propagation Label propagation is a low complexity approach to community detection in complex networks. The current state-of-the-art label propagation based algorithm (LPAm+) detects communities using a two-stage iterative procedure: the first stage is to assign labels, or community memberships, to nodes using label propagation to maximize modularity, a well-known quality function to evaluate the goodness of a community division, the second stage merges smaller communities to further improve modularity. LPAm+ achieves an excellent performance on networks of strong communities, which have more intra-community links than inter-community links. However, as the label propagation procedure in LPAm+ uses a greedy heuristic to maximize modularity, LPAm+ tends to get trapped in poor local maxima on networks with weak communities, which have more inter-community links than intra-community links. We overcome this drawback of LPAm+ by introducing a novel label propagation procedure inspired by the meta-heuristic Record-to-Record Travel algorithm to improve modularity before merging communities. To handle directed networks, we employ a directed version of modularity as the objective function to maximize. We also perform an empirical analysis to examine the sensitivity of our algorithm to its parameters. Experimental results on synthetic networks show that the proposed algorithm, named meta-LPAm+, outperforms LPAm+ in term of modularity on networks with weak communities while retaining a comparable performance on networks of strong communities. For 8 widely used real-world networks, meta-LPAm+ finds the highest modularity value obtained by previously published algorithms on 5 networks and has a comparable or higher modularity value than these algorithms on 3 other networks. How do developers utilize source code from stack overflow? Technical question and answer Q&A platforms, such as Stack Overflow, provide a platform for users to ask and answer questions about a wide variety of programming topics. These platforms accumulate a large amount of knowledge, including hundreds of thousands lines of source code. Developers can benefit from the source code that is attached to the questions and answers on Q&A platforms by copying or learning from (parts of) it. By understanding how developers utilize source code from Q&A platforms, we can provide insights for researchers which can be used to improve next-generation Q&A platforms to help developers reuse source code fast and easily. In this paper, we first conduct an exploratory study on 289 files from 182 open-source projects, which contain source code that has an explicit reference to a Stack Overflow post. Our goal is to understand how developers utilize code from Q&A platforms and to reveal barriers that may make code reuse more difficult. In 31.5% of the studied files, developers needed to modify source code from Stack Overflow to make it work in their own projects. The degree of required modification varied from simply renaming variables to rewriting the whole algorithm. Developers sometimes chose to implement an algorithm from scratch based on the descriptions from Stack Overflow answers, even if there was an implementation readily available in the post. In 35.5% of the studied files, developers used Stack Overflow posts as an information source for later reference. To further understand the barriers of reusing code and to obtain suggestions for improving the code reuse process on Q&A platforms, we conducted a survey with 453 open-source developers who are also on Stack Overflow. We found that the top 3 barriers that make it difficult for developers to reuse code from Stack Overflow are: (1) too much code modification required to fit in their projects, (2) incomprehensive code, and (3) low code quality. We summarized and analyzed all survey responses and we identified that developers suggest improvements for future Q&A platforms along the following dimensions: code quality, information enhancement & management, data organization, license, and the human factor. For instance, developers suggest to improve the code quality by adding an integrated validator that can test source code online, and an outdated code detection mechanism. Our findings can be used as a roadmap for researchers and developers to improve code reuse. Extending Geographical Distribution Range of Reef Needlefish Strongylura incisa (Valenciennes, 1846) (Teleostei: Beloniformes: Belonidae) in the Eastern Indian Ocean with a Key to the Species of Needlefish Occurring in the Area Present study deals with new distributional records of Strongylura incisa (Belonidae) based on five specimens collected from the Andaman Islands and Tuticorin Coast (Tamil Nadu), India during 2016–17. Specimens were identified by absence of scales at bases of dorsal and anal fins, 19–20 dorsal fin rays, 21–23 anal fin rays, 102–113 predorsal scales, dorsal fin origin over 4–5 anal fin rays, prominent elongate spot on cheek between opercle and preopercle, and double lobe of gonad. Strongylura incisa can be distinguished from its most similar congener S. leiura based on number of predorsal scales (100–125 in S. incisa versus 130–180 in S. leiura), dorsal fin origin over anal fin rays (4–6 in S. incisa versus 7–10 in S. leiura), number of gonad lobes (2 in S. incisa versus 1 in S. leiura) and colour characteristics (prominent elongate spot on cheek between opercle and preopercle in S. incisa versus black bar on cheek between opercle and preopercle and anterior region of the body in S. leiura). Present study provides the first documented record of S. incisa from the Andaman and Nicobar Islands and East Coast of India with an updated key for Strongylura species. Further, it is an addition to the ichthyofaunal biodiversity of the Andaman and Nicobar Islands and East Coast of India. New Distributional Record of Flat Needlefish Ablennes hians (Valenciennes, 1846) (Beloniformes: Belonidae) in the North-Eastern Indian Ocean with Taxonomic Details Present study is based on five specimens of flat needlefish Ablenneshians collected from Puri North (Lat. 19047′43.062″N, Long. 85049′38.5788″E), Odisha, India on 7 January 2018. Based on morphometric and meristic characters, the specimens were identified as Ablenneshians (family Belonidae), having laterally compressed body with a series of vertical bars, 26 to 27 anal fin rays, 24–25 dorsal fin rays, 13–14 pectoral fin rays, and single lobe of gonad. A.hians can be distinguished from other three species of needlefishes reported from northeast coast of India based on body shape (body strongly compressed laterally in A. hians vs. rounded in cross section in other three species), number of anal fin rays (24–28 in A. hians vs. 15–25), vertical bars on body (a series of vertical bars in A. hians vs. no vertical bars in others except for S. leiura where few vertical bars are present on anterior half of the body). The occurrence of flat needlefish along Odisha Coast revealed the extension of its known geographical distributional range i.e. from noth of Andhra Pradesh in the North-eastern Indian Ocean. Further, it is an addition to the ichthyofaunal biodiversity of northeast coast of India. First Report of Genus Diploconger (Congridae: Congrinae) from the Indian Coast Present study reports a moderately elongate, light brown coloured eel, Diploconger polystigmatus Kotthaus, 1968, for the first time from Indian waters. The species is differentiated from other congridae in having the following combination of characters: double row of cephalic and lateral-line pores, dark brown strip from the middle of the supratemporal commissure up to the tip of the mouth, dorsal fin origin nearly from the halfway of pectoral fin, each dorsal fin ray have black spots at the base, pre-dorsal vertebrae 9, pre-anal vertebrae 27, total vertebrae 108. The present report also reports the genus Diploconger for the first time from Indian waters. Occurrence of Chondracanthus merluccii (Holten, 1802) and Chondracanthus angustatus Heller, 1865 (Copepoda: Chondracanthidae) in Turkish Marine Waters The Sea of Marmara being an unique inland sea displays the transitional environment between the Black Sea and the Mediterranean Sea. There is limited studies on the parasitic copepods of fish in the Sea of Marmara. Chondracanthus merluccii (Holten, 1802) and Chondracanthus angustatus Heller, 1865 (Copepoda: Chondracanthidae) are reported for the first time in Turkey. C. merluccii were found on the upper and under surface of the mouth, upperside of the tongue, through teeth of the European hake, Merluccius merluccius (L.), while C. angustatus on the gill rakers of the stargazer, Uranoscopus scaber L. captured from the Sea of Marmara. Morphological characters of Chondracanthus merluccii and Chondracanthus angustatus were presented with photos. This study contributes new geographical distribution for these parasites. First Substantiated Record of Molva macrophthalma (Rafinesque, 1810) in Southern Aegean Sea, Turkey A single female specimen of the Spanish ling, Molva macrophthalma (Rafinesque, 1810) (35.3 cm in total length and 96.2 g in weight) was caught by a commercial trammel net at a depth of 393 m on 19 March 2017 from Sıgacık Bay (İzmir coast), Turkey. This paper presents the first substantiated occurrence and, hence, the confirmation of M. macrophthalma in the southern Aegean Sea, Turkey. This record is significant because the last record of M. macrophthalma in the region was made over 50 years ago in the coastal waters of Turkey. Therefore, the species could be considered as exceptionally rare in the Mediterranean, Turkey. Morphometric and meristic characters of the specimen are given in the text. A New Fishery Area for the Swordfish Xiphias gladius Linnaeus, 1758 (Teleostei: Xiphiidae) in the Sea of Oman The first record of the swordfish Xiphias gladius Linnaeus, 1758, in Omani waters is based on a specimen of 1756 mm total length, which has been collected by gill net on the coast of Sur City, 151 Km south of Muscat City, Oman. Morphometric and meristic data are provided and compared with those from other sea basins. Guiding attention of faces through graph based visual saliency (GBVS) In a general scenario, while attending a scene containing multiple faces or looking towards a group photograph, our attention does not go equal towards all the faces. It means, we are naturally biased towards some faces. This biasness happens due to availability of dominant perceptual features in those faces. In visual saliency terminology it can be called as ‘salient face’. Human’s focus their gaze towards a face which carries the ‘dominating look’ in the crowd. This happens due to comparative saliency of the faces. Saliency of a face is determined by its feature dissimilarity with the surrounding faces. In this context there is a big role of human psychology and its cognitive science too. Therefore, enormous researches have been carried out towards modeling the computer vision system like human’s vision. This paper proposed a graphical based bottom up approach to point up the salient face in the crowd or in an image having multiple faces. In this novel method, visual saliencies of faces have been calculated based on the intensity values, facial areas and their relative spatial distances. Experiment has been conducted on gray scale images. In order to verify this experiment, three level of validation has been done. In the first level, our results have been verified with the prepared ground truth. In the second level, intensity scores of proposed saliency maps have been cross verified with the saliency score. In the third level, saliency map is validated with some standard parameters. The results are found to be interesting and in some aspects saliency predictions are like human vision system. The evaluation made with the proposed approach shows moderately boost up results and hence, this idea can be useful in the future modeling of intelligent vision (robot vision) system. Information-seeking behaviour and academic success in higher education: Which search strategies matter for grade differences among university students and how does this relevance differ by field of study? Today, most college students use the Internet when preparing for exams or homework. Yet, research has shown that undergraduates’ information literacy skills are often insufficient. In this paper, we empirically test the relation between information-seeking strategies and grades in university. We synthesise arguments from the literature on information-seeking behaviour and approaches to learning in tertiary education. Building on the distinction between deep- and surface-level learning, we develop a classification of online search strategies and contrast it with traditional information behaviour. Multivariate analyses using a two-wave online survey among undergraduate students at a German university indicate that using advanced online information-seeking strategies is a significant and robust predictor of better grades. However, there are notable differences between subject groups: Traditional information behaviour is still crucial in the humanities. Advanced search strategies are beneficial in all settings, but only one in four students uses these early on, while this share increases to around 50% over the course of studies. An empirical assessment of best-answer prediction models in technical Q&A sites Technical Q&A sites have become essential for software engineers as they constantly seek help from other experts to solve their work problems. Despite their success, many questions remain unresolved, sometimes because the asker does not acknowledge any helpful answer. In these cases, an information seeker can only browse all the answers within a question thread to assess their quality as potential solutions. We approach this time-consuming problem as a binary-classification task where a best-answer prediction model is built to identify the accepted answer among those within a resolved question thread, and the candidate solutions to those questions that have received answers but are still unresolved. In this paper, we report on a study aimed at assessing 26 best-answer prediction models in two steps. First, we study how models perform when predicting best answers in Stack Overflow, the most popular Q&A site for software engineers. Then, we assess performance in a cross-platform setting where the prediction models are trained on Stack Overflow and tested on other technical Q&A sites. Our findings show that the choice of the classifier and automatied parameter tuning have a large impact on the prediction of the best answer. We also demonstrate that our approach to the best-answer prediction problem is generalizable across technical Q&A sites. Finally, we provide practical recommendations to Q&A platform designers to curate and preserve the crowdsourced knowledge shared through these sites. Towards an Autonomous Pelagic Observatory: Experiences from Monitoring Fish Communities around Drifting FADs This work presents a methodological synthesis for the in situ monitoring of fish aggregating devices (FADs) using a combination of optical, echosounder and SCUBA observations conducted in the vicinity of drifting FADs. The acoustic methods allowed, according to the devices used, the description of the spatial organisation and dynamics of biotic scattering layers, individual fishes, schools, shoals and mammals, while visual, photographic and video observations permitted species identification within a range of 0 to ~ 25 m. Based on these results, we elaborate on the interest to combine acoustic and visual methods, and present an autonomous instrumented drifting buoy for remotely monitoring fish diversity and abundance in the pelagic ecosystems. The perspective of autonomously collecting large amounts of basic information useful for ecological and fisheries studies in an ecosystem approach for open sea, as well as coastal pelagic environment, is also emphasized. As perspective we present “Seaorbiter” a futuristic large drifting platform which will allow performing innovative ecosystemic studies taking into account simultaneously all macro components of the pelagic ecosystem. Patterns of knowledge sharing in an online affinity space for diabetes Our research explores how people learn as part of everyday contexts and settings and specifically, we explore the discourse of an online affinity space for diabetics, where participants engage in knowledge sharing and storytelling around disease management. We frame the analyses by examining participants’ meaning making discourse for advancing knowledge and practices situated in everyday, practical activity. Social network analyses were conducted to visualize the structure of the community. Analyses of discourse in the affinity space revealed three primary patterns of knowledge sharing: (a) sharing information; (b) extending perspectives; and (c) communicating repertoires of practice. Our analyses describe recurring narratives, discourse patterns, and constructions, which can be seen as part of the cultural model that defines the diabetes affinity space. We found that personalized storytelling, which included sharing of personal experiences and data such as blood glucose levels, acted as a primary pattern of language use. Our results contribute to an understanding of the role of discourse in supporting personal and community practices and learning in online affinity spaces, as well as implications for the design of technology in supporting knowledge sharing in such spaces. Head Deformity in Epinephelus diacanthus (Teleostei: Epinephelidae) and Oreochromis mossambicus (Teleostei: Cichlidae) Collected from Saudi Arabia and Oman, with a New Record of E. diacanthus to the Arabian Gulf Waters Specimens of spinycheek grouper Epinephelus diacanthus (Teleostei: Epinephelidae) and the Mozambique tilapia, Oreochromis mossambicus (Teleostei: Cichlidae) were reported in this study to show the case of head deformity in individuals of Epinephelus diacanthus collected from waters in the vicinity of Jubail City, Saudi Arabia. Individuals of Oreochromis mossambicus were obtained from the fish market, which are originally collected and brought from the neighbouring Sultanate of Oman have shown pugheadness deformity. The specimen of Epinephelus diacanthus shown to have short neurocranium, absence of upper jaw and normal lower jaw, with the mouth nearly closed. The specimen of Oreochromis mossambicus showed short neurocranium and a normal upward directed upper and lower jaws. The mouth was nearly closed, with the two sides being stuck, which means that the deformity has a significant effect on the mechanism of mouth operation. The case of O. mossambicus is considered more severe than the case of E. diacanthus in spite of missing few bony elements. The possible causes for such deformities are discussed. The first new documented record of Epinephelus diacanthus in the Arabian Gulf waters is reported based on one specimen with total length 420 mm, which has been collected by gill net on the coast of Jubail City. Morphometric and meristic data are provided. Difficulties in defining mobile learning: analysis, design characteristics, and implications Mobile learning, or m-learning, has become an umbrella term for the integration of mobile computing devices within teaching and learning. In the literature, however, use of the terms has been unsystematic. The purpose of this article is to critically examine the principles of mobile learning. First, I examine the extant literature with regard to defining mobile learning. Four definitions of mobile learning categories are described: (1) relationship to distance education and elearning, (2) exploitation of devices and technologies, (3) mediation with technology, and (4) nomadic nature of learner and learning. Second, in an effort to provide a basis on which to ground future mobile learning research, I propose a framework of design characteristics for mobile learning environments. Seven design characteristics are identified and discussed. Finally, I present implications for future research and instructional design. This paper contributes to the field of mobile learning by providing researchers more precise ways to identify and describe the characteristics of mobile learning environments, as well as describe the attributes of successful mobile learners. A Novel Data Mining Approach for Analysis and Pattern Recognition of Active Fingerprinting Components Active fingerprinting is an effective penetration testing technique to know about vulnerability of hosts against security threats and network as a whole. Sometimes firewalls may block fingerprinting packets, hence making the probes infeasible. Measured Round Trip Time (RTTm) is a benign number that can be obtained from communication based on legitimate non malicious packets. In this paper, RTTm has been used along with other timers namely Smoothened Round-trip Time (SRTT), Round-trip Time Variance (RTTVar), Retransmission Time Out (RTO) and Scantime for pattern recognition and association analysis with the aid of cross-correlations. Experimental relationship among these timers are derived to back-up existing theoretical knowledge. A novel method to estimate IP-ID Sequence classes and network-traffic intensity based on these timers has been proposed. Results show that the model can be used to accurately derive (about 100% accuracy) active fingerprinting components IP-ID sequences and link traffic estimation. Analytical results obtained by this study can help in designing high-performance realistic networks and dynamic congestion control techniques. Corruption-tolerant bandit learning We present algorithms for solving multi-armed and linear-contextual bandit tasks in the face of adversarial corruptions in the arm responses. Traditional algorithms for solving these problems assume that nothing but mild, e.g., i.i.d. sub-Gaussian, noise disrupts an otherwise clean estimate of the utility of the arm. This assumption and the resulting approaches can fail catastrophically if there is an observant adversary that corrupts even a small fraction of the responses generated when arms are pulled. To rectify this, we propose algorithms that use recent advances in robust statistical estimation to perform arm selection in polynomial time. Our algorithms are easy to implement and vastly outperform several existing UCB and EXP-style algorithms for stochastic and adversarial multi-armed and linear-contextual bandit problems in wide variety of experimental settings. Our algorithms enjoy minimax-optimal regret bounds, as well as can tolerate an adversary that is allowed to corrupt upto a universally constant fraction of the arms pulled by the algorithm. The unknown in the Egyptian uprising: towards an anthropology of al-Ghayb During the Egyptian uprising in 2011, a TV crew accidentally filmed a ghostly horseman in the midst of protesters. This essay takes the ghostly horseman as a starting point for thinking about the possibilities of an anthropology of al-ghayb, the invisible and unknown. Drawing on fieldwork in Egypt, as well as online reports and contestations of apparitions, visions, and dreams seen during the uprising, I suggest that accounts of the unseen pose a profound challenge to (and open up new possibilities for) doing ethnographic research, writing ethnography, and thinking anthropologically. Inspired by Michael Taussig, I suggest that the challenge is not to undo the invisible but to find a language that runs along the seam where the visible and the invisible connect and disconnect. Soft voting technique to improve the performance of global filter based feature selection in text corpus In text classification, the Global Filter-based Feature Selection Scheme (GFSS) selects the top-N ranked words as features. It discards the low ranked features from some classes either partially or completely. The low rank is usually due to varying occurrence of the words (terms) in the classes. The Latent Semantic Analysis (LSA) can be used to address this issue as it eliminates the redundant terms. It assigns an equal rank to the terms that represent similar concepts or meanings, e.g. four terms “carcinoma”, “sarcoma”, “melanoma”, and “cancer” represent a similar concept, i.e. “cancer”. Thus, any selected term by the algorithms from these four terms doesn’t affect the classifier performance. However, it does not guarantee that the selection of top-N LSA ranked terms by GFSS are the representative terms of each class. An Improved Global Feature Selection Scheme (IGFSS) solves this issue by selecting an equal number of representative terms from all the classes. However, it has two issues, first, it assigns the class label and membership of each term on the basis of an individual vote of the Odds Ratio (OR) method thereby limiting the decision making capability. Second, the ratio of selected terms is determined empirically by the IGFSS and a common ratio is applied to all the classes to assign the positive and negative membership of the terms. However, the ratio of positive and negative nature terms varies from one class to another and it may be very less for one class, whereas high for other classes. Thus, one common negative features ratio used by the IGFSS affects those classes of a dataset in which there is an imbalance between positive and negative nature words. To address these issues of IGFSS, a new Soft Voting Technique (SVT) is proposed to improve the performance of GFSS. There are two main contributions in this paper: (i) The weighted average score (Soft Vote) of three methods, viz. OR, Correlation Coefficient (CC), and GSS Coefficients (GSS) improves the numerical discrimination of words to identify there positive and negative membership to a class. (ii) A mathematical expression is incorporated in the IGFSS that computes a varying ratio of positive and negative memberships of the terms for each class. The membership is based on the occurrence of the terms in the classes. The proposed SVT is evaluated using four standard classifiers applied on five bench-marked datasets. The experimental results based on Macro_F1 and Micro_F1 measures show that SVT achieves a significant improvement in the performance of classifiers in comparison of standard methods. Flooding disaster resilience information framework for smart and connected communities This paper presents the research challenges of designing a combined physical sensor- and social sensor-based information framework to collect heterogeneous flooding disaster data, and then to fuse those data and generate actionable understandings. Our overall objective is to improve the response preparedness of critical infrastructures, contributing to the goal of smart and connected communities. We propose methods to model physical and social sensors, and open demographic data integration with regional knowledge, and to leverage these fused data for understanding impending events and conditions deleterious to lives and properties. In addition, the proposed system will predict the disaster events and provide knowledge-based recommendations to inform emergency management personnel to enable the resilience of the smart and connected communities. Preliminary experiments for the framework are promising. Further work is needed to validate the framework in collaboration with the local emergency managers. Area coverage of heterogeneous wireless sensor networks in support of Internet of Things demands As the Internet of Things (IoT) evolves, more and more Wireless Sensor Networks (WSNs) are being deployed in the real world. Connected vehicles, smart grids, smart cities, smart healthcare, networks of robots, and disaster recovery networks are some examples. In WSNs, the area coverage is one of the most important quality of service metrics. A WSN without enough area coverage yields incorrect results. So calculating the covered area of a WSN is mandatory. Previous studies have used a simple approach: all nodes send their location to the sink, and it calculates the covered area centrally which makes huge unnecessary communication overhead. In our previous work titled Distributed Exact Coverage Rate Calculation, we calculated the covered area of a homogenous WSN in a distributed manner. In this paper, we provide a Heterogeneous Distributed Precise Coverage Rate (HDPCR) mechanism that calculates the covered area of a Heterogeneous Wireless Sensor Network by using a localized mechanism.
 With the use of boundary detection mechanisms, the HDPCR detects the boundary of the network and calculates its area. HDPCR also detects holes and calculates their area precisely. By subtracting these two calculated values, the covered area of the network can be computed. Many related studies have evaluated the coverage rate approximately with error and require more calculations to reduce the error rate. HDPCR calculates the coverage rate precisely without an error rate using simple arithmetic calculations. The exhaustive simulation also shows the superiority of HDPCR as compared to the previous approaches.
 A novel JSON based regular expression language for pattern matching in the internet of things The Internet of Things work by constantly sensing the physical properties in the vicinity of the user such as ambient light, sounds, motion and temperature. These sensors produce huge volumes of data that has to be efficiently sifted for relevant events required triggering certain actions. In addition, filtering has to be performed to ensure that privacy-sensitive confidential data is not leaked. Efficient and expressive pattern matching is thus a key enabling technology for the full realization of ambient and humanized computing. The bulk of research in this area has focused on the use of specialized hardware and reducing of the memory footprint. Unfortunately, there has been limited work if any on optimizing the core elements of pattern matching- the regular expression language and the compilation process that is responsible for converting patterns into internal data structures. The importance of writing good REs so that on compilation they do not lead to unrealizable data structures is relatively less understood. In the proposed research, we empirically compare different RE processing engines and practically demonstrate that the compilation phase is highly memory intensive and time-consuming as compared to the matching phase -and hence is worth exploring for new techniques and optimizations. As a second important contribution, we propose a novel technique for defining regular expressions by utilizing JavaScript Object Notation. Our evaluation with carefully created patterns shows that the performance of the proposed technique is at par with competing approaches. It is also less ambiguous, extensible, more expressive and much appropriate for defining large and complex patterns. Topic modeling in marketing: recent advances and research opportunities Using a probabilistic approach for exploring latent patterns in high-dimensional co-occurrence data, topic models offer researchers a flexible and open framework for soft-clustering large data sets. In recent years, there has been a growing interest among marketing scholars and practitioners to adopt topic models in various marketing application domains. However, to this date, there is no comprehensive overview of this rapidly evolving field. By analyzing a set of 61 published papers along with conceptual contributions, we systematically review this highly heterogeneous area of research. In doing so, we characterize extant contributions employing topic models in marketing along the dimensions data structures and retrieval of input data, implementation and extensions of basic topic models, and model performance evaluation. Our findings confirm that there is considerable progress done in various marketing sub-areas. However, there is still scope for promising future research, in particular with respect to integrating multiple, dynamic data sources, including time-varying covariates and the combination of exploratory topic models with powerful predictive marketing models. DIS-C: conceptual distance in ontologies, a graph-based approach This paper presents the DIS-C approach, which is a novel method to assess the conceptual distance between concepts within an ontology. DIS-C is graph based in the sense that the whole topology of the ontology is considered when computing the weight of the relationships between concepts. The methodology is composed of two main steps. First, in order to take advantage of previous knowledge, an expert of the ontology domain assigns initial weight values to each of the relations in the ontology. Then, an automatic method for computing the conceptual relations refines the weights assigned to each relation until reaching a stable state. We introduce a metric called generality that is defined in order to evaluate the accessibility of each concept, considering the ontology like a strongly connected graph. Unlike most previous approaches, the DIS-C algorithm computes similarity between concepts in ontologies that are not necessarily represented in a hierarchical or taxonomic structure. So, DIS-C is capable of incorporating a wide variety of relationships between concepts such as meronymy, antonymy, functionality and causality. User-centered recommendation using US-ELM based on dynamic graph model in E-commerce The recommender systems can gain the needs and interests of users by analyzing the user history data and then help the users making decisions on appropriate choices in E-commerce. However, with the increasing of data volume and the popularization of information network, the participation of users in E-commerce activities is growing deeply. How to analyze the user preferences and make a user-centered efficient recommendation is an urgent problem to be further researched. In this paper, we first propose the user-centered recommendation based on dynamic graph model to express the user preferences and gain the user preference vectors for recommendation. Then, after gaining the user preferences vectors, we propose the user clustering algorithm using US-ELM to cluster the users into different clusters. Last, we provide two recommendation algorithms, which can present top-k recommendation, respectively the group recommendation and personal recommendation. With the extensive experiments, our recommendation algorithms can effectively express the user preferences and reach a good performance. An experimental study on symbolic extreme learning machine With the advent of big data era, the volume and complexity of data have increased exponentially and the type of data has also been increased largely. Among all different types of data, symbolic data plays an important role in the study on machine learning model. It has been proved that feed-forward neural network (FNN) has a good ability to deal with numeric data but relatively clumsy with symbolic data. In this paper, a special type of FNN called Extreme Learning Machine (ELM) is discussed for handling symbolic data. Experimental results demonstrate that, unlike traditional back propagation based FNN, ELM has a better performance in comparison with C4.5 which is generally acknowledged as one of the best algorithms in handling symbolic data classification problems. In this performance comparison, some key evaluation criteria such as generalization ability, time complexity, the effect of training sample size and noise-resistance ability are taken into account. A term correlation based semi-supervised microblog clustering with dual constraints Microblog clustering is very important in many web applications. However, microblogs do not provide sufficient word occurrences. Meanwhile the limited length of these messages prevents traditional text clustering approaches from being employed to their full potential. To address this problem, in this paper, we propose a novel semi-supervised learning scheme fully exploring the semantic information to compensate for the limited message length. The key idea is to explore term correlation data, which well captures the semantic information for term weighting and provides greater context for microblogs. We then formulate microblog clustering problem as a semi-supervised non-negative matrix factorization co-clustering framework, which takes advantage of both prior domain knowledge of data points (microblogs) in the form of pair-wise constraints and category knowledge of features (terms). Our approach not only greatly reduces the labor-intensive labeling process, but also deeply exploits hidden information from microblog itself. Extensive experiments are conducted on two real-world microblog datasets. The results demonstrate the effectiveness of the proposed approach which produces promising performance as compared to state-of-the-art methods. An efficient and fast algorithm for community detection based on node role analysis The community structure of networks provides a comprehensive insight into their organizational structures and functional behaviors. Label propagation is one of the most commonly adopted community detection algorithm with nearly linear time complexity. It ignores the difference between nodes when breaking ties, leading to poor stability and the occurrence of the monster community. We note that different community-oriented node roles impact the label propagation in different ways. In this paper, we propose a role-based label propagation algorithm (roLPA), in which the heuristics with regard to community-oriented node role were used. We have evaluated the proposed algorithm on both real and artificial networks. The result shows that roLPA outperforms other state-of-the-art community detection algorithms. Hiding information against structural re-identification Connections between users of social networking services pose a significant privacy threat. Recently, several social network de-anonymization attacks have been proposed that can efficiently re-identify users at large scale, solely considering the graph structure. In this paper, we consider these privacy threats and analyze de-anonymization attacks at the model level against a user-controlled privacy-enhancing technique called identity separation. The latter allows creating seemingly unrelated identities in parallel, even without the consent of the service provider or other users. It has been shown that identity separation can be used efficiently against re-identification attacks if user cooperate with each other. However, while participation would be crucial, this cannot be granted in a real-life scenario. Therefore, we introduce the y-identity model, in which the user creates multiple separated identities and assigns the sensitive attribute to one of them according to a given strategy. For this, we propose a strategy to be used in real-life situations and formally prove that there is a higher bound for the expected privacy loss which is sufficiently low. Differentially Private Top-k Items Based on Least Mean Square——Take E-Commerce Platforms for Example User preference data broadly collected from e-commerce platforms have benefits to improve the user’s experience of individual purchasing recommendation by data mining and analyzing, which may bring users the risk of privacy disclosure. In this paper, we explore the problem of differential private top-k items based on least mean square. Specifically, we consider the balance between utility and privacy level of released data and improve the precision of top-k based on post-processing. We show that our algorithm can achieve differential privacy over streaming data collected and published periodically by server provider. We evaluate our algorithm with three real datasets, and the experimental results show that the precision of our method reaches 85% with strong privacy protection, which outperforms the Kalman filter-based existing methods. Factored Item Similarity and Bayesian Personalized Ranking for Recommendation with Implicit Feedback Item recommendations aim to predict a list of items (e.g., items on Amazon website) for each user that he or she might like. In fact, implicit feedback, such as transaction records in e-commerce websites and the “likes” behavior in social networks website (e.g., Facebook), has been received more and more attention in the scenarios of item recommendation. The core of the recommender system is the ranking algorithm which exploits the implicit feedback and generates the personalized item list to meet user’s specific preferences. In most of the previous studies, the pairwise personalized ranking techniques empirically achieve better performance than the matrix factorization and adaptive k nearest-neighbor method since the pairwise ranking methods can directly reflect the model user’s ranking preference on items. In most of the recent works, factored item similarity techniques which learn the global item similarity by utilizing two low-dimensional latent factor matrices achieve better performance than other state-of-art top-N methods with predefined similarity, such as cosine similarity. The individual relative preference assumption among observed items and unobserved items are critical for the pairwise ranking methods. As a response, this paper proposes a new and improved preference assumption based on the factored item similarity and individual preference. In addition, a novel recommendation algorithm correspondingly named factored item similarity and Bayesian Personalized Ranking model is designed. The novelty of the algorithm is that it can (1) learn the global item similarity with latent factor models. (2) utilize effective pairwise ranking methods to deal with the item recommendation problems with implicit feedback. (3) assign different item weights on explicit feedback and implicit feedback. Empirical results show that this model outperforms other state-of-the-art top-N recommendation methods on two public datasets in terms of prec@5 and ndcg@5. It can be found that the advantage of FSBPR lies in its ability to exploit implicit feedback and capture global item similarity. “Deep-Onto” network for surgical workflow and context recognition PurposeSurgical workflow recognition and context-aware systems could allow better decision making and surgical planning by providing the focused information, which may eventually enhance surgical outcomes. While current developments in computer-assisted surgical systems are mostly focused on recognizing surgical phases, they lack recognition of surgical workflow sequence and other contextual element, e.g., “Instruments.” Our study proposes a hybrid approach, i.e., using deep learning and knowledge representation, to facilitate recognition of the surgical workflow.MethodsWe implemented “Deep-Onto” network, which is an ensemble of deep learning models and knowledge management tools, ontology and production rules. As a prototypical scenario, we chose robot-assisted partial nephrectomy (RAPN). We annotated RAPN videos with surgical entities, e.g., “Step” and so forth. We performed different experiments, including the inter-subject variability, to recognize surgical steps. The corresponding subsequent steps along with other surgical contexts, i.e., “Actions,” “Phase” and “Instruments,” were also recognized.ResultsThe system was able to recognize 10 RAPN steps with the prevalence-weighted macro-average (PWMA) recall of 0.83, PWMA precision of 0.74, PWMA F1 score of 0.76, and the accuracy of 74.29% on 9 videos of RAPN.ConclusionWe found that the combined use of deep learning and knowledge representation techniques is a promising approach for the multi-level recognition of RAPN surgical workflow. Fusion Model for Hazard Association Network Development: A Case in Elevator Installation and Maintenance This paper proposes a fusion model for developing a data-driven risk association network, based on historical inspection records. The fusion model first re-categorizes the hazards based on the similarity in their occurrence patterns. Second, spatial and temporal heterogeneity of the hazard occurrence is examined, after which site-specific records as outliers are removed from the database. Third, a structured learning approach is used to investigate the causal relations between safety risks and the weight of each relation is calculated based on the association rules. Finally, the causal relations and weightings are fused to form the hazard association network, based on which critical hazards can be identified for safety management strategy planning. Safety management for an elevator installation and maintenance is used as a domain to validate the proposed fusion model, which develops the hazard association network using a dataset with 110,698 safety inspection records on 25,729 sites (with elevator installation or maintenance) managed by an elevator company. Using the developed network, critical hazards on the sites are identified for proactive construction management. Knowledge of the Sexual Transmission of Zika Virus and Preventive Practices Against Zika Virus Among U.S. Travelers Individuals are often at increased risk of acquiring infectious disease while traveling. We sought to understand knowledge, attitudes and practices (KAP) regarding Zika virus among travelers from the United States. A total of 1043 study participants were recruited from a probability-based internet panel. Participants self-reported their knowledge of Zika infection and modes of transmission, and identified actions they had taken to prevent Zika infection and transmission including actions to prevent unintentional pregnancy since becoming aware of the Zika virus. Logistic regression was used to model the odds of taking preventive actions against Zika infection with adjustment for potential confounding factors. Knowledge of the sexual transmissibility of Zika virus significantly increased the odds of taking a preventive action against Zika infection, especially condom use or sexual abstention. Participants reported preferences for receiving information about Zika from private doctors and from the Internet. Discrepancies between where travelers seek information about Zika and how they would like to receive information regarding Zika were also found. These findings suggest that improving targeted messaging through online media may increase awareness of the sexual transmissibility of Zika as well as improve health communications with U.S. travelers. Travelers who are unaware of potential disease risks are less likely to adopt personal protective measures to protect themselves and reduce disease spread. Thus, future work should focus on improving communication and providing education to adopt effective prevention strategies while traveling. A Two-stage Iterative Approach to Improve Crowdsourcing-Based Relevance Assessment Crowdsourcing has emerged as a viable platform to implement the relevance assessment of information retrieval. However, since crowdsourcing touches independent and anonymous workers, the quality control on assessment results has become a hotspot within academia and industry. For this problem, we propose a two-stage iterative approach by integrating the ensemble classifier and expert guidance. Specifically, in first stage an ensemble classifier is employed to select unreliable assessment objects for experts to validate. Then in second stage, the expectation maximization is utilized to update all assessment results in terms of the validation feedback. This loop continues until the cost limit is reached. Simulation experiment demonstrates that compared with existing solutions, our approach can eliminate more noise and thereby achieve a higher accuracy, while maintaining an acceptable running time and a low labor cost. A Novel Optimization Method for Ontology Matching Based on Heuristic Population Evolution Algorithm Whether for the Big Data or for the Internet of things, the heterogeneity of knowledge has always been an unavoidable problem. Ontologies have been acknowledged to be the core methodology for the heterogeneity of knowledge, and ontology matching is an important method to solve the heterogeneity of ontologies. This paper describes a novel ontology matching method to optimize the matching result. First, the method puts forward a novel ontology matching weight calculation method and a mathematical model to optimize the similarity integration. And then this method establishes an improved heuristic population evolution algorithm (shuffled frog leaping algorithm) to implement the optimization model. Finally, Ontology Alignment Evaluation Initiative (OAEI) test sets are used to evaluate the proposed method, and the feasibility and effectiveness of this method are proved by comparing with other matching systems in OAEI competition. Associative composition of components with double-sided interfaces Distributed systems are often organized in chains of components (e.g. business process chains), where each component naturally has a double-sided (left and right) interface. We suggest a corresponding, highly abstract and general framework (in mathematical terms: a monoid) of components and their composition, with minimal assumptions on the underlying global infrastructure (in fact, just a global set of symbols). As a fundamental property, decisive for the composition of more than two components, composition of such properties turns out to be associative. We discuss a number of instantiations of this framework (mainly classes of Petri nets), some of which preserve important properties (such as soundness of workflows) under composition. We glance at a number of generalizations and specializations. An analysis of emotion-exchange motifs in multiplex networks during emergency events In this paper, we present an analysis of the emotion-exchange patterns that arise from Twitter messages sent during emergency events. To this end, we performed a systematic structural analysis of the multiplex communication network that we derived from a data-set including more than 1.9 million tweets that have been sent during five recent shootings and terror events. In order to study the local communication structures that emerge as Twitter users directly exchange emotional messages, we propose the concept of emotion-exchange motifs. Our findings suggest that emotion-exchange motifs which contain reciprocal edges (indicating online conversations) only emerge when users exchange messages that convey anger or fear, either in isolation or in any combination with another emotion. In contrast, the expression of sadness, disgust, surprise, as well as any positive emotion are rather characteristic for emotion-exchange motifs representing one-way communication patterns (instead of online conversations). Among other things, we also found that a higher structural similarity exists between pairs of network layers consisting of one high-arousal emotion and one low-arousal emotion, rather than pairs of network layers belonging to the same arousal dimension. Too many tags spoil the metadata: investigating the knowledge management of scientific research with semantic web technologies Scientific research is increasingly characterised by the volume of documents and data that it produces, from experimental plans and raw data to reports and papers. Researchers frequently struggle to manage and curate these materials, both individually and collectively. Previous studies of Electronic Lab Notebooks (ELNs) in academia and industry have identified semantic web technologies as a means for organising scientific documents to improve current workflows and knowledge management practices. In this paper, we present a qualitative, user-centred study of researcher requirements and practices, based on a series of discipline-specific focus groups. We developed a prototype semantic ELN to serve as a discussion aid for these focus groups, and to help us explore the technical readiness of a range of semantic web technologies. While these technologies showed potential, existing tools for semantic annotation were not well-received by our focus groups, and need to be refined before they can be used to enhance current researcher practices. In addition, the seemingly simple notion of “tagging and searching” documents appears anything but; the researchers in our focus groups had extremely personal requirements for how they organise their work, so the successful incorporation of semantic web technologies into their practices must permit a significant degree of customisation and personalisation.