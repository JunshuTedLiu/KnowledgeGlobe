Assessing the effect on high school teachers of direct and unrestricted access to the internet: A case study of an east central florida high school The purpose of this study was to investigate the effect direct and unrestricted access to the Internet had on a group of high school teachers. Based on the naturalistic inquiry paradigm, this study explored the barriers these teachers encountered when using the Internet, how and when they elected to use the Internet, the factors that influenced their continued use of the Internet, and the transitions they experienced from using the Internet. Data collection was based on Patton's (1990) three approaches to interviewing; data analysis was based in part on Miles and Huberman's (1984) model of data reduction and display and on Spradley's (1979) task of domain analysis. Findings suggested: that teachers require ongoing Internet training, technical support, home Internet access, and time in which to learn and incorporate the Internet into their classes; that Internet use can increase teachers' self-esteem and improve their attitudes toward computers and education; and that use of the Internet by teachers encourages them to restructure their classes and schedules to accommodate Internet resources within their classrooms. A management architecture based on network topology information This paper describes the development of a formalism to represent Network Topology Information that provides hierarchical views of the network elements distribution, usable with advantage by management applications. It reviews some topology discovery tools and emphasizes the lack of a common methodology to represent the network cabling and organize the agent distribution information. The formalism, consisting on a descriptive language and on an object-oriented topology base, is described and some application areas, where this information can be used, are pointed. Finally it presents a Management API, as part of the overall architecture, that helps to show the usability of the Topology Information Base. The world-wide web and plant molecular biology The Internet has been functional since 1967 and has been operating without interruption for over 20 years. Although local service can be lost, the network will retain its integrity and recover from almost any imaginable combination of faults, whether natural or intentional. This robustness, the global scope, the availability of quality public domain software, and the inherent democracy of the Internet, have combined to eclipse similar efforts. Many scientists have found the Internet immediately useful for communication in the form of electronic mail. Shared resources, an intrinsic benefit of networks, are now becoming apparent in the form of the World-Wide Web (WWW). Research abstracts  A general purpose neural network simulator system for medical data processing We developed a general purpose neural network simulator system for medical data processing. This system has a flexible network definition language. Users can define arbitrary hierarchical neural networks using the definition language to analyze medical data that contains some complex patterns. The learning algorithm used in this system is back propagation. Learning curves are displayed on multiple windows. This is a general purpose system, so it can be used for various kinds of medical data processing such as one dimensional signal processing or two dimensional image processing. The system can run on a standard UNIX workstation, which is faster and more powerful than most personal computers. The system needs an X window system/Motif Motif and C compiler. These are standard system programs already available on most UNIX workstations. The source code of the system can be retrieved from our anonymous ftp site via Internet. Statistical synchronization among participants in real-time multimedia conferencing In this paper, an algorithm to determine the set of packets generated continuously and periodically from different participants that are arriving at a node either for mixing at the master of a conference, or for simply playing back at a regular participant of a conference, is proposed. The essence of the algorithm is to estimate the expected packet arrival time (or reference time) for each participant. With the reference time at hand, the maximum jitter and the optimum waiting time for a mixer to wait packets from all participants can be determined. An enhancement to improve synchronization which deals with the estimation of the time offsets between the individual periods of the sources and the period of the receiver is also presented. The error of the proposed algorithm is enumerated by the Chernoff bound and demonstrated by simulation and is shown to be acceptable in practical application. The algorithm can also be employed when traffic sources operate with different periods. In-depth modifications of implanted amorphous carbon films Amorphous carbon films (a-C:H) and nitrogen incorporated carbon films [a-C:H(N)] deposited by a self-bias glow discharge have been implanted with 70 keV nitrogen ions at fluences of 0.6, 1 and 2×1017 N/cm2. The in-depth modifications caused by ion implantation were determined by means of nuclear techniques, such as Rutherford Backscattering Spectrometry (RBS), Nuclear Reaction Analysis (NRA) and Elastic Recoil Detection Analysis (ERDA), as well as by Auger Electron Spectroscopy (AES) and Raman scattering. ERDA profiles show that nitrogen implantation causes hydrogen depletion, the amount of which depends on the film composition and on the ion fluence. In a-C:H(N) films nitrogen loss was also measured. The induced structural modifications in both a-C:H and a-C:H(N) films were followed by both AES, using factor analysis, and microprobe Raman spectroscopy. They turn out to be related to the energy deposited by the incident ions. Our results indicate that the ion-beam bombardment causes in both a-C:H and a-C:H(N) films an increase of either the degree of disorder or the ratio between sp2/sp3 bonds across the hydrogen-depleted layer, which depends on the ion fluence. The prehistory of Cyprus: Problems and prospects The archaeological record of prehistoric Cyprus is rich, diverse, well-published, and frequently enigmatic. Regarded by many as a “bridge” between western Asia and the Aegean, Cyprus and its past are frequently seen from scholarly perspectives prevalent in one of those two cultural areas. Its material culture, however, differs radically from that of either area. Apart from the early colonization episodes on the island (perhaps three during the pre-Neolithic and Neolithic), evidence of foreign contact remains limited until the Bronze Age (post-2500 B. C.). This study seeks to present the prehistory of Cyprus from an indigenous perspective, and to examine a series of archaeological problems that foreground Cyprus within its eastern Mediterranean context. The study begins with an overview of time, place, and the nature of fieldwork on the island, continues with a presentation and discussion of several significant issues in Cypriot prehistory (e.g., insularity, colonization, subsistence, regionalism, interaction, social complexity, economic diversity), and concludes with a brief discussion of prospects for the archaeology of Cyprus up to and “beyond 2000”. The medical applications of the internet  All aboard the internet  ABME on the Internet-WorldWideWeb and BMEnet  Are you wired to pharm/bio@internet?  How to access Journal of Clinical Monitoring abstracts on the Internet  Copyright and you  Book reviews  Magnetic field experiment on the Freja satellite Freja is a Swedish scientific satellite mission to study fine scale auroral processes. Launch was October 6, 1992, piggyback on a Chinese Long March 2C, to the present 600×1750 km, 63° inclination orbit. The JHU/APL provided the Magnetic Field Experiment (MFE), which includes a custom APL-designed Forth, language microprocessor. This approach has led to a truly generic and flexible design with adaptability to differing mission requirements and has resulted in the transfer of significant ground analysis to on-board processing. Special attention has been paid to the analog electronic and digital processing design in an effort to lower system noise levels, verified by inflight data showing unprecedented system noise levels for near-Earth magnetic field measurements, approaching the fluxgate sensor levels. The full dynamic range measurements are of the 3-axis Earth's magnetic field taken at 128 vector samples s−1 and digitized to 16 bit, resolution, primarily used to evaluate currents and the main magnetic field of the Earth. Additional 3-axis ‘AC’ channels are bandpass filtered from 1.5 to 128 Hz to remove the main field spin signal, the range is±650 nT. These vector measurements cover Pc waves to ion gyrofrequency magnetic wave signals up to the oxygen gyrofrequency (∼40 Hz). A separate, seventh channel samples the spin axis sensor with a bandpass filter of 1.5 to 256 Hz, the signal of which is fed to a software FFT. This on-board FFT processing covers the local helium gyrofrequencies (∼160 Hz) and is plotted in the Freja Summary Plots (FSPs) along with disturbance fields. First data were received in the U.S. October 16 from Kiruna, Sweden via the Internet and SPAN e-mail networks, and were from an orbit a few hours earlier over Greenland and Sweden. Data files and data products, e.g., FSPs generated at the Kiruna ground station, are communicated in a similar manner through an automatic mail distribution system in Stockholm to PIs and various users. Distributed management of spacecraft operations by the science team is also achieved by this advanced communications system.An exciting new discovery of the field-aligned current systems is the high frequency wave power or structure associated with the various large-scale currents. The spin axis ‘AC’ data and its standard deviation is a measure of this high-frequency component of the Birkeland current regions. The exact response of these channels and filters as well as the physics behind these wave and/or fine-scale current structures accompanying the large-scale currents is being pursued; nevertheless, the association is clear and the results are used for the MFE Birkeland current monitor calculated in the MFE microprocessor. This monitor then sets a trigger when it is greater than a commandable, preset threshold. This ‘event’ flag can be read by the system unit and used to remotely command all instruments into burst mode data taking and local memory storage. In addition,Freja is equipped with a 400 MHz ‘Low Speed Link’ transmitter which transmits spacecraft hcusekeeping that can be received with a low cost, portable receiver. These housekeeping data include the MFE auroral zone current detector; this space weather information indicates the location and strength of ionospheric current systems that directly impact communications, power systems, long distance telephone lines and near-Earth satellite operations. The JHU/APL MFE is a joint effort with NASA/GSFC and was co-sponsored by the Office of Naval Research and NASA/Headquarters in cooperation with the Swedish National Space Board and the Swedish Space Corporation. Erratum: the 2.27 day period of WR-134 (HD 191765) The original temporal analysis of a 12 night spectral timeseries of WR-134 has been found to be flawed and a re-analysis shows that the line profile variations are indeed periodic. When combined with a 4 night timeseries taken 45 days earlier, a period near 2.27 d is found in periodograms of the Heii λ5412 line centroid,rms line width, and line skew variations. When the emission line residuals are ordered as a function of phase, a sinuous feature appears to “snake” about the line center with an amplitude of ± 500 km s−1. This is ≈ 20 larger than the line centroid amplitude; the calculation of which is heavily weighted by static portions of the line profile. In addition to the “snake,” emission residuals appear that move away from line center on unbound trajectories and are thought to result from the interaction of a periodic driver with the unstable flow of the radiation driven wind. The nature of the periodic driver is a topic for discussion. Receiver-initiated communication with ST-II As part of its work on the Heidelberg Transport System (HeiTS) for multimedia communication, the IBM European Networking Center (ENC) has implemented the Experimental Internet Stream Protocol, Version 2 (ST-II). ST-II is a connection-oriented internetworking protocol for multiple-destination communication of real-time data such as digital audio and video. Although ST-II fits well into typical multimedia application scenarios, some functions are still missing No mechanisms are provided for receiver-initiated communication, allowing receivers to join streams, specify their quality of service (QOS) requirements, or initiate stream establishment and resource reservation. In this paper, we present some extensions to tile protocol that make it fulfill the receivers' needs move adequately. Our approach allows intermediate nodes to execute stream management functions on behalf of the sender. This way, the protocol scales better with the number of receivers per stream, as the origin does not need to track every target. All aboard the internet  GUIDS: A graphical user interface development system in UniECAD UniECAD is an integrated electronic CAD system, the user interface development system is the key of the integration of UniECAD. This paper presents the architecture of GUIDS, a graphical user interface development system in UniECAD, and then discusses a series of new techniques and methods in the design and the implementation of this system around the following aspects: the editing environment of interface elements, the implementation of dialogue control and the automatic generation of interface code. As an example, the generation of the main interfaces of UniECAD shows the procedure of the development of user interfaces with this development system. Full or-parallemism and restricted And-parallelism in BTM BTM is a new And/Or parallel execution model for logic programs which exploits both full Or-parallelism and restricted And-parallelism. The advantages of high parallelism and low run time cost make BTJ, an experimental execution system of BTM implemented on a nonshared-memory multiprocessor system, achieve significant speedup for both And-parallel and Or-parallel logic programs. Compiling CIL rewriting language for multiprocessors The high-level Compiler Intermediate Language CIL is a general-purpose description language of parallel graph rewriting computational model intended for parallel implementation of declarative languages on multiprocessor systems. In this paper, we first outline a new Hybrid Execution Model(HEM) and corresponding parallel abstract machine PAM/TGR based on Extended parallel Graph Rewriting Computational Model EGRCM for implementing CIL language on distributed memory multiprocessor systems. Then we focus on the compiling CIL language with various optimizing techniques such as pattern matching, rule indexing, node ordering and compile-time partial scheduling. The experimental results on a 16-node Transputer Array demonstrates the effectiveness of our model and strategies. The learning convergence of CMAC in cyclic learning In this paper we discuss the learning convergence of the cerebellar model articulation controller (CMAC) in cyclic learning. We prove the following results. First, if the training samples are noiseless, the training algorithm converges if and only if the learning rate is chosen from (0,2). Second, when the training samples have noises, the learning algorithm will converge with a probability of one if the learning rate is dynamically decreased. Third, in the case with noises, with a small but fixed learning rate ε the mean square error of the weight sequences generated by the CMAC learning algorithm will be bounded byO(ε). Some simulation experiments are carried out to test these results. Adaptive memory coherence algorithms in DSVM Based on the characteristics of distrubuted system and the behavior of parallel programs, this paper presents the fixed and randomized competitive memory coherence algorithms for distributed shared virtual memory. These algorithms exploit parallel programs' locality of reference and exhibit good competitive property. Our simulation shows that the fixed and randomized algorithms achieve better performance and higher stability than other strategies such as write-invalidate and write-update. Processing real-time transactions in a replicated database system A database system supporting a real-time application has to provide real-time information to the executing transactions. Each real-time transaction is associated with a timing constraint, typically in the form of a deadline. It is difficult to satisfy all timing constraints due to the consistency requirements of the underlying database. In scheduling the transactions it is aimed to process as many transactions as possible within their deadlines. Replicated database systems possess desirable features for real-time applications, such as a high level of data availability, and potentially improved response time for queries. On the other hand, multiple copy updates lead to a considerable overhead due to the communication required among the data sites holding the copies. In this paper, we investigate the impact of storing multiple copies of data on satisfying the timing constraints of real-time transactions. A detailed performance model of a distributed database system is employed in evaluating the effects of various workload parameters and design alternatives on the system performance. The performance is expressed in terms of the fraction of satisfied transaction deadlines. A comparison of several real-time concurrency control protocols, which are based on different approaches in involving timing constraints of transactions in scheduling, is also provided in performance experiments. Applications of object-oriented technology to the integration of heterogeneous database systems The object-oriented paradigm has several features that facilitate the integration of heterogeneous data management systems. One of the main problems in the integration is to provide users with the same data model and language to access very different systems. This problem exists in all kinds of distributed heterogeneous data management systems, independently from their integration architecture (like classical distributed databases, federated databases, multidatabases). This paper shows that the use of an object-oriented data model for building a “uniform” view of several databases can greatly simplify this task, and actually extends the scope of integration towards two directions. The first concerns the integration of data management systems to which traditional integration techniques, based on mappings among data models, cannot be applied. The second direction moves the goal of integration to re-using not only data but to re-using data and application software using these data. In the paper we also briefly discuss some requirements for an object-oriented integrated platform. Generating conformance tests for nondeterministic protocol machines We present a method of generating test cases from the software specifications which are modeled by nondeterministic finite state machines. It is applicable to both non-deterministic and deterministic finite state machines. When applied to deterministic machines, this method yields usually smaller test suites with full fault coverage than the existing methods that also assure full fault coverage. In particular, the proposed method can be used to test the control portion of software specified in the formal specification languages SDL or ESTELLE. Building case-based preliminary design systems: A Hopfield network approach This paper addresses the issue of building a case-based preliminary design system by using Hopfield networks. One limitation of Hopfield networks is that it cannot be trained, i.e. the weights between two neurons must be set in advance. A pattern stored in Hopfield networks cannot be recalled if the pattern is not a local minimum. Two concepts are proposed to deal with this problem. They are the multiple training encoding method and the puppet encoding method. The multiple training encoding method, which guarantees to recall a single stored pattern under appropriate initial conditions of data, is theoretically analyzed, and the minimal number of times for using a pattern for training to guarantee recalling of the pattern among a set of patterns is derived. The puppet encoding method is proved to be able to guarantee recalling of all stored patterns if attaching puppet data to the stored patterns is available.An integrated software PDS (Preliminary Design System), which is developed from two aspects, is described. One is from a case-based expert system—CPDS (Case-based Preliminary Design System), which is based on the algorithm of the Hopfield and developed for uncertain problems in PDS; the other is RPDS (Rule-based Preliminary Design System), which attacks logic or deduced problems in PDS. Based on the results of CPDS, RPDS can search for feasible solution in design model. CPDS is demonstrated to be useful in the domains of preliminary designs of cable-stayed bridges in this paper. Research on decompiling technology Decompiling, as a means of analysing and understanding software, has great practical value. This paper presents a kind of decompiling method offered by the authors, in which the techniques of library-function pattern recognition, intermediate language, symbolic execution, rule-based data type recovery, program transformation, and knowledge engineering are separately applied to different phases of decompiling. Then it is discussed that the techniques of developing expert systems are adopted to build a decompiling system shell independent of the knowledge of language and program running environment. The shell will become a real decompiler, as long as the new knowledge of application environment is interactively acquired. An automatic hierarchical delay analysis tool The performance analysis of VLSI integrated circuits (ICs) with flat tools is slow and even sometimes impossible to complete. Some hierarchical tools have been developed to speed up the analysis of these large ICs. However, these hierarchical tools suffer from a poor interaction with the CAD database and poorly automatized operations. We introduce a general hierarchical framework for performance analysis to solve these problems. The circuit analysis is automatic under the proposed framework. Information that has been automatically abstracted in the hierarchy is kept in database properties along with the topological information. A limited software implementation of the framework, PREDICT, has also been developed to analyze the delay performance. Experimental results show that hierarchical analysis CPU time and memory requirements are low if heuristics are used during the abstraction process. Page-query compaction of secondary memory auxiliary databases Prestoring redundant data in secondary memory auxiliary databases is an idea that can often yield improved retrieval performance through better clustering of related data. The clusters can be based on either whole query results or, as this paper indicates, on more specialized units called page-queries. The deliberate redundancy introduced by the designer is typically accompanied by much unnecessary redundancy among the elements of the auxiliary database. This paper presents algorithms for efficiently removing unwanted redundancy in auxiliary databases organized into page-query units. The algorithms presented here extend prior work done for secondary memory compaction in two respects: First, since it is generally not possible to remove all unwanted redundancies, the paper shows how can the compaction be done to remove the most undesirable redundancy from a system performance point-of-view. For example, among the factors considered in determining the worst redundancies are the update behavior and the effects of a particular compaction scheme on memory utilization. Second, unlike traditional approaches for database compaction which aim merely at reducing the storage space, this paper considers the paging characteristics in deciding on an optimal compaction scheme. This is done through the use of page-queries. Simulation results are presented and indicate that page-query compaction results in less storage requirements and more time savings than could be obtained by standard non-page-query compaction. Quantitative measures for self-organizing topographic maps Self-organizing topographic maps have found many applications as systems capable of unsupervised learning. They are based on the competitive learning algorithm applied to low-dimensional (in practice one, two or three-dimensional) structure of artificial neurons. The iterative algorithm used for competitive learning converges slowly and is computationally very intensive. In this paper, direct mapping on the continuous space based on the minimization principle is used to map the high-dimensional input data to the low-dimensional target space. The problem of finding the best low-dimensional representation of the data is reduced to a minimization problem or to the solution of a system of nonlinear algebraic equations. Status quo and future prospects of the total hospital information system of a Japanese medical college Six years have passed since the total hospital information system of Miyazaki Medical College, nicknamed PHOENIX, began its functions for the first time. It started with order entry systems, and has accomplished various systems, leaving one entry system unfinished; the injection order entry system which will be completed in the near future. It was revealed that the waiting period was most reduced at the hospital pharmacy. The waiting period for the visitors was also reduced. Usefulness of the PHOENIX system was greatly advanced by the function of a unique system of personal computer LAN, nicknamed PALM. This personal computer environments consisted of 200 or more Apple Macintosh computers. In this PALM environment, file servers, CD-ROM MEDLINE, clinical information databases, electronic mails (available in LAN and Internet) and sharing of printers are on service 24 hr a day. Media scaling in a multimedia communication system HeiTS, the Heidelberg Transport System, is a multimedia communication system for real-time delivery of digital audio and video. HeiTS operates on top of guaranteedperformance networks that apply resource reservation techniques. To make HeiTS also work with networks for which no reservation scheme can be realized (for example, Ethernet or existing internetworks), we implement an extension to HeiTS which performs media scaling at the transport level: The media encoding is modified according to the bandwidth available in the underlying networks. Both transparent and nontransparent scaling methods are examined. HeiTS lends itself to implement transparent temporal and spatial scaling of media streams. At the HeiTS interface, functions are provided which report information on the available resource bandwidth to the application so that nontransparent scaling methods may be used, too. Both a continuous and discrete scaling solution for HeiTS are presented. The continuous solution uses feedback messages to adjust the data flow. The discrete solution also exploits the multipoint network connection mechanism of HeiTS. Whereas the first method is more flexible, the second technique is better suited for multicast scenarios. The combination of resource reservation and media scaling seems to be particularly well suited to meet the varying demands of distributed multimedia applications. Health-related quality of life measures for women with urinary incontinence: the Incontinence Impact Questionnaire and the Urogenital Distress Inventory Urinary incontinence (UI) is a relatively common condition in middle-aged and older women. Traditional measures of symptoms do not adequately capture the impact that UI has on individuals' lives. Further, severe morbidity and mortality are not associated with this condition. Rather, Ul's impact is primarily on the health status and health-related quality of life (HRQOL) of women. Generic measures of HRQOL inadequately address the impact of the condition on the day-to-day lives of women with UI. The current paper presents data on two new condition-specific instruments designed to assess the HRQOL of UI in women: the Urogenital Distress Inventory (UDI) and the Incontinence Impact Questionaire (IIQ). Used in conjunction with one another, these two measures provide detailed information on how UI affects the lives of women. The measures provide data on the more traditional view of HRQOL by assessing the impact of UI on various activities, roles and emotional states (IIQ), as well as data on the less traditional but critical issue of the degree to which symptoms associated with UI are troubling to women (UDI). Data on the reliability, validity and sensitivity to change of these measures demonstrate that they are psychometrically strong. Further, they have been developed for simple, self-administration. An educators’ guide to information access across the internet Educational use of the Internet network has exploded in the past few years. Most colleges and universities now provide Internet access to students and educators for instructional and reasearch purposes. Electronic mail, on-line conferencing, software access, and remote login are some of the many services which make it convenient to use the Internet as a global database. Since data is distributed in thousands of computers worldwide, accessing information in an organized manner is sometimes complicated. Client software tools are available that facilitate access to information by providing the user with a more intuitive interface. This paper provides summary information on the use of listservs, telnet, ftp, archie, and gopher application tools which are available to connect to and access data from remote hosts on the Internet. Tearing down the wall: Integrating ISO and internet Management Today, despite “protocol wars” fought with the fervor and rhetoric usually reserved for politics or religion, many network management platform products support both SNMP and CMIP. Increasingly, the challenge is not to pick a single “best” management protocol, but rather to find some way of gluing together a diverse set of networked devices which speak a hodge-podge of standard and proprietary management protocols. In this heterogeneous environment, end-to-end management requires an integrated, unified view of the managed network, despite differences in management protocol and information structure. Integrated management can be facilitated by the development of “proxy” mechanisms and Management Information Base (MIB) translation procedures. Key to achieving timely, effective integrated management is to preserve and leverage from existing commercial investment in both ISO/CCITT and Internet-based management technologies through deployment of common methods and tools which support integration. The Internet  Internet accessto the Journal of Economics and Finance  Cooperative computing and integrated system management: A critical comparison of architectural approaches New information processing service structures based on client-server concepts gain growing importance in both scientific and commercial computing because theyprovide a high flexibility with respect to the positioning of building blocks of a distributed application. The price for this flexibility, however, is a more complex system management. The paper discusses the new management requirements in a cooperative computing environment. The management platforms needed in this context must provide appropriate models for cooperation, services for supporting distributed management applications and an information model as a basis for the modeling of resources relevant to system management. The paper presents and evaluates approaches to solutions existing so far in research and development. All aboard the internet  Collaborative document annotation using electronic mail The primary purpose of this paper is to describe an approach to software development, the small scale approach, that is particularly appropriate for groupware that has a target user population that is truly global. Many of the reasons why the small scale approach is appropriate are described.To support the paper's primary purpose, the domain of document annotation in collaborative writing is used to illustrate the requirements of such global groupware. A simulation shows how the proposed software might be used by individuals and how annotations might be automatically combined. The requirements analysis from this leads to a high level program design which is implemented, for illustration, as a PERL program. Research abstracts  A framework for adding real-time distributed software fault detection and isolation to SNMP-based systems management We consider the problem of fault detection and isolation in systems that consist of real-time distributed cooperating processes. A framework for adding fault detection and isolation capabilities to SNMP-based distributed management systems is presented. The framework revolves around the use of a formal specification model of the cooperating processes which we refer to as “local directed graphs”. We describe the local directed graph model, and a fault monitoring and isolation architecture that implements the framework. In doing so, we address the problem of the size of our formal description and also show that this architecture is suited to the management of internetworks. Lastly, we present an example illustrating the operation of the architecture. Tesseral harmonic coefficients of order 30 from 14 resonant satellite orbit analyses The results from 14 satellite orbit analyses, two of which are new objects, are used to determine individual tesseral harmonic coefficients of 30th-order and even degree. Six C, S pairs are evaluated by solving the equations using a modified least-squares technique. The results are compared with comprehensive geopotential models. The recent models GRIM4-C1, GEM-T3 and JGM-2 emerge well from such tests and are generally closest to the resonance values. A tentative solution is found for four pairs of harmonic coefficients of 30th-order and odd degree. An overview of the astrophysics data system The Astrophysics Data System (ADS) is a distributed data system that provides access to astronomical data and literature information. The data are stored at different data centers. They are searched and retrieved by the users in real time over the Internet The ADS provides access to about 300 catalogs and 4 data archives. In addition about 160,000 abstracts from the astronomical literature can be searched. Besides the data access the ADS provides services such as plotting and image display tools, coordinate conversion tools and table manipulation capabilities. The ADS is available for free to users interested in astronomical data worldwide. Currently there are no restrictions to accessing any of the data in the ADS. The ideology of population control in the UN draft plan for Cairo This paper examines the influence of population control ideology on the draft plan for the UN Cairo Conference on Population and Development. It is argued that this draft plan can only be fully understood in the context of the recent history of the population control movement and of the empirical reality of population control in particular countries. The paper focuses on the origins of the ideology of population control in the eugenics movement initially, and more recently in organisations such as International Planned Parenthood Federation. The role of the United Nations Population Fund (UNFPA), in promoting an incremental approach towards the wider acceptance of population control since the first intergovernmental conference on population in Bucharest in 1974, is outlined. Despite the serious loss of credibility for the UN, through the association of the UNFPA with the Chinese population control programme — the most coercive programme of its type in history — the UN in the draft plan for Cairo continues to promote the ideology of population control. This paper argues for the need to develop a more positive model of development, which acknowledges the complementarity between the lack of development of poorer countries and their potential for significant progress, and the overdevelopment of industrialised regions, whose future growth is increasingly based on intense competition for shrinking markets. Density and size of comet Shoemaker–Levy 9 deduced from a tidal breakup model ALTHOUGH comets have been studied throughout most of recorded history, a detailed understanding of their internal properties is still lacking. Recent observations1 of the split comet Shoemaker–Levy 9—actually a spectacular string of cometary fragments that resulted from the tidal disruption of a single parent body as it passed close to Jupiter2–5—have therefore stimulated much interest, as they provide an unprecedented opportunity to investigate the physical properties of comets more generally6–8. I report here simulations of the tidal breakup of the parent comet, which I assume to have been an assemblage of a large number of spherical components bound together only by gravity. Following the initial tidal disruption of the assemblage, the particles coalesce rapidly by mutual gravitation into a chain of larger fragments, the morphology of which depends critically on the density of the components. By comparing the size, number and distribution of the stimulated fragments with observations of Shoemaker–Levy 9, I determine an average comet density of about 0.5gcm-3 and a parent comet diameter of about 1.8 km. Communication support for distributed collaborative applications The development of distributed, multimedia, collaborative applications requires the resolution of communication issues such as concurrency control and temporal and causal synchronization of traffic over related data streams. Existing transport and/or session-layer protocols do not include the desired support for multistream, multipoint communication. In this paper, we propose new communication abstractions and mechanisms that facilitate the implementation of the necessary coordination and concurrency control semantics in a collaborative application. We propose a protocol suite called themultiflow conversation protocol (MCP) for the realization of these abstractions and describe its prototype implementation in an internetwork of workstations. The paper also describes our experience with the prototype and results of a performance evaluation. Displaying radiologic images on personal computers: Practical applications and uses This is the fifth and final article in our series for radiologists and imaging scientists on displaying, manipulating, and analyzing radiologic images on personal computers (PCs). There are many methods of transferring radiologic images into a PC, including transfer over a network, transfer from an imaging modality storage archive, using a frame grabber in the image display console, and digitizing a radiograph or 35-mm slide. Depending on the transfer method, the image file may be an extended gray-scale contrast, 16-bit raster file or an 8-bit PC graphics file. On the PC, the image can be viewed, analyzed, enhanced, and annotated. Some specific uses and applications include making 35-mm slides, printing images for publication, making posters and handouts, facsimile (fax) transmission to referring clinicians, converting radiologic images into medical illustrations, creating a digital teaching file, and using a network to disseminate teaching material. We are distributing a 16-bit image display and analysis program for Macintosh computers, Dr Razz, taht illustrates many of the principles discussed in this review series. The program is available for no charge by anonymous file transfer protocol (ftp).