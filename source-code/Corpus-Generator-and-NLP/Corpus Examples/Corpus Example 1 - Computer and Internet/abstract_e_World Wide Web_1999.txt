The SGF metadata framework and its support for social awareness on the World Wide Web The widespread use of metadata is transforming the WWW into an information space that can be accessed not only by humans, but also by software agents. In this article, one application for metadata is more closely examined: the description of Web sites structures in a machine understandable way. The Structured Graph Format (SGF) is introduced as an XML‐based format supporting the description of Web spaces as structured graphs. The SGF framework, built around this format specification, is then described. This integrated and extensible set of software components supports the generation, the distribution and the processing of SGF metadata. Three approaches to the problem of generating SGF metadata are compared and highlight a tradeoff between quality and cost. SGF consumers are then presented as components that process the metadata for some purpose. An SGF consumer that uses the metadata to dynamically generate interactive site maps is presented. The discussion then argues for the need to increase social awareness on the WWW. In other words, it raises the issue of monitoring the activity occurring within Web sites. The notion of awareness is first introduced and situated in the context of Computer Supported Cooperative Work (CSCW). Different ways to apply awareness to the WWW are then reviewed. Finally, the SGF framework is described as a valuable foundation for building awareness systems on the Web, with two main advantages. First, because SGF metadata supports the definition of regions within a Web site, at different granularities, it ensures the scalability of monitoring systems. It thus gives users of these systems a very flexible way to define regions of interest and to monitor activity in more meaningful ways. Second, the site maps generated on the basis of SGF metadata provide an efficient way to represent the activity occurring within the monitored site. These explicit representations, which are useful to analyze activity, are contrasted with abstract representations, which are useful to maintain peripheral awareness about ongoing activity on the Web. Mercator: A scalable, extensible Web crawler This paper describes Mercator, a scalable, extensible Web crawler written entirely in Java. Scalable Web crawlers are an important component of many Web services, but their design is not well‐documented in the literature. We enumerate the major components of any scalable Web crawler, comment on alternatives and tradeoffs in their design, and describe the particular components used in Mercator. We also describe Mercator's support for extensibility and customizability. Finally, we comment on Mercator's performance, which we have found to be comparable to that of other crawlers for which performance numbers have been published. Topic Distillation and Spectral Filtering This paper discuss topic distillation, an information retrieval problemthat is emerging as a critical task for the www. Algorithms for this problemmust distill a small number of high-quality documents addressing a broadtopic from a large set of candidates.We give a review of the literature, and compare the problem with relatedtasks such as classification, clustering, and indexing. We then describe ageneral approach to topic distillation with applications to searching andpartitioning, based on the algebraic properties of matrices derived fromparticular documents within the corpus. Our method – which we call special filtering – combines the use of terms, hyperlinks and anchor-textto improve retrieval performance. We give results for broad-topic querieson the www, and also give some anecdotal results applying the sametechniques to US Supreme Court law cases, US patents, and a set of WallStreet Journal newspaper articles. Web-based administration of a personality questionnaire: Comparison with traditional methods The World-Wide Web holds great promise as a mechanism for questionnaire-based research. But are data from Web-based questionnaires comparable to data from standard paper-and-pencil questionnaires? This study assessed the equivalence of the Ruminative Responses Scale in a Web-based format and in a paper-and-pencil format among introductory psychology, upper-level psychology, and non-psychology students. Internal consistency coefficients were comparable across the groups. The participants in the Web sample reported higher levels of self-focused rumination than did the other groups. Women in the Web sample reported more self-focused rumination than did women in the other groups. In the Web sample, results did not covary with access location. These results suggest that findings from Web-based questionnaire research are comparable with results obtained using standard procedures. The computerized Web interface may also facilitate self-disclosure among research participants. Research on the Internet: Validation of a World-Wide Web mediated personality scale Two studies were performed to assess the validity of a World-Wide Web (WWW) measure of self-monitoring. In Study 1, Usenet Newsgroups likely to be read by high and low self-monitors were identified and a comparison was made of the extent to which contributors engaged in a form of self-presentation (use ofhandles orscreen names) likely to be influenced by self-monitoring tendencies. Handles were used significantly more frequently in thehigh self-monitoring Newsgroups, supporting the distinction made. In Study 2, participants recruited through these sets of Newsgroups completed the WWW-mediated test. Those from the high self-monitoring groups scored significantly higher. Self-reports of self-monitoring behavior also reflected scores on the scale. The results are interpreted as demonstrating the construct validity of the instrument used and the viability of criterion-group-oriented methods in Internet-mediated research. Document Categorization and Query Generation on the World Wide Web Using WebACE We present WebACE, an agent for exploring and categorizing documents onthe World Wide Web based on a user profile. The heart of the agent is anunsupervised categorization of a set of documents, combined with a processfor generating new queries that is used to search for new relateddocuments and for filtering the resulting documents to extract the onesmost closely related to the starting set. The document categories are notgiven a priori. We present the overall architecture and describe twonovel algorithms which provide significant improvement over HierarchicalAgglomeration Clustering and AutoClass algorithms and form the basis forthe query generation and search component of the agent. We report on theresults of our experiments comparing these new algorithms with moretraditional clustering algorithms and we show that our algorithms are fastand sacalable. Instructional uses of the WWW: An evaluation tool The development of the World Wide Web (WWW) and the subsequent introduction of different browsers, with their extensions, has changed the Internet from a text‐only communications tool to a powerful multimedia platform whose potential applications are increasing day by day. Research efforts in the computer aided education field are represented by a broad spectrum of applications, from the virtual classroom to remote courses. In these environments, visualising the progress of students in a certain course is an important part of the learning process. At the Universidad Politecnica de Valencia we are experimenting with WWW based software tools and networks for computer aided learning. This research process has resulted in the development of a teacher's authoring tool and an evaluation application; both developed using Java and based on Internet browsers. With this evaluation tool, teachers can easily create questions of different types – which are stored on a database. These questions can later be used to compose different exams or exercises for different students depending on the course and the objective of the examinations. The advantages include an improvement in the fulfillment of the teacher's duties; an increase in the responsiveness of the exam results to the level of student understanding; and the potential for using the application in distance learning and training. Teaching Contemporary Physics Topics Using Real-Time Data Obtained via the World Wide Web As a teaching tool, the World Wide Web (WWW) is unprecedented in its ability to transmit information and enhance communication between scientist and student. Just beginning to be developed are sites that actively engage the user in the learning process and provide hands-on methods of teaching contemporary topics. These topics are often not found in the classroom due to the complexity and expense of the laboratory equipment and the WWW is an ideal tool for overcoming this difficulty. This paper presents a model for using the Internet to teach high school students about plasma physics and fusion energy. Students are given access to real-time data, virtual experiments, and communication with professional scientists via email. Preliminary data indicate that student collaboration and student-led learning is encouraged when using the site in the classroom. Scientist/student mentoring is enhanced with this form of communication. Authoring SMIL documents by direct manipulations during presentation This paper presents Smil‐Editor – an authoring environment to write multimedia documents encoded in SMIL. The main feature of Smil‐Editor is to strongly integrate the presentation view, in which the document is executed, with the editing process. In this view objects can be selected to perform a wide set of editing actions ranging from attributes setting to direct spatial or temporal editions. This way of editing a multimedia document is close to the well‐known WYSIWYG paradigm used by usual word‐processors. Moreover, in order to help the author to specify the temporal organisation of documents, Smil‐Editor provides an execution report displayed through a timeline view. This view also contains information which helps the author to understand why such execution occurred. These multiple and synchronised views aim at covering the various needs for authoring multimedia documents. Feasibility of quality-of-life research on the Internet: A follow-up study Objective: Assessment of the feasibility of conducting clinical research with patients using electronic mail and the World Wide Web. Design: We re-contacted 463 patients with ulcerative colitis (UC) and 154 benign prostatic hyperplasia (BPH) patients who had provided us with email addresses as part of a Web-based study. In an electronic mail message, we informed patients of a web site with a new study and invited them to participate. We then examined the factors associated with patients' participation in the new study. Results: Completion rates were 28% for UC patients (single mailing) and 48% for BPH patients (up to four mailings in a two-month period). Some patients could not be contacted due to invalid email addresses (23%). Those who completed the new study tended to be older, and less time had elapsed since their participation in the previous study. Furthermore, their health-related quality-of-life had significantly improved since the previous study. Conclusion: It is possible to use direct electronic mail contact to conduct follow-up research with patients. Response rates appear to be related to the number of messages sent, age of the recipients, and time since the initial contact. A Web‐based multi‐agent system for interpreting medical images A difficult problem in medical image interpretation is that for every image type such as x‐ray and every body organ such as heart, there exist specific solutions that do not allow for generalization. Just collecting all the specific solutions will not achieve the vision of a computerized physician. To address this problem, we develop an intelligent agent approach based on the concept of active fusion and agent‐oriented programming. The advantage of agent‐oriented programming is that it combines the benefits of object‐oriented programming and expert system. Following this approach, we develop a Web‐based multi‐agent system for interpreting medical images. The system is composed of two major types of intelligent agents: radiologist agents and patient representative agents. A radiologist agent decomposes the image interpretation task into smaller subtasks, uses multiple agents to solve the subtasks, and combines the solutions to the subtasks intelligently to solve the image interpretation problem. A patient representative agent takes questions from the user (usually a patient) through a Web‐based interface, asks for multiple opinions from radiologist agents in interpreting a given set of images, and then integrates the opinions for the user. In addition, a patient representative agent can answer questions based on the information in a medical information database. To maximize the satisfaction that patients receive, the patient representative agents must be as informative and timely as communicating with a human. With an efficient pseudo‐natural language processing, a knowledge base in XML, and user communication through Microsoft Agent, the patient representative agents can answer questions effectively. Face and Contents Validity, and Feasibility of Healthometer: A Delphi Study Our recent descriptive report in this journal of “Healthometer,” an interactive instrument for self-mediated health counseling, was well received in the medical systems research and development community. Suitable for paper as well as electronic mediation and covering both screening, processing/storage, and advisory tasks in a positively reinforcing and confidential way, it has on the whole been considered to be an interesting model in the advancing “information society technologies” perspective; however, a closer scientific and operative evaluation is warranted. Because the power of the instrument in addition to the hardware is critically dependent upon the wet- and software quality, the present study aims at examining in the same forefront forum some basic aspects of all these modalities, with emphasis on face and content validity, feasibility, and users' attitudes and opinions. It can be summarized, quite briefly, that all of them were satisfactory. Introduction: Data Mining on the Internet  Addressing first- and second-order barriers to change: Strategies for technology integration Although teachers today recognize the importance of integrating technology into their curricula, efforts are often limited by both external (first-order) and internal (second-order) barriers. Traditionally, technology training, for both preservice and inservice teachers, has focused on helping teachers overcome first-order barriers (e.g., acquiring technical skills needed to operate a computer). More recently, training programs have incorporated pedagogical models of technology use as one means of addressing second-order barriers. However, little discussion has occurred that clarifies the relationship between these different types of barriers or that delineates effective strategies for addressing different barriers. If pre- and inservice teachers are to become effective users of technology, they will need practical strategies for dealing with the different types of barriers they will face. In this paper, I discuss the relationship between first- and second-order barriers and then describe specific strategies for circumventing, overcoming, and eliminating the changing barriers teachers face as they work to achieve technology integration. Activity periods of an infinite server queue and performance of certain heavy tailed fluid queues A fluid queue with ON periods arriving according to a Poisson process and having a long-tailed distribution has long range dependence. As a result, its performance deteriorates. The extent of this performance deterioration depends on a quantity determined by the average values of the system parameters. In the case when the the performance deterioration is the most extreme, we quantify it by studying the time until the amount of work in the system causes an overflow of a large buffer. This turns out to be strongly related to the tail behavior of the increase in the buffer content during a busy period of the M/G/∞ queue feeding the buffer. A large deviation approach provides a powerful method of studying such tail behavior. Experience with adaptive mobile applications in Odyssey In this paper, we present our experience with application-aware adaptation in the context of Odyssey, a platform for mobile data access. We describe three applications that we have modified to run on Odyssey – a video player, a Web browser, and a speech recognition system. Our experience indicates that it is relatively simple to incorporate applications into Odyssey, and that application source code is not always essential. Although our applications were built without knowledge of each other, Odyssey is able to run them concurrently without interference. However, our experience also exposes important areas of future work. Specifically, it reveals the difficulty of balancing agility with stability in adaptation, and emphasizes the need for controlled exposure of internal Odyssey state to users. “Hebdo-Chim,” a Web Application to Support Learning in a Chemistry Methods Pre-Service Course The potential of the Internet for Education has been enthusiastically reported by many. It is clear, though, that science educators need to be prepared and supported in order to adopt it, as is the case with any innovation. But first they should be motivated; and one way to do this is by letting them experience the benefits of the innovation in their own learning. By showing them appropriate uses, no matter how modest, in teacher preparation courses, we can motivate and influence future science teachers; they, like any teachers, tend to model what is “taught by example.” This paper describes one use of the Web in support of learning in a course in Chemistry Methods for pre-service teachers at Université de Montréal. Designed as a “weekly,” it is a site meant to be an integral, if not central, part of the course. In addition to permanent access to links and course outline, course notes and information relevant to each class were posted every week. An assignment involving a “treasure hunt” on the Web is also described. Gambling and the Health of the Public: Adopting a Public Health Perspective During the last decade there has been an unprecedented expansion of legalized gambling throughout North America. Three primary forces appear to be motivating this growth: (1) the desire of governments to identify new sources of revenue without invoking new or higher taxes; (2) tourism entrepreneurs developing new destinations for entertainment and leisure; and (3) the rise of new technologies and forms of gambling (e.g., video lottery terminals, powerball mega-lotteries, and computer offshore gambling). Associated with this phenomenon, there has been an increase in the prevalence of problem and pathological gambling among the general adult population, as well as a sustained high level of gambling-related problems among youth. To date there has been little dialogue within the public health sector in particular, or among health care practitioners in general, about the potential health impact of gambling or gambling-related problems. This article encourages the adoption of a public health perspective towards gambling. More specifically, this discussion has four primary objectives:1. Create awareness among health professionals about gambling, its rapid expansion and its relationship with the health care system;2. Place gambling within a public health framework by examining it from several perspectives, including population health, human ecology and addictive behaviors;3. Outline the major public health issues about how gambling can affect individuals, families and communities;4. Propose an agenda for strengthening policy, prevention and treatment practices through greater public health involvement, using the framework of The Ottawa Charter for Health Promotion as a guide.By understanding gambling and its potential impacts on the public's health, policy makers and health practitioners can minimize gambling's negative impacts and appreciate its potential benefits. Ambient media for peripheral information display In our everyday lives, we use surrounding living and working environments to arrange physical information artefacts such as books, pictures and calendars. However, when it comes to consumption of computer-based information, this is almost entirely based on attending to screens that separate us from our surroundings. In this paper, we explore the augmentative use of non-computer artefacts in our surroundings for peripheral display of digital information. We discuss system integration of such ambient media, the mapping of information to ambient media, and issues of flexibility and user control. Ambient media have been in everyday use in our work environment, and we report observations from which we conclude their utility as extensions to our digital information spaces. Is there a place for department stores on the Internet? Lessons from an abandoned pilot The potential of Internet retailing is widely promoted yet some retailers have been slow to embrace the technology. Department stores typically have only an information presence on the World Wide Web. This paper describes a pilot project by one of Australia's leading department stores. The case is used to describe certain lessons about Internet retailing and the use of pilot projects. Department stores' decisions whether to adopt electronic retailing are analysed in terms of the opportunities and threats that face them and the capabilities required for success. The conclusion is reached that, as yet, there is no obvious place for established department stores on the Internet. Employment profiles and compensation onsThe authors’ expectations are that AECT members and other educational technologists will use the information presented to establish or increase their own levels of compensation (or the compensation levels of those they supervise). Knowledge of the education levels, experience, salaries, and work motivation factors of fellow educational technologists may enable you to negotiate more effectively and fairly. Problems and Our Solutions for Implementing Telemedicine Systems There are several problems on the practical use of telemedicine, for example, the difficulties involved in promoting communication between medical facilities, uncooperative clinicians, and the absence of high-speed circuits and high-resolution CRT. From the Japanese point of view, we suggest ways to resolve these problems. We will analyze and propose scenarios for realizing successful communications among medical institutions, medical communication and its characteristics, barriers to the promotion of communications among medical institutions, second-opinion centers, and separate satellites and separate circuits. We also mention the World Wide Web for teleconsultation, provision of assistance to people with data handicaps via a communications satellite, and assistance to programs designed for training telemedicine specialists. Using a communication satellite, we offer programs that explain preventive medicine, support activities for nursing at home, explain the risks of fast food, and support activities for the handicapped and women in a simple manner to computer illiterates. A Model of Factors Affecting the Growth of Retailing on the Internet We propose a model that links factors affecting the growth of retailing on the Internet (the Net). Specifically, we examine the roles of the following factors: product-related (risk, efforts, and information intensity); medium-related (interactivity, variety of channels, logical capability, and underlying communication model); consumer-related (preference for home-based shopping, technical orientation, and access to the Net); firm-related (information intensity and expertise in direct marketing), and environment-related (critical mass of consumers and retailers online). We examine how at each stage of the consumption process, Net-based retailing affects value delivery to consumers. Managerial implications of the model are discussed. Virtual marketplaces: Building Management Information Systems for internet brokerage On the World Wide Web (WWW), an increasing number of new trading forms for brokerage of business transactions are emerging. Almost inevitably, central contactpoints on the WWW are being formed, so-called virtual marketplaces, where supply and demand meet. The organisation they require is carried out by a central operator, who offers his brokerage services on a business footing. The aim of this paper is the generation of practical components of a Management Information System (MIS) for such marketplaces that are only accessible online. To do this, the theoretical assumptions of virtual marketplaces are combined with a case study of a German internet-broker for used cars. Examining Space–Time Interaction in City-Level Homicide Data: Crack Markets and the Diffusion of Guns Among Youth This paper examines the linkage between crack market activity and gunhomicide suggested by Blumstein (1995), who argues that the arrival ofcrack stimulated an increased availability of guns among juveniles. Thisgreater availability of guns, the argument continues, is responsible for thesharp upswing in juvenile homicide experienced in the United States in themid-1980s. Using city-level data on crack arrests and gun-related juvenilehomicide, we fit a change-point version of the Bass (1969) model ofinnovation diffusion. We find that, in most large American cities, thediffusion process for crack cocaine experienced an onset of dramatic growththat was followed by a similar, slightly slower growth in gun homicidescommitted by juveniles. We further use cluster analysis to find that thespatial patterning of the two processes is similar, starting on the East andWest Coasts and working their way toward other regions of the nation. Gunuse in homicide among slightly older offenders (ages 18–24) alsoexperienced a change at roughly the same time as the juveniles, but the rateof diffusion was considerably milder than for the younger group; offendersages 25 or older generally show no growth in gun-related homicide whatsoever. In addition, there is no detectable surge in juvenile nongun homicide activity. Based on these findings, we conclude that the crack cocaine markets–gun availability linkage is highly plausible, and we suggest directions for future research in clarifying the dynamics of the late-1980s surge in juvenile homicide. Speed Culture: Fast Strategies in Televised Commercial Ads While various theorists explain the postmodern moment or culture by pointing at an acceleration of transformations in macro-social institutions, such explanations remain often abstract and removed from everyday experience. Seeking to concretize how speed is articulated in popular texts, I analyze here the various strategies TV commercial ads deploy to inscribe speed as a normal and desirable quality of everyday life, objects, and self. Focusing on both pictorial and textual dimensions, I call the reader's attention to these strategies, and discuss the possible social and psychological consequences of the ideological orientations they articulate. Intelligent systems using Web-pages as knowledge base for statistical decision making In this paper, we propose an approach to the construction of an intelligent system that handles various domain information provided on the Internet. The intelligent system adopts statistical decision-making as its reasoning framework and automatically constructs probabilistic knowledge, required for its decision-making, from Web-pages. This construction of probabilistic knowledge is carried out using aprobability interpretation idea that transforms statements in Web-pages into constraints on the subjective probabilities of a person who describes the statements. In this paper, we particularly focus on describing the basic idea of our approach and on discussing difficulties in our approach, including our perspective. A Multimedia World Wide Web Based Conference Minute System for Group Collaboration In this paper, we propose a World Wide Web based conference minute system which preserves important information in the conference as web documents and maintains hyperlinks to related minutes to accelerate the progress of the project. This system, based on our Java application sharing framework, provides capabilities to record and replay the audio/video data of the video conference and operations of the shared applications used in the meeting in a synchronous manner. Through the hyperlinks, people can quickly understand the logic flow of every meeting held in the project duration and select any part in a meeting for replay to know what has actually happened. Furthermore, when a new meeting is held, a minute template is generated by the system to inherit the hyperlinks from its predecessors. Other related minutes and the execution status of the resolutions in the project can be easily accessed through the hyperlinks between them. Through the help of the system, execution of the project and management of its organizational memory could be easily achieved to increase the productivity of the collaborative groups with geographically dispersed members. The life span of a specific topic on the web In this case study a first attempt was made to explore data on the Web for a certain period of time by using bibliometric methods for analysis. The period under investigation was between January 3, 1998 and June 7, 1998. An additional search was carried out on June 20, 1999. The terms used were “informetrics or, informetric”. The results show that substantial changes occurred to the “literature on the Web” on informetrics during this period. Three specific trends were observed: some documents disppeared, new ones were added and some underwent changes. Let (us help) the consumer beware!  Preface The 2nd conference on 'Algal Modelling: Processes and Management' was held at The University of Reading on Monday July 6th 1998. A full capacity of 50 delegates attended, including visitors from overseas and the U.K. water industry. Twelve papers were delivered in two sessions chaired by Prof. Colin Reynolds (Institute of Freshwater Ecology & Visiting Prof. at the University of Reading) and Professor Paul Whitehead (Director of the Aquatic Environments Research Centre at the University of Reading). A key-note address was given by Prof. Steve Chapra (University of Colorado and University of Reading). Selected papers from the conference have been produced and refereed and now form part of this special issue of Hydrobiologia.The conference followed the 1st event held in 1996 and published in Hydrobiologia (Howard, 1997a) and aimed to provide an up-to-date review of the state-of-the-art of algal modelling and its application to the management of problem algal blooms. The problem of toxic cyanobacteria provided the basis for the conference and for the papers presented though attention was given to a wide range of hydrological and biological processes. In recent years, there has been widespread media coverage of the cyanobacterial bloom problem prompted by the rapid deaths of many animals after immersion in water containing toxic cyanobacteria and, in 1996, the apparent fatal poisoning of 45 dialysis patients in Brazil after receiving water supplied by a bloom-affected river. Public concern about the potential threat of toxic cyanobacteria is usually justified. The apparent increase in the frequency of problem blooms may reflect increased monitoring, but is also a symptom of the global eutrophication problem. In England and Wales, the organisation responsible for monitoring and protecting the water environment, the National Rivers Authority (now part of the Environment Agency) established a Toxic Algae Task Group in 1989. At the first conference, the group's Secretary, Dr Ferguson, reviewed the extent of the algal bloom problem (Ferguson, 1997) and justified the group's strategy of encouraging the development of modelling solutions for use in management. The original group report (NRA, 1990) states that 'the use of predictive models to quantify the development of algal blooms in relation to changes in environmental variables should be evaluated and further research initiated so that models can be used to devise management plans for different bodies of water'.Arising from this was the development of the PROTECH (Phytoplankton RespOnse to Environmental Change) series of models (Reynolds Irish, 1997). This volume includes two papers by Elliott et al. that report the further development and validation of the latest version of the model. Findings suggest that the model remains stable when run under a range of different conditions and when applied to the Blelham Tarn enclosure. The workers conclude that PROTECH is a suitable tool for the exploration of community assembly in relation to the ecological themes of competition, succession and biodiversity.Easthope & Howard present an advanced technique called 'general sensitivity analysis' (GSA) as part of their description of the validation of the model, CLAMM. CLAMM predicts the growth and movement of Microcystis under environmentally variable conditions and makes use of advanced computer technology to provide 3-D visualization. In addition to promising GSA results, the model is applied quite successfully to a lowland reservoir over successive years and this application is a significant development upon the earlier SCUM models which were summarized in Howard (1997b).Both models appear to have advanced the earlier versions presented at the first conference. There is perhaps a case for combining the advanced computing-based approach and GSA of Easthope & Howard with the vigorously scrutinised science of Elliott et al. to provide a new level of computer simulation in this field. There is also a case for both models to be reprogrammed in a format that will allow them to be accessible to interested researchers via the World Wide Web. A key focus for discussion during the conference was the potential of making models such as SCUM (Howard, et al. 1996) and PROTECH available on the World Wide Web. The view of certain senior delegates from NERC Institute of Freshwater Ecology and the Environment Agency was that there were perceived problems within such organisations that Web-based models may present problems. These problems include issues of computer system security, quality assurance of model output and control over general usage, particularly the inappropriate application of model software. The real problem according to Howard (1997b) is that we are still at a stage where model use generally means reading and analysing published model algorithms and results in academic papers, and maybe devising a new software implementation of the model. Both the new implementation and the original model software are rarely seen let alone used by other researchers and this remains a hindrance in model development. The non-commercial sharing of software via floppy disk or CD-ROM is inconvenient and relies on users having appropriate hardware and operating systems. A model devised in Visual C++ for Windows 95 will be useless to a research unit operating Unix workstations for example. New languages such as Java are, however, platform-independent, run within a Web browser without need for local installation of files and have been designed to protect the user machine from any possible security risk. When accessing a web page that contains an embedded Java 'applet', the code required to run the applet is downloaded from the remote web server and executed by an interpreter built into the web browser. Language design does not allow for any access to be made by the applet to the local file system and when the Web browser is closed no trace of the applet remains on the local system. The development of versions of software systems for use as Java applets on the Web is becoming routine in commerce and in many academic disciplines, but not, at present, in hydrobiology.Despite the shortcomings in adopting new technology, this volume includes papers that demonstrate real progress in actual scientific modelling. The paper by Frisk et al. is particularly concerned with nutrient dynamics, whilst Krivtsov et al. provide new insights into the phytoplankton of Rostherne Mere. As at the first conference, a key feature has been the inter-disciplinary nature of the research presented with contributions from biologists, engineers, geographers, mathematicians and professional software engineers working in academia and industry. This diverse range of specialists have once again brought new insights to the problems and prospects of “Algal Modelling: Processes and Management”. Designing a college web-based course using a modified personalized system of instruction (PSI) model onWith proper planning and attention to instructional design, Web-based courses can provide a convenient and instructionally sound method of course delivery. The PSI model is a proven model that can be applied to many Webbased courses DeepMatrix – An open technology based virtual environment system In this paper we present DeepMatrix, a shared 3D virtual environment system based on two open technologies, Java and the Virtual Reality Modeling Language (VRML). The system is designed for use on current consumer hardware and requires only a standard Web browser with VRML plug-in. Due to a lean client-server implementation, system performance is superior to comparable approaches and easily extensible. This paper also introduces authoring for DeepMatrix environments and discusses results drawn from a large experimental implementation of widely varying and interconnected virtual worlds. Integrated Video and Text for Content-based Access to Video Databases This paper introduces a new approach to realize video databases. The approach consists of a VideoText data model based on free text annotations associated with logical video segments and a corresponding query language. Traditional database techniques are inadequate for exploiting queries on unstructured data such as video, supporting temporal queries, and ranking query results according to their relevance to the query. In this paper, we propose to use information retrieval techniques to provide such features and to extend the query language to accommodate interval queries that are particularly suited to video data. Algorithms are provided to show how user queries are evaluated. Finally, a generic and modular video database architecture which is based on VideoText data model is described. The Politics of Principal Evaluation This article examines the politics of principal evaluation through both an extensive review of the literature and in-depth interviews with principals and superintendents. The findings reveal that the format and processes used in principal evaluation often vary from one district to another and that principals and superintendents frequently hold different perspectives about the purposes and usefulness of evaluation. Most principals felt their evaluations were not useful and were unduly influenced by political forces beyond their control. Superintendents believed that evaluations were well developed and useful in assessing principal competence. Principals should take an active role in the development and implementation of evaluation processes, while superintendents need to clearly communicate evaluation processes, purposes, and sources of information. Dynamics of Transmission Provision in a Competitive Power Industry The main objective of this paper is to revisit the operations and planning of an electric power system, and, more specifically, of its transmission system. The intent is to formulate the underlying problems as decision-making problems with specific performance objectives. Once this is done, it becomes possible to identify open research questions on this subject, including their dependence on the overall industry structure. The impact of self-similar traffic on network delay The effect of self-similar traffic on the delay of a single queue system is studied through the use of the measured traffic and models as input process. A model-driven simulation-based method is then proposed for the computation of mean line delay in a network design. Both the hybrid-FGN and the FARIMA algorithms have been used to synthesize self-similar sample paths. The comparison results with real-traffic data sets firmly establish the usefulness of the proposed model-driven simulation-based method. A practical database method is also introduced that helps the designer to determine the parameters in network design. This approach may play an important role in network design and analysis. Visualisation and integration of G protein-coupled receptor related information help the modelling: Description and applications of the Viseur program G Protein-Coupled Receptors (GPCRs) constitute a superfamily of receptors that forms an important therapeutic target. The number of known GPCR sequences and related information increases rapidly. For these reasons, we are developing the Viseur program to integrate the available information related to GPCRs. The Viseur program allows one to interactively visualise and/or modify the sequences, transmembrane areas, alignments, models and results of mutagenesis experiments in an integrated environment. This integration increases the ease of modelling GPCRs: visualisation and manipulation improvements enable easier databank interrogation and interpretation. Unique program features include: (i) automatic construction of ‘Snake-like’ diagrams or hyperlinked GPCR molecular models to HTML or VRML and (ii) automatic access to a mutagenesis data server through the Internet. The novel algorithms or methods involved are presented, followed by the overall complementary features of the program. Finally, we present two applications of the program: (i) an automatic construction of GPCR snake-like diagrams for the GPCRDB WWW server, and (ii) a preparation of the modelling of the 5HT receptor subtypes. The interest of the direct access to mutagenesis results through an alignment and a molecular model are discussed. The Viseur program, which runs on SGI workstations, is freely available and can be used for preparing the modelling of integral membrane proteins or as an alignment editor tool. Florida's medicaid mental health carve-out: Lessons from the first years of implementation Florida, like many other states, has embarked on an experiment with managed mental health care for Medicaid enrollees. Under a 1915(b) waiver, the state's Medicaid agency began a mental health carve-out demonstration in March 1996 in the Tampa Bay area. This qualitative case study seeks to ascertain the impact of the carve-out (and, by comparison, HMO arrangements) on the public mental health sector. Findings suggest that the carve-out demonstration has succeeded in creating a fully integrated mental health delivery system with financial and administrative mechanisms that support a shared clinical model. However, other findings raise concerns about the HMO model in terms of stability, access to care, efficiency, and more generally about the shifting of risk and public responsibility “downstream” to private organizations without sufficient governmental oversight. These findings may offer guidance for other states implementing major managed care policy initiatives for disabled Medicaid enrollees. Collective Intelligence and its Implementation on the Web: Algorithms to Develop a Collective Mental Map Collective intelligence is defined as the ability of a group to solve more problems than its individual members. It is argued that the obstacles created by individual cognitive limits and the difficulty of coordination can be overcome by using a collective mental map (CMM). A CMM is defined as an external memory with shared read/write access, that represents problem states, actions and preferences for actions. It can be formalized as a weighted, directed graph. The creation of a network of pheromone trails by ant colonies points us to some basic mechanisms of CMM development: averaging of individual preferences, amplification of weak links by positive feedback, and integration of specialised subnetworks through division of labor. Similar mechanisms can be used to transform the World-Wide Web into a CMM, by supplementing it with weighted links. Two types of algorithms are explored: 1) the co-occurrence of links in web pages or user selections can be used to compute a matrix of link strengths, thus generalizing the technique of &201C;collaborative filtering&201D;; 2) learning web rules extract information from a user&2018;s sequential path through the web in order to change link strengths and create new links. The resulting weighted web can be used to facilitate problem-solving by suggesting related links to the user, or, more powerfully, by supporting a software agent that discovers relevant documents through spreading activation. Sharing lesson plans over the World Wide Web:important components Teachers differ greatly in the way they plan their lessons, using personalised formats and incorporating specific lesson components. A new resource for lesson ideas is the World Wide Web. Literally thousands of lesson plans can be found online. Are lesson plans on the web different from the lesson plans teachers write? What lesson components should be present to support teachers' needs? A study was conducted comparing the components in teacher-generated lesson plans with components found in web-published lesson plans. Results show that teachers use more logistical references in their plans while web-published lessons present more descriptive information. Combining components that teachers find important with components that are important in communicating a clear and concise description of a lesson resulted in a proposed format for web-published lesson plans. Future research should focus on validating the proposed format, investigating if this format can facilitate cross-cultural sharing of lessons, and creating a web-based lesson plan component database that allows teachers to personalise lesson formats. The Parable of Matthew: Identity Politics, Politics of Desire, and the Politics of Performance Matthew Shepard's murder in October 1998 provides an opportunity to examine and reconsider identity politics as they play out around Matthew's body. After a sustained critique of identity politics, the author proposes, in their stead, a politics of performance, which offers a constructive alternative to current political agendas by allowing the simultaneous preservation and erasure of difference. A comparative study of successful aging in three Asian countries The purpose of this research is to examine successful aging in three Asian countries, Indonesia, Sri Lanka and Thailand, using data from the WHO regional studies of Health and Social Aspects of Aging. Successful aging is defined as having no ADL difficulties, and at most, one Nagi difficulty. For the most part, the results are similar to those using more developed populations in that gender, age, no morbid conditions, positive attitude toward one's own aging and the ability to manage money are all correlated with successful aging. An interesting finding is that for the least developed country, Indonesia, being an unskilled worker is significantly associated with increased odds for successful aging. The final section of the paper discusses the theoretical implications regarding a disability transition for elderly populations as countries develop. We also focus on the current political and economic situations in the countries under study, and consider the possible impacts on elderly health. Display and Interactive Languages for the Internet: HTML, PDF, and Java The rapid rise of the Internet has lead to new technologies. These include HTML, the basic ‘markup’ language for pages on the World Wide Web; PDF, a file format designed for precise layout control of documents like working papers and journals, and Java, a general purpose language ideally suited for use on the Internet. This paper introduces these technologies. StarDOM: From STAR format to XML StarDOM is a software package for the representation of STAR files as document object models and the conversion of STAR files into XML. This allows interactive navigation by using the Document Object Model representation of the data as well as easy access by XML query languages. As an example application, the entire BioMagResBank has been transformed into XML format. Using an XML query language, statistical queries on the collected NMR data sets can be constructed with very little effort. The BioMagResBank/XML data and the software can be obtained at http://www.nmr.embl-heidelberg.de/nmr/StarDOM/ Web Log Mining als Marktforschungsinstrument für das World Wide Web  Surfing the Crime Net: Restorative Justice Up until this point, Surfing the Crime Net has directed its attention towards the prevalence and quality of World Wide Web sites on specific criminal justice questions in terms of research interest for academics and practitioners. This edition of Crime Net shifts its perspective to consider some of the issues involved in using the Web as an educational resource and tool, particularly its value for distance and professional learning. There is still a substantive topic discussed, that of restorative justice resources available over the Web, but they will be presented in terms of seeking to consider forms of electronic delivery for learners. Within this context, we will be assessing the justification for employing computer-based learning techniques at a pedagogical level, as well as some of the theoretical and practical considerations involved in structuring electronic delivery in a distance learning format. The overarching theme involves understanding Web-based learning not as an automatic solution to the distance learning question, but as a process or methodology which, when employed appropriately, has the potential to provide learners with a rich, contextual and authentic learning experience about restorative justice. Hydroxyl Radical Concentrations Estimated from Measurements of Trichloroethylene during the EASE/ACSOE Campaign at Mace Head, Ireland during July 1996 During the EASE/OXICOA campaign of the NERC ACSOE programme, trichloroethylene and a wide range of man-made halocarbons and radiatively-active trace gases were monitored with high precision and high frequency throughout July 1996 at Mace Head on the Atlantic Ocean coast of Ireland. Trichloroethylene concentrations in concert with many other trace gases became elevated as regionally-polluted and photochemically-aged air masses reached Mace Head. However, as the anticyclonic air masses retreated during 19 and 20 July, trace gas concentrations remained elevated for a significant period. During this 2–4 day period, trichloroethylene concentrations decayed significantly, though the concentrations of the other more chemically-inert trace gases did not. A detailed interpretation of this behaviour using a Lagrangian dispersion model has allowed the estimation of average and peak OH radical concentrations of 3 and 9×106 molecule cm-3, respectively, during the travel from the source areas in the U.K. and the low countries out to Mace Head. Using a simple box model, the available Mace Head measurements, when combined into a detailed chemical mechanism, generated OH radical concentrations which peaked at 7×106 molecule cm-3, in close agreement with the estimates based on trichloroethylene decay. Tailchaser (Tlc): A new mouse mutation affecting hair bundle differentiation and hair cell survival We have undertaken a phenotypic approach in the mouse to identifying molecules involved in inner ear function by N-ethyl-N-nitrosourea mutagenesis followed by screening for new dominant mutations affecting hearing or balance. The pathology and genetic mapping of the first of these new mutants, tailchaser (Tlc), is described here. Tlc/+ mutants display classic behavioural symptoms of a vestibular dysfunction, including head-shaking and circling. Behavioural testing of ageing mice revealed a gradual deterioration of both hearing and balance function, indicating that the pathology caused by the Tlc mutation is progressive, similar to many dominant nonsyndromic deafnesses in humans. Based on scanning electron microscopy (SEM) studies, Tlc clearly plays a developmental role in the hair cells of the cochlea since the stereocilia bundles fail to form the characteristic V-shape pattern around the time of birth. By young adult stages, Tlc/+ outer hair bundles are grossly disorganised although inner hair bundles appear relatively normal by SEM. Increased compound action potential thresholds revealed that the Tlc/+ cochlear hair cells were not functioning normally in young adults. Similar to inner hair cells, the hair bundles of the vestibular hair cells also do not appear grossly disordered. However, all types of hair cells in the Tlc/+ inner ear eventually degenerate, apparently regardless of the degree of organisation of their hair bundles. We have mapped the Tlc mutation to a 12 cM region of chromosome 2, between D2Mit164 and D2Mit423. Based on the mode of inheritance and map location, Tlc appears to be a novel mouse mutation affecting both hair cell survival and stereocilia bundle development. On Negotiations and Deal Making in Electronic Markets Negotiation has traditionally been an important element in all types of commerce. As electronic commerce systems become generally available on the Internet, there is a need to support negotiation in the context of deal making. However, as in the physical world, the type of negotiation mechanism required is context dependent. In particular, we distinguish between the support required in the context of single deal and support required in coordinating negotiations across multiple deals. A framework is presented to describe deal making and negotiation in the context of a single deal. It is used to illustrate four representative Internet-based automated trading scenarios and to help understand the success of the scenario featuring online auctions. While popular, online auctions are limited in that they permit negotiation only along a single dimension such as price. We present ongoing work on multi-attribute negotiation mechanisms, and outline important new concepts relevant to supporting coordination across multiple deals.