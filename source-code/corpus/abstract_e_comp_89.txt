Defect-induced stabilization of diamond films THE growth of diamond thin films by low-pressure vapour deposi-tion should find technological applications in such diverse fields as hard coatings for cutting tools, lens coatings, heat sinks and electronics1–4. Under such growth conditions diamond should be unstable relative to graphite, yet diamond is formed in practice. Present approaches to describing the growth of diamond explain this paradox by means of a variety of surface kinetic reactions that are controlled by concentrations of adsorbed hydrogen and hydrocarbon radicals5–8. Here we propose a simpler explanation, that high vacancy concentrations are present near the growth face of the diamond film. The formation energy of vacancies in diamond is lower than in graphite, so that large vacancy concentrations (1–8%) can raise the formation energy of graphite above diamond, permitting nucleation and stable growth of diamond. This quasi-thermodynamic model is fundamentally different from the kinetic models for diamond film growth, and predicts that the growth of films with low defect concentrations will be difficult. Memory referencing characteristics and caching performance of AND-Parallel Prolog on shared-memory multiprocessors This paper presents the performance analysis results for the RAP-WAM AND-Parallel Prolog architecture on shared-memory multiprocessor organizations. The goal of this parallel model is to provide inference speeds beyond those attainable in sequential systems, while supporting conventional logic programming semantics. Special emphasis is placed on sequential performance, storage efficiency, and low control overhead. First, the concepts and techniques used in the parallel execution model are described, along with the general methodology, benchmarks, and simulation tools used for its evaluation. Results are given both at the memory reference level and at the memory organization level. A two-level shared-memory architecture model is presented together with an analysis of various solutions to the cache coherency problem. Finally, RAP-WAM shared-memory simulation results are presented. It is argued that the RAP-WAM model can exploit coherent caches and attain speeds in excess of 2 MLIPS with current shared-memory multiprocessing technology for real applications that exhibit medium degrees of parallelism. A survey of dynamic network flows Dynamic network flow models describe network-structured, decision-making problems over time. They are of interest because of their numerous applications and intriguing dynamic structure. The dynamic models are specially structured problems that can be solved with known general methods. However, specialized techniques have been developed to exploit the underlying dynamic structure. Here, we present a state-of-the-art survey of the results, applications, algorithms and implementations for dynamic network flows. Uses and effects of learner control of context and instructional support in computer-based instruction The present study examines uses and effects of learner-control of the context or theme of practice examples on a statistics lesson in combination with learner control of the number of examples examined. Subjects were 227 undergraduate students assigned to 15 treatments formed by crossing five context conditions (learner control, education, business, sports, no-context) with three instructional support conditions (learner-control, maximum, minimum). No differences in achievement were attained as a function of either treatment variable. Findings showed, however, that learners who received preferred contexts (i.e., learner-control-context subjects) selected a greater number of examples than those who received prescribed contexts. In addition, achievement was positively related to the frequency with which subjects varied the number of examples selected across lessons. Despite the absence of achievement benefits, the learner-control-context strategy elicited highly favorable student reactions as a learning orientation. Compilation techniques for a reconfigurable LIW architecture Matching an application to an architecture in structure and size is a way of achieving higher computation speed. This paper presents a combination of a compiler and a reconfigurable long instruction word (RLIW) architecture as an approach to the matching problem. Configurations suitable for the execution of different parts of a program are determined by a compiler, and code is generated for both reconfiguring the hardware and performing the computation. The RLIW machine, consisting of multiple processing and global data memory modules, effectively utilizes the fine-grained parallelism detected in programs by a compiler. The long word instructions control the operation of processing and memory modules in the system. To reduce the data transfer between processing modules and data memory modules, we provide reconfigurable interconnections among the processing modules which permit direct communication. The compiler uses new techniques, including region scheduling, generation of code for reconfiguration of the system, and memory allocation techniques, to achieve improved performance. Algorithms for packing operations into long word instructions and techniques for effectively assigning memory modules to the operands required by an instruction are developed. Results of the experiments to evaluate the system indicate that speedups of 60–300% can be obtained for both scientific and nonscientific programs. The reconfigurable architecture is responsible for much of the speedup. Also, the results indicate that the major problem of memory bottleneck faced in designing parallel systems is successfully attacked. Hypermedia and instruction Hypermedia and its educational applications are discussed in this article. Background information is provided, and software, knowledge representation, navigation, and authoring issues are described. Issues about hypermedia's role in learning also are examined, and hypotheses are advanced about when it may be an effective teaching approach. One conclusion is that hypermedia has strengths and weaknesses and that it may become merely hyped media unless good teaching applications are developed. LINPACK routines based on level 2 BLAS Extended or Level 2 BLAS is intended to improve the performance of portable programs on high-performance computers. In this paper we examine where Extended BLAS routines may be inserted in LINPACK so that no changes in the parameter list have to be made. We also discuss why, for some algorithms, a simple restructuring in terms of Level 2 BLAS fails. We do not attempt to redesign the algorithms or to change the data structure. We concentrate on the translation of calls to the original (Level 1) BLAS into calls to Level 2 BLAS to improve readability, modularity, and efficiency. This examination results in a still portable subset of LINPACK with a better performance than the original routines. The measured performances of original and modified LINPACK routines on the CDC CYBER 990, CDC CYBER 205, CRAY X-MP, and the NEX SX-2 are compared and analyzed. Automated Knowledge Acquisition for Strategic Knowledge Strategic knowledge is used by an agent to decide what action to perform next, where actions have consequences external to the agent. This article presents a computer-mediated method for acquiring strategic knowledge. The general knowledge acquisition problem and the special difficulties of acquiring strategic knowledge are analyzed in terms of representation mismatch: the difference between the form in which knowledge is available from the world and the form required for knowledge systems. ASK is an interactive knowledge acquisition tool that elicits strategic knowledge from people in the form of justifications for action choices and generates strategy rules that operationalize and generalize the expert's advice. The basic approach is demonstrated with a human–computer dialog in which ASK acquires strategic knowledge for medical diagnosis and treatment. The rationale for and consequences of specific design decisions in ASK are analyzed, and the scope of applicability and limitations of the approach are assessed. The paper concludes by discussing the contribution of knowledge representation to automated knowledge acquisition. Actor grammars Actor systems are a model of massively parallel systems based on asynchronous message passing. This paper presents a formalism for a restricted version of actor systems in the framework of graph grammars. To this aim actor grammars are introduced, motivated, and illustrated by examples. Some of the basic properties pertinent to graph transformations in actor grammars are discussed. Automated knowledge acquisition for strategic knowledge Strategic knowledge is used by an agent to decide what action to perform next, where actions have consequences external to the agent. This article presents a computer-mediated method for acquiring strategic knowledge. The general knowledge acquisition problem and the special difficulties of acquiring strategic knowledge are analyzed in terms of representation mismatch: the difference between the form in which knowledge is available from the world and the form required for knowledge systems. ASK is an interactive knowledge acquisition tool that elicits strategic knowledge from people in the form of justifications for action choices and generates strategy rules that operationalize and generalize the expert's advice. The basic approach is demonstrated with a human-computer dialog in which ASK acquires strategic knowledge for medical diagnosis and treatment. The rationale for and consequences of specific design decisions in ASK are analyzed, and the scope of applicability and limitations of the approach are assessed. The paper concludes by discussing the contribution of knowledge representation to automated knowledge acquisition. Hypercube embedding heuristics: An evaluation The hypercube embedding problem, a restricted version of the general mapping problem, is the problem of mapping a set of communicating processes to a hypercube multiprocessor. The goal is to find a mapping that minimizes the length of the paths between communicating processes. Unfortunately the hypercube embedding problem has been shown to be NP-hard. Thus many heuristics have been proposed for hypercube embedding. This paper evaluates several hypercube embedding heuristics, including simulated annealing, local search, greedy, and recursive mincut bipartitioning. In addition to known heuristics, we propose a new greedy heuristic, a new Kernighan-Lin style heuristic, and some new features to enhance local search. We then assess variations of these strategies (e.g., different neighborhood structures) and combinations of them (e.g., greedy as a front end of iterative improvement heuristics). The asymptotic running times of the heuristics are given, based on efficient implementations using a priority-queue data structure. Modelling knowledge and action in distributed systems We present a formal model that captures the subtle interaction between knowledge and action in distributed systems. We view a distributed system as a set ofruns, where a run is a function from time toglobal states and a global state is a tuple consisting of anenvironment state and alocal state for earch process in the system. This model is a generalization of those used in many previous papers.Actions in this model are associated with functions from global states to global states. Aprotocol is a function from local states to actions. We extend the standard notion of a protocol by definingknowledge-based protocols, ones in which a process' actions may depend explicitly on its knowledge. Knowledge-based protocols provide a natural way of describing how actions should take place in a distributed system. Finally, we show how the notion of one protocolimplementing another can be captured in our model. A strategy for coordination of end-user computing in hospital departments This paper discusses issues associated with end-user computing and describes an approach for coordination of departmental end-user computing that was implemented at Concord Hospital, Sydney, Australia. There are a number of pressures for end-user computing. These include the inability of IS departments to provide the services that users require, the increasing spread of computing skills and the advent of the microcomputer. The traditional approaches to computing are discussed and the concern over security explained. The potential benefits of greater involvement of users in systems development and support is discussed. Information systems development needs to be a collaborative activity. At Concord Hospital, a number of departments have responsibility for their own local minicomputer and microcomputer based systems. Coordination of this systems effort was seen as important in order to maximize the benefits of end-user computing while seeking to minimize duplication of effort, data or systems, or security risks. Counselor intervention strategies for computer-assisted career guidance: An information-processing approach  French and soviet experiments in computer literacy: parallels and contrasts  The relationship between learner control of CAI and locus of control among hispanic students This study investigated achievement and motivation effects related to locus of control (internal and external) and three levels of learner control (no control, moderate control, and high control).The 101 seventh-grade and eighth-grade Hispanic subjects were classified as internal or external based on locus of control (loc) scores, blocked by sex and grade, then randomly assigned to the three levels of learner control in separate versions of a CAI instructional program in science.Internalloc subjects did not choose more en route practice than externals and did not perform better under high learner control. Theloc results raise questions about the usefulness of locus of control as it relates to instruction, at least with populations similar to the present one. Symposium overview computer simulation of inorganic solids  symposium on ‘Computer Simulation of Inorganic Solids’, organised by the Applied Solid State Chemistry Group Trust and by the Polar Solids Group of the Royal Society of Chemistry was held in London, in December 1988. Speakers at the forefront of research in computational techniques for simulation of inorganic solid materials reviewed the range of application of these methods and the prospects for future application to a largely non-specialist audience.The meeting was timely and well supported. The majority of the talks given are presented in this volume. This overview discusses the recent expansion of interest in solid state chemistry and the important role that computer simulation methods have played in this growth. The papers are summarised with especial reference to recent developments in solid state chemistry and materials science. A “micro” process planning system based on integer programming for prismatic parts produced on horizontal machining centers An automated process planning system is developed for manufacturing prismatic parts on a Horizontal Machining Center. The goal is to demonstrate the feasibility of using the integer (0–1) programming technique to determine the optimal sequence of machining operations. The final process sequence reflects constraints on tool life. Precedence relationships due to fixturing and/or process requirements are also taken into account. Decision logic for selection of cutting tools and calculation of necessary machining parameters are also developed. Process plans for test workpieces are produced and the effectiveness of the system demonstrated. An operator net model for distributed systems An operator net is a graph consisting of nodes and directed arcs. While operator nets are syntactically similar to dataflow nets, they completely separate the operational semantics from the mathematical semantics. In this paper we define an operational semantics for operator nets that intuitively corresponds to communication in a distributed system. The operational semantics of operator ator nets provide a formal model for a distributed system that is an intermediate point between the actual system and a mathematical model. Abstract properties are expressed using relations on events and messages of an operator net. Corresponding operational specifications can be written using Lucid equations that define a node as a mathematical function on infinite history sequences. The operational specifications are executable and can be easily transformed into a practical implementation of the system. Examples of such specifications are included in the paper. Characterizations of the decidability of some problems for regular trace languages The decidability of the equivalence problem and the disjointness problem for regular trace languages is considered. By describing the structure of the independence relations involved, precise characterizations are given of those concurrency alphabets for which these problems are decidable. In fact, the first problem is decidable if and only if the independence relation is transitive, while the second problem is decidable if and only if the independence relation is a so-called transitive forest. Automated support for building and extending expert models Building a knowledge-based system is like developing a scientific theory. Although a knowledge base does not constitute a theory of some natural phenomenon, it does represent a theory of how a class of professionals approaches an application task. As when scientists develop a natural theory, builders of expert systems first must formulate a model of the behavior that they wish to understand and then must corroborate and extend that model with the aid of specific examples. Thus there are two interrelated phases of knowledge-base construction: (1) model building and (2) model extension. Computer-based tools can assist developers with both phases of the knowledge-acquisition process. Workers in the area of knowledge acquisition have developed computer-based tools that emphasize either the building of new models or the extension of existing models. The PROTÉGÉ knowledge-acquisition system addresses these two activities individually and facilitates the construction of expert systems when the same general model can be applied to a variety of application tasks. On the micro and macro perspectives of stamping engineering Sheet metal engineering is a complex, iterative and interactive process. The whole function of stamping engineering is to select the material, devise the process, the tooling, the set-up and the operational support infrastructure to produce the final part shape, subject to part specifications, budget, and time constraints. There is little application of formal engineering science. It follows from this practice that there is a requirement for extensive die tryout and rework and there is usually little confidence that the sheet steel, the process, and tooling have been optimized. This leads to high cost, long leadtimes, schedule delays and, most important, a lack of control of the whole stamping cycle.The paper provides an overview of stamping engineering from the micro and the macro perspectives. The different engineering tasks are reviewed from a system’s perspective. The role of computer modelling is also examined, with emphasis on the applicability of the different computer tools and need for a broad methodology to support the application of the computer tools. The paper concludes by describing some of the efforts at FTI aimed at formalizing and documenting some of the stamping engineering tasks’ expertise.This work reinforces the need and the necessity for a strong relationship and interaction between the steel supplier, the lubricant supplier, the press supplier, the tooling supplier, and the stamping operation team. The nucleus of future computing  Automated Support for Building and Extending Expert Models Building a knowledge-based system is like developing a scientific theory. Although a knowledge base does not constitute a theory of some natural phenomenon, it does represent a theory of how a class of professionals approaches an application task. As when scientists develop a natural theory, builders of expert systems first must formulate a model of the behavior that they wish to understand and then must corroborate and extend that model with the aid of specific examples. Thus there are two interrelated phases of knowledge-base construction: (1) model building and (2) model extension. Computer-based tools can assist developers with both phases of the knowledge-acquisition process. Workers in the area of knowledge acquisition have developed computer-based tools that emphasize either the building of new models or the extension of existing models. The PROTÉGÉ knowledge-acquisition system addresses these two activities individually and facilitates the construction of expert systems when the same general model can be applied to a variety of application tasks. Technology, teachers, and the search for school reform Educational technologists have increasingly moved away from direct involvement in the world of the classroom teacher in recent years. The assumptions that technologists and teachers make about educational technology and about teaching now diverge markedly. At the same time, new reform proposals have suggested ways to restructure schools that would enhance the role of teachers as instructional decision-makers and offer them more control over their professional work life. Elements of the reform agenda are characterized, and ways are described in which educational technologists might work together with teachers in pursuit of reform goals. These include: (1) preparation of models for teaching-with-technology; (2) design of intelligent software; (3) creation of technologically based tools to support teachers' professional work and development; and (4) improvement of research about technology in education. FMS scheduling as cooperative problem solving This paper describes an FMS scheduling method that treats an FMS as a group of problem-solving agents cooperating to perform manufacturing jobs. The main thrusts of such a method include the ability to handle the dynamically changing production conditions, its taking into account the communication method, the improved reliability, and the use of distributed control. The paper emphasizes research issues associated with various aspects of the cooperative problem-solving method, including: (1) dynamic task assignments, (2) the coordination mechanism, and (3) knowledge-based scheduling as problem solving. A simulation study which compares the performance of the cooperative problem solving approach with that of the more traditional scheduling approaches is also reported. Parallel general prefix computations with geometric, algebraic, and other applications We introduce a generic problem component that captures the most common, difficult “kernel” of many problems. This kernel involves general prefix computations (GPC). GPC's lower bound complexity of Ω(n logn) time is established, and we give optimal solutions on the sequential model inO(n logn) time, on the CREW PRAM model inO(logn) time, on the BSR (broadcasting with selective reduction) model in constant time, and on mesh-connected computers inO(√n) time, all withn processors, plus anO(log2n) time solution on the hypercube model. We show that GPC techniques can be applied to a wide variety of geometric (point set and tree) problems, including triangulation of point sets, two-set dominance counting, ECDF searching, finding two-and three-dimensional maximal points, the reconstruction of trees from their traversals, counting inversions in a permutation, and matching parentheses. Computer-assisted evaluation of respiratory data in ventilated critically ill patients In intensive care unit, a lot of data are currently available but remain unused by nurses and residents because of complexity of analysis. We have developed a system for interpretation of respiratory data (RESPAID) in order to improve monitoring of patients under respiratory support and also to provide a high level of information. RESPAID is a real-time system which interprets quantitative and qualitative aspects of the usual respiratory data at different levels of information. Initial knowledge base was built from data given by four specialists in intensive care. Major attention was paid to different aspects of the system: monitor interface, user interface and time representation. Data are issued from standard respirators and/or monitors used in the intensive care unit. Informations provided by RESPAID are alarm identification, ventilator settings modification and proposal for physiological evolution of the patient or suspected complication.RESPAID runs on IBM PCAT3 with 1st class shell. It is currently in clinical validation procedure. The effect of ordering on preconditioned conjugate gradients We investigate the effect of the ordering of the unknowns on the convergence of the preconditioned conjugate gradient method. We examine a wide range of ordering methods including nested dissection, minimum degree, and red-black and consider preconditionings without fill-in. We show empirically that there can be a significant difference in the number of iterations required by the conjugate gradient method and suggest reasons for this marked difference in performance.We also consider the effect of orderings when an incomplete factorization which allows some fill-in is performed. We consider the effect of automatically controlling the sparsity of the incomplete factorization through drop tolerances and level of fill-in. Machine learning: a survey of current techniques Machine learning is the essence of machine intelligence. When we have systems that learn, we will have true artificial intelligence. Many machine-learning strategies exist, this paper reviews the state of the art in machine learning and provides a glimpse of the pioneers of present machine-learning systems and strategies. Learning in noisy domains, the evolutionary learning, learning by analogy and explanation-based learning are just some of the methods covered. Emphasis is placed on the algorithms employed by many of the systems, and the merits and disadvantages of various approaches. Finally an examination of VanLehn's theory of impasse-driven learning is made. Standardized acquisition of bedside data: The IEEE P1073 medical information bus The absence of standards for medical device communications has stymied the acceptance and success of automated clinical data management systems. Even devices with simple RS-232 data output ports require special interfacing hardware and software. Due to the number and variety of medical devices available, each with their own peculiar data output configuration, it has been impractical to interface with most of them. Limited by manual data entry, most computerized patient data management systems have failed to deliver the productivity gains their users expected.The forthcoming IEEE P1073 Medical Information Bus (MIB) Standard promises to correct this situation with a single powerful bedside device interface method. The MIB will provide specifications for all hardware and software necessary for medical data communications. The MIB handles the need for automatic recognition of new devices placed at a bedside, automatic reconfiguration of the network, binding of a device to a particular patient's bedside and many other issues unique to the medical data communications environment. The MIB is expected to undergo formal IEEE balloting in 1990 and promises to open a new era in data management for clinical patient care. FGCS ’88 onIn every aspect, the conference was successful. We achieved the first two objectives of the conference: to publicize our results, and to provide the opportunity for presenting papers and holding discussions. From the viewpoint of international cooperation, the value of the conference is inestimable. We believe that the final objective of learning from the conference and reflecting it in our future research will be satisfied. An educational technology curriculum for converging technologies In the 1990s, digital encoding will cause different modalities for presenting information to converge. These convergent technologies will require a new kind of educational designer and a realignment of graduate programs. Curriculum revision efforts should address the commonalities inherent in convergent technologies and should incorporate emerging design tools, procedures, and principles. This article reviews issues related to converging technologies and describes how they are affecting the Educational Technology curriculum at San Diego State University. Software modelling of manufacturing systems: A case for an object-oriented programming approach With the recognition of the importance of Computer Integrated Systems (CIM) in improving manufacturing productivity, there is a pressing need for good software modelling approaches to support efficient design and control of manufacturing systems. Software design concepts based on Object-Oriented Programming (OOP) are emerging as powerful techniques for developing large scale software systems. This paper presents important features of object-oriented computing and the relevance of such an approach in modelling and developing software for manufacturing systems. Fault tolerant processes A process is said to be fault tolerant if the system provides proper service despite the failure of the process. For supporting fault-tolerant processes, measures have to be provided to recover messages lost due to the failure. One approach for recovering messages is to use message-logging techniques. In this paper, we present a model for message-logging based schemes to support fault-tolerant processes and develop conditions for proper message recovery in asynchronous systems. We show that requiring messages to be recovered in the same order as they were received before failure is a stricter requirement than necessary. We then propose a distributed scheme to support fault-tolerant processes that can also handle multiple process failures. ODM: An object oriented data model ODM is a new data model thatintegrates the features of object oriented programming languages (e.g. Smalltalk-80) and Relational Data Model (RDM). It extends the data structures and operations of RDM and also provides the features of object oriented programming such as improved semantics, data abstraction, reusability of data structures and codes, and extensibility.We have introduced the concept of ‘u-set’ (uniform set) as an extension of relation of RDM. We employ messages to define an extension of RDM attributes and tuples. Definition of classes for databases, u-sets, and tuples allows us to define new (or modify existing) operations for the databases, u-sets, or tuples. Each database and its elements are u-sets.Au-set is a set of elements ‘conformable’ to a fixed class, namely the ‘base class’ of the u-set. This roughly means that all elements of a u-set support the operations defined in its base class, but they need not have identical data structures. The base class may be any class, the elements of a u-set are not necessarily tuples. This provides an arbitrary deep hierarchy of tuples, u-sets and databases. In particular set-valued, tuple-valued, and derived attributes are supported, and an element of a u-set may be another u-set. Computer simulation of liquid crystals e review recent progress in the computer simulation of liquid crystals, with special emphasis on hard particle models. Surprisingly, the simplest molecular models, taking account only of molecular size and shape, are sufficient to generate a wide variety of liquid crystalline phases, closely analogous to those observed in real life. Thermodynamic stability of different phases is very sensitive to shape, and presumably will also be sensitive to further details of intermolecular interactions as they are incorporated into the model. Realistic atom-atom potential models of liquid crystals are available, but the associated simulations are quite expensive. Thus, while idealized models may be used to study quite general, fundamental properties of mesophases, the modelling of specific liquid crystal systems in a realistic way remains a great challenge. Progress continues to be made on both these fronts. Contributing authors  The capacitated maximal covering location problem with backup service The maximal covering location problem has been shown to be a useful tool in siting emergency services. In this paper we expand the model along two dimensions — workload capacities on facilities and the allocation of multiple levels of backup or prioritized service for all demand points. In emergency service facility location decisions such as ambulance sitting, when all of a facility's resources are needed to meet each call for service and the demand cannot be queued, the need for a backup unit may be required. This need is especially significant in areas of high demand. These areas also will often result in excessive workload for some facilities. Effective siting decisions, therefore, must address both the need for a backup response facility for each demand point and a reasonable limit on each facility's workload. In this paper, we develop a model which captures these concerns as well as present an efficient solution procedure using Lagrangian relaxation. Results of extensive computational experiments are presented to demonstrate the viability of the approach. Geometric and conceptual knowledge representation within a generative model of visual perception A representation scheme of knowledge at both the geometric and conceptual levels is offered which extends a generative theory of visual perception. According to this theory, the perception process proceeds through different scene representations at various levels of abstraction. The geometric domain is modeled following the CSG (constructive solid geometry) approach, taking advantage of the geometric modelling scheme proposed by A. Pentland, based on superquadrics as representation primitives. Recursive Boolean combinations and deformations are considered in order to enlarge the scope of the representation scheme and to allow for the construction of real-world scenes. In the conceptual domain, objects and relationships are represented using KL-ONE, a frame-based knowledge representation formalism which allows hierarchical and structural descriptions. Questions arising from the integration of logical and analogical knowledge representation are also faced; in the end likeness and approximation relationships between objects and prototypical conceptual models for classification purposes are investigated within the framework of fuzzy set theory. Offsets of curves on rational B-spline surfaces The construction of offset curves is an important problem encountered often in design processes and in interrogation of geometric models. In this paper the problem of construction of offsets of curves lying on the same parametric surface is addressed. A novel algorithm is introduced, whose main feature is the use of geodesic paths to determine points of the offset. The offset is then approximated in the underlying surface parameter space by B-splines interpolating data points obtained by traveling a known distance along the geodesics departing from corresponding points of the progenitor in a direction perpendicular to the latter. A comprehensive error checking scheme has been devised allowing adaptive improvement of the approximation of the offset. The applicability of the algorithm is demonstrated by number of numerical examples. Study of protein sequence comparison metrics on the connection machine CM-2 Software tools have been developed to do rapid, large-scale protein sequence comparisons on databases of amino acid sequences, using a data parallel computer architecture. This software enables one to compare a protein against a database of several thousand proteins in the same time required by a conventional computer to do a single protein-protein comparison, thus enabling biologists to find relevant similarities much more quickly, and to evaluate many different comparison metrics in a reasonable period of time. We have used this software to analyze the effectiveness of various scoring metrics in determining sequence similarity, and to generate statistical information about the behavior of these scoring systems under the variation of certain parameters. A strategy for automated modeling of frame structures Structural analysis involves three main phases, namely, modeling, analysis, and results processing. Computer software exists in order to automate the analysis and results processing phases. However, modeling is still done largely by hand, especially for frame structures.This paper considers automated modeling of frame structures in the context of a computer integrated system for structural engineering design. In this system a structure is defined in terms of its components and connections, and it is necessary to create an analysis model in terms of nodes, elements, boundary conditions, and so on. The paper describes the logic of the modeling process, describes the output from the process, and shows how this output can be used as input data for a structural analysis program. The scope is limited to determination of the structure stiffness properties and assembly of the stiffness matrix. Feedback in written instruction: The place of response certitude This paper reviews written feedback from an information-processing perspective. The first section discusses the question of feedback as a reinforcer, and describes the feedback paradigm used as a conceptual guide for the following sections. In the second section we evaluate research on the form and content of feedback. In the last section, a model is developed that applies concepts from servocontrol theory to the feedback sequence. Finally, we report three experiments which support the major predictions of the control model. Genetic algorithms: Foundations and applications Genetic algorithms are defined. Attention is directed to why they work: schemas and building blocks, implicit parallelism, and exponentially biased sampling of the better schema. Why they fail and how undesirable behavior can be overcome is discussed. Current genetic algorithm practice is summarized. Five successful applications are illustrated: image registration, AEGIS surveillance, network configuration, prisoner's dilemma, and gas pipeline control. Three classes of problems for which genetic algorithms are ill suited are illustrated: ordering problems, smooth optimization problems, and “totally indecomposable” problems. Computer modelling: Future directions ecent developments in computing and in the theory of simulation have extended greatly the successes of the modelling of ionic crystals pioneered by Mott and Littleton. This has changed the way in which computer experiments are brought to bear on an increasing range of solid-state phenomena. Yet applied science creates new demands, both in the form of new types of system and in terms of the complexity and subtlety of what is studied. The author's brief survey looks at some of the successes and gaps from interfaces and catalysts to neurotransmitters and from superconductors to slags. Of mice and men — data capture in the clinical environment We have considered the problem of data capture in the critical care area. Review of previous work on the use of alternative input devices for speed and accuracy of data entry yields conflicting information but suggests that requirements depend on input task, data type and skill of the user. We have performed comparative studies of the QWERTY keyboard, cursor control keys, mouse and graphics tablet for data entry in two intensive therapy unit (ITU) environments. The graphics tablet proved overall the best of the devices studied. We report the potential applications of this type of input device. Deadlock-free message routing in multicomputer networks The execution of a concurrent computation by a network of processors requires a routing algorithm that is deadlock free. Many routing algorithms proposed for processor networks have the potential of deadlock due to the cyclic topology of the network. In this paper we first formalize the concept of message routing. Next, we show a method by which a deadlock-free routing algorithm can be constructed out of a given routing algorithm. Finally the method is illustrated by constructing deadlock-free routing algorithms for cartesian product processor networks. An information-theoretic approach to the written transmission of old English Information theory offers a means for analyzing some constraints on the reading and copying process in Old English. Entropy for strings of various lengths offers a baseline measure of the uncertainty involved in transmission of Old English texts, while avoiding the pitfalls of applying models of modern reading to early medieval practice. Analysis of lengthy prose and verse texts in Old English revealed uniformly high values for entropy at all string lengths. High entropies may be the result of the language's irregular orthography, poetic koiné, and several dialects and imply that the language may have been easy to write but difficult to read. The low redundancy of the language which its high entropy values indicate suggests that the reader of Old English played an enhanced role in “decoding” a text and may provide an explanation for the high variability in the transmission of Old English verse.Katherine O'Brien O 'Keeffe is Professor of English at Texas A&M University and a co-director of its Interdisciplinary Group for Historical Literary Study. Books received (brief review) 