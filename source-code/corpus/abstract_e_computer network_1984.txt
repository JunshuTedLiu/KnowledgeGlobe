The paging drum queue: A uniform perspective and further results  uniform perspective for the performance analysis of drums organised around the sector concept under Poisson input is presented which allows further results on their operating characteristics to be naturally derived. Closed-form expressions are provided for (i) the mean device service time, (ii) the device utilisation, (iii) the drum efficiency, (iv) the mean device busy period, and (v) the mean number of page transfers per busy period. Both the FCFS and SLTF scheduling disciplines are studied. When the number of sectors per track is large, it is shown that some of the above quantities could be approximated by extremely simple asymptotic formulae if a suitable time unit is adopted. Generalisations of some of these results which are applicable to the performance analysis of solid-state secondary memory and certain computer communications systems — the polling server and the token ring — are also presented. Simulation output analysis for local area computer networks his paper develops discrete event simulation methods for local area computer networks. We restrict attention to local network simulations with an underlying stochastic process that can be represented as a generalized semi-Markov process. Using a “geometric trials” criterion for recurrence, we establish a regenerative process structure for passage times in this setting. This leads to an estimation procedure for delay characteristics of ring and bus networks. Evaluation des performances du réseau en anneau AMBRE pour distribution d’abonnés mbre est un réseau numérique de distribution d’abonnés dont les caractéristiques principales sont une topologie en anneau, l’intégration de tous les services et la mise en paquets de toutes les données. Chaque communication génère un flux de paquets périodique et déterministe. On étudie le temps d’accès au réseau, le temps de transport des paquets, la perte de caractère périodique des flux en reception.Abstractambre is an integrated services digital loop-like network for public local communication. Each communication is a sequence of fixed-length packets separated by fixed-length silences. Performance evaluation is carried through in terms of access-time and transport delays with emphasis on inter packet delay fluctuations. A dual ascent approach for steiner tree problems on a directed graph The Steiner tree problem on a directed graph (STDG) is to find a directed subtree that connects a root node to every node in a designated node setV. We give a dual ascent procedure for obtaining lower bounds to the optimal solution value. The ascent information is also used in a heuristic procedure for obtaining feasible solutions to the STDG. Computational results indicate that the two procedures are very effective in solving a class of STDG's containing up to 60 nodes and 240 directed/120 undirected arcs.The directed spanning tree and uncapacitated plant location problems are special cases of the STDG. Using these relationships, we show that our ascent procedure can be viewed as a generalization ofboth the Chu-Liu-Edmonds directed spanning tree algorithm and the Bilde-Krarup-Erlenkotter ascent algorithm for the plant location problem. The former comparison yields a dual ascent interpretation of the steps of the directed spanning tree algorithm. Computer aid for gastrointestinal endoscopy units he numerous diagnostic and therapeutic procedures being performed in gastrointestinal endoscopy services are often inadequately recorded and not easily available for retrieval and analysis. This data management can only be provided by the use of computer technology. We have found that the modular, powerful minicomputer answers the needs, standing alone or as part of a computer network. The programs are based on a series of menu-driven prepared options allowing on-line data entry with minimal demands on physician’s understanding of computer technology or finger dexterity. Standardized reports are immediately available and simple analysis possible. Programs for upper and lower endoscopic examinations and colon tumor screening are in clinical trial and other programs will be added. Future progress in gastrointestinal endoscopy will require the better utilization of existing data by computer assistance.RésuméLes nombreux actes diagnostiques et thérapeutiques réalisés dans un service d’endoscopie digestive sont souvent mal enregistrées et difficilement disponibles à des fins d’études et d’analyses. Une gestion correcte des données ne peut être menée qu’avec utilisation de l’informatique. Nous avons pu constater que le mini-ordinateur modulaire, par sa puissance répond tout à fait à nos besoins, qu’il soit utilisé individuellement ou relié à un réseau informatique. Les programmes s’appuient sur une série d’options préétablies (« menu »), permettant une entrée des données en ligne et ne nécessitant pas de la part de l’opérateur de grandes compétences en matière d’informatique ou de frappe sur clavier. Des compte-rendus standardisés sont disponibles immédiatement ainsi que l’analyse simple. Des programmes pour l’exploration endoscopique du tractus digestif supérieur et inférieur ainsi que pour la détection des tumeurs coliques sont actuellement testés. D’autres programmes complémentaires seront ensuiteélaborés. Les progrès à venir en matière d’endoscopie digestive passeront obligatoirement par une meilleure utilisation des données existantes et ce grâce à l’informatique. On the message complexity of distributed problems An important question in distributed computing is the analysis of the influence that the amount of topological information available at the processors has on the complexity of solution algorithms for a given problem. Only few results, typically expressed in terms of lower-bounds, are known to date. In this paper, we prove lower-bounds for two well-known distributed problems: extrema-finding and minimum-weight spanning-tree construction. To establish these results, we introduce the notion of reducibility and equivalence between the elements of a class of distributed problems. We also prove the worst-case optimality of two algorithms. Q & A These questions and answers are drawn from the Human Sexuality medical advisory and information service (GO HSX on the Compu-Serve computer network), of which Mr. and Mrs. Lewis are publishers. Their books includeThe People's Medical Manual, The Electronic Confessional, Sex and Health, The Parent's Guide to Teenage Sex and Pregnancy, andSex Education Begins at Home. Mr. Lewis is a contributing editor of Medical Aspects of Human Sexuality and edits the “Sex Q&A” column for RN Magazine. An inexpensive experiment controller for stand-alone applications or distributed processing networks An inexpensive microprocessor-based experiment controller available as a commercially produced bare printed circuit board is described. It provides for up to 8 inputs and 40 outputs. An 8K Experiment Control BASIC (ECBASIC) and a compiled Experiment Control Language (ECL) are also available. Both languages have internal structures that enable time and responses to be dealt with in a very convenient and error-free manner. Time and response count are automatically incrementing variables that need only to be tested by the user program. ECBASIC provides a 10-msec accuracy, whereas ECL provides a l-msec accuracy. ECL provides extensive support for event-driven programs. The board and software are designed to provide an orderly migration from a stand-alone controller running ECBASIC as a relay-rack replacement, to a network of ECBASIC machines on a small computer, to a true error-checking network of ECL machines on a time-sharing operating system of a large computer. Network communication is over the three wires of a single unmodified RS232 asynchronous serial communication line. White collar productivity: The search for the holy grail The search for increased productivity has led to a great many claims about how it might be accomplished. Nowhere have the claims been more brazen and yet less well supported empirically than those made on behalf of the technologies designed to support office work. The paper examines some of the arguments and claims made, suggesting that most of them are off target. While the new technologies may be of substantial value, the emphasis should beon increased effectiveness, not on greater efficiency. Only then are the visions of the future likely to be fulfilled. Practical Benefits from Standardisation his paper highlights the advantages and disadvantages of adopting standards and briefly describes the present standards scene. The redevelopment of the South West Universities Computer Network is then used as a case study to show the need for a coherent development strategy and well planned transition route when adopting standards in a live user network. Performance Comparison of Local Area Network Architectures There are two major styles of architecture for local area networks
1)Bus Networks;2)Ring Networks.
which themselves are divided into several sub-classes within each major aivision. This paper briefly outlines models of the performance of Ethernet, Cambridge Ring and token ring architectures. The response time achievable using the different architectures is studiea both as a function of network length and as the rate of message submission increases. Protocols and Facilities In this chapter the general framework for the mathematical models to be presented in the remainder of the text is constructed. In consonance with the objectives of this book, only limited detail is presented. We wish to model and analyze rather than to build and operate. If we realize our objective, considerable assistance will be given to the task of building and operating. † The Management of the SERC Network he SERC network is a large X25 network with 11 exchanges and about 160 DTEs. The network and its developement is described elsewhere in this series of papers. Managing a network of this size is a substantial and complicated task. Initially there was little or no management but now considerable effort is going into providing the various facilities needed. There is still considerable developement to do. The literature on ‘practical’ network management is sparse and suitable equipment is even sparser. The UNIVERSE project The introduction of small dish satellite services based on broadcast geostationary satellites has led to a number of experiments designed to exploit the novel features of such a network. The DARPA Wideband Experiment in the US, Project Nadir in France, a proposed German experiment and project UNIVERSE in the UK are all active in this area. These novel features include very low error rates, broadcast transmission, very high data rates but also long delays. The BIBNETT 2 project The BIBNETT 2 project is a joint research project where the main goal is to clarify whether system-to-system communication between independently developed information systems, running on different types of computers, is economic and technical feasible in Norway. Four systems will be connected via an OSI network. Open Systems Interconnection Open Systems Interconnection (OSI) refers to communications between computer systems which can freely exchange information by virtue of their mutual adherence to a set of standards. The standards in question are based on an architechtural model of communications systems, known as the Reference Model for Open Systems Interconnection (OSI). This model is the starting point for looking at OSI, and provides a much needed framework for the orderly development of standards for information interchange. SITA Advanced Telecommunications Services As is now widely known, the world of telecommunications is undergoing a profound mutation and accelerated development with the emergence and introduction of new telecommunications services in all aspects of human and systems communications. The progressive introduction of services, such as Videotex, Teletex, Telefax, Teleconferencing and Videoconferencing in the business, social and individual environment, and the worldwide availability of low cost data communication made possible by the international deployment of data networks and satellite systems, are likely to induce lasting changes in many aspects of human activities, and to have an impact as important in the future as the introduction of computers thirty years ago. Introduction to the Workshop Session on “The Merging of Defence and Civilian Interests in Computer Communications” All of us here have a deep interest in the future of information technology. Many of us are actively working towards a future in which information technology will be a fundamental component of our society’s infrastructure. I am not sure whether we are already in the midst of a technological revolution; but the development of electronics, computing, and telecommunications is indeed bringing about changes in our industrial and social life, and it is particularly affecting the ways that our companies, corporations and government departments conduct their business. If we scientists, engineers and technologists are successful in developing and promoting the ideas that we are discussing at this Advanced Study Institute, there may indeed be a revolution, and we must bear some responsibility for the acceptability of the outcome. Telplanner(TM): An Expert System for Planning Communications Services A case study, “Certain aspects of telecommunication transport tradeoffs”, was done for the International Telecommunication Union (1) in conjunction with their “Telecommunications for Development” project (2). The main purpose of this study was to see if a useful figure of merit for communications, similar to that for computers, could be developed. Convergence of LAN and Digital Telephone Exchange Systems This paper explains the value of integrating all voice and data services under the umbrella of the open systems interconnect (OSI) architecture which is being developed by the International Standards Organisation. On the Influence of the Interaction between Entities for Protocol Description We will follow the architecture for OSI proposed by ISO. Within this architecture, protocols describe the rules and formats of communication among homogeneous entities; they are independent of interaction properties of entities with entities in adjacent layers. The description of functions of the entity for the protocol of the layer N, if it is to be complete, includes the mechanisms used for the interaction with entities in adjacent layers N+1 and N−1. Sane of the factors that are important for the consideration of interaction rules are resource assignement for interaction among entities, its consequence in through put at the service access point N and N−1 and the utilisation of resources within the entity N. The SERC Network — Its History and Development he Rutherford Laboratory of the UK Science and Engineering Research Council (SERC) has provided computing facilities to university located scientists since the early 1960s. The facilities now offered are very varied and are provided on a large number of dissimilar computers located at many sites. To allow easy access to and from these facilities a sophisticated network now exists to interconnect them. This network has developed steadily since the early 1970s and is now a very substantial network with over 160 hosts.It is, in fact, about half the size of the UK public network PSS. This paper describes the development of this network and discusses some of the difficulties encountered and the decision taken from time to time. Communications Services and Distributed System Operations The ISO Open Systems Interconnection Model has provided the basis for discussions at successive NATO Advanced Study Institutes. As implementations of systems exploiting the lower levels of the ISO OSI model become available commercially, the spotlight is tending to turn more toward the shortcomings which make the productive interconnection of heterogeneous computer systems elusive. The diminishing cost of computers, coupled with their increasing power, is inducing an explosion of diverse small systems, many of which are being used for the manipulation of valuable corporate data. The rapid growth of distributed systems is not wholly without risk and, together with today’s focus on information technology, a need for a professional information engineer is becoming apparent. The questions below were offered as a starting point for discussion on some of these issues. Key Management for Data Encipherment Key management is of vital concern in designing a secure data handling system. Various methods have been suggested, but there is, as yet, no move towards formulating a standard. The paper examines four possible systems and shows how each has important areas of application. Open Systems Interconnection Protocols The concept of Open Systems Interconnection (OSI) has now been in existence for several years. The aim of OSI was (and is) to allow the standardization of a small set of general-purpose protocols which would make possible the widespread interconnection of heterogeneous computer systems. The protocols are being developed according to a Reference Model which partitions communications into seven layers. This has permitted parallel development of the protocols for the different layers. The first protocols to emerge are for the transport and session layers. These protocols are crucial to the success of OSI, providing as they do the network- and application- independent connection of systems. The capabilities provided by these protocols and their history, functionality, and status are examined. The Demand and Supply of Telematic Services in Portugal The first part of this contribution quantifies and classifies the computer installations in Portugal, and points out their geographical distribution. We then describe the characteristics of the tradicional data communication circuits provided by the Portuguese PTT.The portuguese packet switching public data network, Telepac, is then described. A mention is made of some national contributions to that network. Finally, some future telematic services that might be offered by the Portuguese PTT are mentioned. Telecommunications-Related Information Services the User’s Point of View n many countries the PTT’s have introduced new non-voice services. Most of them to face unexpected difficulties, the worst of which laying in non-technical problem areas.For the end-user telephony is an integral, rather uncomplicated information service with clear possibilities, limitations, costs, benefits and protocols. In most cases one single supplier is responsible for maintaining a good quality of the entire service.Non-voice telecommunication services are mostly just an element in a more comprehensive information service, which, in the end-users perception, is much more complex. Very often the quality of each element is well-defined but nobody seems to be responsible for the overall quality of the whole.This paper sides with the user’s viewpoint on new telecommunications-related information services, identifying a few problem areas and discussing which approaches could be appropriate there. Authentication and Signature for Message Handling In many data communication applications it is desirable to be able to prove receipt and authenticity of a transmitted document. Encipherment techniques are available which make this possible. The paper gives an outline of the principles involved. Network Management in a Service Environment The South West Universities Regional Computer Centre (SWURCC) was established in 1975 to meet the needs of the ‘big batch’ users at University College Cardiff, the University of Wales Institute of Science and Technology and the Universities of Exeter, Bath and Bristol. All user work reaches SWURCC’s twin ICL 2980 system via X.25 links. Apart from the mixed scientific batch workload a user terminal service based on X.29 offers editing and job submission facilities. Our basic philosophy includes the provision of a consistent and reliable user interface, comprehensive documentation, high quality software, adoption of standards and a strong inclination towards manufacturer supplied and supported solutions rather than the ‘do it yourself’ approach. Information Technology — The Requirements Information technology is difficult to define, at least, it is difficult to limit the definition but it certainly includes office automation, messaging, data processing, industrial and process control, database systems, learning systems and computing. The Transition towards Open Working The computer centres of universities and research institutions in the United Kingdom are being connected together according to OSI principles. Users are already able to access a variety of computer systems from the same terminal and the implementation of non-proprietary standards to transfer files and jobs in progress. This paper outlines the motivation behind the project and its overall structure. It discusses the Choice of standards in advance of internationally ratified specifications and the mechanisms for getting implementations of than on a number of different types of computer. The process of transition towards the new standards currently in progress at a large national centre is also described. Services for Supporting Application Layer Protocols for Distributed Database Systems A kernel providing transaction oriented communication services is proposed as an interface module between distributed database systems and generalized communication systems. In order to support an easy and efficient implementation of application layer protocols for distributed database systems, this kernel provides high level services for transaction oriented message exchange, commit processing, etc. It is argued that the underlying generalized communication system should provide services for connectionless data transmission, multicast and the surveillance of remote sites. Congestion and Flow Control In terms of the open systems interconnection protocol discussed in Section 2.3, the models that we have considered to this point are part of the link level. In this and in the next chapter we develop models for higher-level protocols, primarily at the network level. In this chapter we study congestion and flow control, in the next, routing. Random Access Systems The salient result that emerges from the analyses of polling systems in the previous chapter is the large impact of overhead—particularly at light loading. Because of overhead, performance deteriorates with the number of stations irrespective of the total traffic in the system. On the other hand, as the total traffic in the system increases, the impact of overhead diminishes. All of this may be attributed to the overhead inherent in the action of a central controller. In terms of the effect of overhead on performance similar results apply to TDMA. These properties motivate the consideration of random access techniques in which control is distributed. As we shall see, random access systems are relatively insensitive to the number of active stations in the systems so that performance is best when there are a large number of lightly loaded sources. The difficulty is that these systems are very vulnerable to increases in the level of aggregate traffic. Computational Techniques for Evaluation of Communication System Performance n this paper we review some techniques for evaluating the performance of digital communication systems operating on channels characterized by additive Gaussian noise as well as linear and nonlinear distortions. The parameters considered are the error probability, the minimum distance (useful when a maximum-likelihood sequence receiver is used), the cutoff rate (useful when coding has to be used on the channel), and the spectral occupancy (useful when two or more users share the same frequency band). The emphasis is placed on the computational algorithms that allow these parameters to be evaluated numerically. An Overview of Data Encipherment in the Public Domain This paper traces the development of data encipherment in the public domain, examining the available algorithms, and discusses the way in which standards for applications of data encipherment are developing. It also serves as an introduction to the two associated papers on key management and data signature. Feasibility Studies on New Information Technologies The eighties are foreseen as the dawn of the information age. Information is defined as anything that may change the level of knowledge (entropy) of human beings. The information acts on every sense of the human. In this paper, we shall limit the discussion only to the senses of seeing and hearing. Information is still supplied to people in most cases in conventional ways. Therefore, as more and more people deal with creating information, the amount of information received by anybody has become tremendous. People can’t withstand the information received unless they filter out a large part of it.New information techologies (LT) appear one after the other. People have various possibilities to solve, the same problem. Many of the possible new information technologies are technologies looking for problems and markets.Many countries are in the same quandry.Which information technology (IT) to select? How and where to introduce it? Th i.a is a need to make feasibility studies before the selection process. Sometimes pilot projects are indicated; sometimes they may be skipped.By forming a proper feasibility study team and defining well its framework, significant future outlays can be avoided.This paper suggests possible ways to deal with feasibility studies with the purpose of reaching the proper decisions once such a process is finalized. Issues in Multimedia Computer-Based Message System Design and Standardization Computer-based message systems are increasingly being used for formal and informal communication, supporting the exchange of textual messages. However such nontext media as voice and graphics are very important for human communication. As a result, there is a growing need for the support of media other than text in such systems.This paper addresses various technical issues that will be decisive for the future implementation and standardization of multimedia message systems. Such issues range from new mechanisms for the distribution of multimedia mail to the design of editing tools for nontext media. Names, Addresses, and Directory Services for Computer Mail In an environment of several computer-based message systems (CBMS) interconnecting companies both within and beyond national borders, users would find it quite inconvenient to have to refer to one another by means of system addresses. Furthermore, users operating in a distributed environment may have multiple addresses and change their locations fairly frequently. To support better management of system resources, therefore, a mapping service from names to addresses would be a desirable component.Presented in this paper is a framework for the design of distributed directory systems in internetwork, international CBMS environments. The objective is to provide the requisite mechanisms for mapping from user-oriented naming standards to system-oriented addresses. First a model is introduced to specify the role of directory systems in CBMS environments and to determine which names and addresses are present in such environments. Functional models are then used to illustrate general design criteria for the implementation of distributed directory systems. An Integrated Hospital Computer Network at the University of Leuven Four acute care hospitals and a psychiatry institute are at present integrated in the organization of the University Hospitals of Leuven. The hospitals - St. Rafael, St. Pieter and Gasthuisberg - are located in Leuven city and the others in the nearby villages of Pellenberg and Lovenjoel at a distance of 6 km from Leuven center. A local computer network for the experimental data acquisition at BESSY For the users of the Berlin dedicated electron storage ring for synchrotron radiation (BESSY) a local computer network has been installed: The system is designed primarily for data acquisition and offers the users a generous hardware provision combined with maximum software flexibility. Discrete Event Formalisms and Simulation Model Development The theory and techniques of discrete event modelling and simulation have advanced substantially over the past two decades. An integrative approach, making use of discrete event formalisms, should now be used when developing computer simulations. An important formalism is the DEVS model — a mathematical representation of the class of discrete event systems. Other formalisms, such as modelling strategies, provide a “world view” in which to conceptualize the simulation model.In this chapter, the formalisms are first described. Next, detailed case studies of simulations within three problem domains are considered: (1) insect population dynamics; (2) nuclear waste management; and (3) computer communication networks. For each case study, the formalisms are shown as intimately intertwined in the model formulation and simulation development. Advantages and Disadvantages of Nursing Information Systems in Intensive Care A computer system in intensive care can give the medical staff, the nursing staff and other hospital workers the needed information at the right time and place. It provides the required data accurately and timely. The objectives of the present paper is to describe the main advantages and disadvantages of the computer system, which is in use since three years at the Military Hospital of Brussels. Modeling and analysis of computer and communications systems with queueing networks: An analytical study  Reliable Remote Procedure Calls (Extended Abstract) A very convenient means of arranging communication between ‘client’ and ‘server’ processes in a distributed system is to make use of Remote Procedure Calls (RPC’s) enabling clients to invoke services offered by remote servers and obtain appropriate results. Conceptually, a very simple client-server protocol is needed to implement an RPC mechanism: the client sends its service request as a ‘call’ message to the server, and waits for a reply; the server on the other hand receives the ‘call’ message, performs the service and sends the result as a ‘reply’ message to the client. Despite the apparent simplicity of such a protocol, a number of reliability issues are involved that require careful analysis during the design phase. This paper briefly reviews work done at Newcastle in this area, details of which can be found in [Pan82, Shr82, Shr83, Shr84]. Network Layout and Reliability In all of our previous work, we considered the performance of networks, the exact topologies of which were known. In this chapter we shall move one step back, so to speak, and consider the layout of networks which, broadly defined, minimize costs under a set of user requirements. Pedagogy is the primary reason for postponing the discussion of this topic until the very last chapter of the book. The material in this chapter is a distinct departure since the underlying mathematical models are different from the previous chapters of the book. Much of our previous discussion is based of queueing theoretic models, whereas the material in this chapter is based on circuit theory, in particular graph theory.† Moreover, as we shall see, some of the techniques used in this chapter rely on results of previous chapters. The postponing of the discussion of layout techniques has had little impact on our development. In many applications, the topology is determined by factors other than line costs. In local area networks, for example, topology may be determined by the performance of an accessing technique. Models of the Task Assignment Problem in Distributed Systems The paper presents a model for optimum partitioning of tasks over a multiple-processor system. The minimization of the interprocessor communications overhead and/or the message average delay are considered as a design criterion. The algorithmic approaches to the problem are briefly described and improuvements to the case of multiple copies of tasks are considered. A large set of references covering the area are included. A Computer Assisted Pathology System n view of the increasing number of pathology examinations performed in our hospital (33.000 in 1982) and the pressure to reduce personnel costs, it was decided in 1981 to develop a computer supported system for the pathology department. The main objectives for automation were faster flow of information between reception and signout, online retrieval of patient diagnostic information, improved quality of reports, ease of classification and retrieval of diagnostic categories and automatic billing. These purposes were met in a locally designed and elaborated system in our hospital information system, which is based on two IBM mainframes for administrative and management applications and a network of HP minicomputer system for various laboratory and dedicated clinical application (1–2). The system is operated by secretaries and doctors, and requires no previous knowledge of computer handling. Routing-Flow Allocation As stated in the previous chapter, the two techniques that are used to control congestion within a network are flow control and routing. Hiaving considered flow control we now turn to routing. The goal of routing is to spread the flow throughout the network so as to avoid congestion. Since we are interested in the minimization of congestion, flow control in conjunction with routing must also be considered. As we shall see presently there are models in which routing and flow control can be considered in the same theoretical framework. Imbedded Markov Chain Analysis of Time-Division Multiplexing In general terms, multiplexing is a means for sharing facilities among a number of users and sources. As we have seen in Section 2.4, the standard techniques for doing this in the telephone networks are frequency-division multiplexing (FDM) and time-division multiplexing (TDM). The explosive growth of digital technology has favored the development of TDM for sharing the capacity of transmission lines. Moreover, the digital basis of time-division multiplexing makes it a natural vehicle for data traffic. In this chapter we shall analyze the performance of time-division multiplexing and a variant, asynchronous time-division multiplexing. This analysis is closely related to the analysis of the M/G/1 queue in the previous chapter inasmuch as both use the imbedded Markov chain approach.