Teletraffic and the queuing theory The term “mass service theory” introduced by A.Ya. Khinchin implies that the probability theory is applied to the studies of diverse problems of servicing which often are related to formation of queues. The problems of design and calculation of the communication networks and their elements are pivotal here. In the shop language of the communication community this domain of knowledge is called the teletraffic theory. The first publication of A.K. Erlang whose centenary is celebrated this year was devoted namely to this theory. It is not occasional that this jubilee is observed. Today the mankind has the worldwide communication network offering fabulous possibilities in information transmission, storage, and processing. Creation of such a network would not be possible without wide use of the teletraffic theory. Convivial software: an end-user perspective on free and open source software The free and open source software (Foss) movement deserves to be placed in an historico-ethical perspective that emphasizes the end user. Such an emphasis is able to enhance and support the Foss movement by arguing the ways it is heir to a tradition of professional ethical idealism and potentially related to important issues in the history of science, technology, and society relations. The focus on software from an end-user’s perspective also leads to the concept of program conviviality. From a non-technical perspective, however, software is simply a new example of technology, and the effort to assure that technology is developed in a socially responsible manner has a significant history. The argument thus begins with observations about the history of technology. This leads to critical reflections on the development of professional engineering ethics, and to a discussion of the alternative technology movement. Finally, it concludes by indicating some criteria to consider when imagining the design of convivial software. Time and British politics: Memory, the present and teleology in the politics of New Labour This paper seeks to consider the New Labour project using insights drawn from a growing literature on temporality. The paper first explores New Labour's partisan and complex mobilisation of memory. It is argued that the disarticulation of the party from its past was integral to the notion of the Third Way and initially served to recast the identity of the party. However, as New Labour evolved the mobilisation of memory shifted to generate support for its contentious programme of modernisation. New Labour also held as distinctive and as politicised a view of the present as it did of the past. Its construction of its present is explored, particularly its claims to present a measured response to a new hyperactive economic, social and political epoch and its record in accelerating the speed at which politics is conducted. The paper concludes by identifying how these constructions of past and present converge and offer explanation for the difficulties in identifying the legacy of Tony Blair, and more broadly the absence of teleology in the New Labour project. Knowledge management: a review of the field and of OR's contribution This paper examines the field of knowledge management (KM) and identifies the role of operational research (OR) in key milestones and in KM's future. With the presence of the OR Society journal Knowledge Management Research and Practice and with the INFORMS journal Organization Science, OR may be assumed to have an explicit and a leading role in KM. Unfortunately, the origins and the evidence of recent research efforts do not fully support this assumption. We argue that while OR has been inside many of the milestones there is no explicit recognition of its role and while OR research on KM has considerably increased in the last 5 years, it still forms a rather modest explicit contribution to KM research. Nevertheless, the depth of OR's experience in decision-making models and decision support systems, soft systems with hard systems and in risk management suggests that OR is uniquely placed to lead future KM developments. We suggest that a limiting aspect of whether OR will be seen to have a significant profile will be the extent to which developments are recognized as being informed by OR. Greedy distributed optimization of multi-commodity flows The multi-commodity flow problem is a classical combinatorial optimization problem that addresses a number of practically important issues of congestion and bandwidth management in connection-oriented network architectures. We consider solutions for distributed multi-commodity flow problems, which are solved by multiple agents operating in a cooperative but uncoordinated manner. We provide the first stateless greedy distributed algorithm for the concurrent multi-commodity flow problem with poly-logarithmic convergence. More precisely, our algorithm achieves $${1+\varepsilon}$$ approximation, with running time $${O(H{\cdot} \log^{O(1)}m{\cdot} (1{/}\varepsilon)^{O(1)})}$$ where H is the number of edges on any allowed flow-path. No prior results exist for our model. Our algorithm is a reasonable alternative to existing polynomial sequential approximation algorithms, such as Garg–Könemann (Proceedings of the 39th Annual Symposium on Foundations of Computer Science, Palo Alto, CA, USA, pp. 300–309, 1998). The algorithm is simple and can be easily implemented or taught in a classroom. Remarkably, our algorithm requires that the increase in the flow rate on a link is more aggressive than the decrease in the rate. Essentially all of the existing flow-control heuristics are variations of TCP, which uses a conservative cap on the increase (e.g., additive), and a rather liberal cap on the decrease (e.g., multiplicative). In contrast, our algorithm requires the increase to be multiplicative, and that this increase is dramatically more aggressive than the decrease. Was the Internet the Only Option? Which Way Should Business and Information Systems Engineering Go? In global competition, the Internet turned out to be the single and hegemonial infrastructure for communication. It has become the “nervous system” of today’s networked economy. While the first phase provided communication services, like e-mail, the WWW has established an interactive platform to allow easy access to advanced services. Now, in its “third” or cooperative phase, the Internet will finally lead to a ubiquitous informatization where business processes and applications become interleaved beyond the boundaries of enterprises. For this phase, many analogies to the emergence of the Internet can be observed. War Internet die einzige Option? Welchen Weg soll die Wirtschaftsinformatik gehen? fassungIn den 80er- und 90er-Jahren des vorigen Jahrhunderts hat das Internet den Sieg über alle anderen damals noch möglichen Optionen der Telekommunikation errungen. Heute ist es das unumstrittene und hegemoniale „Nervensystem“ einer vernetzten Wirtschaft. Dies ist vor allem eine Folge des Innovationsschubes, den das Internet ungebrochen auslöst. In seiner ersten Phase standen einfache Kommunikationsdienste, wie E-Mail, im Zentrum der Nutzungsnachfrage. Während das WWW die Grundlage für die heutige Internetökonomie schuf, wird in der nun aktuellen dritten oder kooperativen Phase eine ubiquitäre Informatisierung der Wirtschaft und Gesellschaft durch die Vernetzung von Prozessen und Anwendungen eingeleitet. Vor allem dafür fallen Analogien zur Entstehung des Internets auf.AbstractIn global competition, the Internet turned out to be the single and hegemonial infrastructure for communication. It has become the “nervous system” of today’s networked economy. While the first phase provided communication services, like e-mail, the WWW has established an interactive platform to allow easy access to advanced services. Now, in its “third” or cooperative phase, the Internet will finally lead to an ubiquitous informatization where business processes and applications become interleaved beyond the boundaries of enterprises. For this phase, many analogies to the emergence of the Internet can be observed. e-Commerce 
              Electronic commerce (e-Commerce) is fast becoming a way of life in the 21st century. As more and more consumers and organizations resort to electronic means for conducting purchases and facilitating business transactions, it has become vital for academic researchers as well as practitioners to understand its workings, and be able to analyze problems and rectify weaknesses. This is complicated by the fact that the e-Commerce market is global and constantly evolving, with new and innovative business models and products being introduced at a rapid pace. We present an overview of the status of e-Commerce, with particular emphasis on academic research. Specifically, we review the historical background of e-Commerce, discuss theoretical frameworks, and examine e-Commerce models. We also discuss emerging trends as well as pressing issues facing the e-Commerce marketplace.
             Management of Variable Data Streams in Networks This work is motivated by the observation that traffic in large networks like the Internet is not controlled by a central authority but rather by a large number of selfish agents interacting in a distributed manner. Game theory predicts that selfish behaviour in such a system leads to a Nash equilibrium, but it does not, in general, explain, how such an equilibrium can be reached. We focus on this dynamic aspect.In the first part of this survey, we develop a dynamic model of selfish, adaptive behaviour in routing networks. We show how and under which conditions such behaviour can give rise to a stable state and analyse the convergence time. Based on these results we design a distributed algorithm to compute approximate equlibria.In the second part, we build upon the theory developed so far in order to design an online traffic engineering protocol which proceeds by adapting route weights on the time scale of seconds. We show that our protocol converges quickly and significantly improves network performance. OSPF for Implementing Self-adaptive Routing in Autonomic Networks: A Case Study Autonomicity, realized through control-loop structures operating within network devices and the network as a whole, is an enabler for advanced and enriched self-manageability of network devices and networks. In this paper, we argue that the degree of self-management and self-adaptation embedded by design into existing protocols needs to be well understood before one can enhance or integrate such protocols into self-managing network architectures that exhibit more advanced autonomic behaviors. We justify this claim through an illustrative case study: we show that the well-known and extensively used intra-domain IP routing protocol, OSPF, is itself a quite capable self-managing entity, complete with all the basic components of an autonomic networking element like embedded control-loops, decision-making modules, distributed knowledge repositories, etc. We describe these components in detail, concentrating on the numerous control-loops inherent to OSPF, and discuss how some of the control-loops can be enriched with external decision making logics to implement a truly self-adapting routing functionality. How is speech processed in a cell phone conversation? Although most people see the cell phone as an extension of conventional wired phone service or POTS (plain old telephone service), the truth is that cell phone technology is extremely complex and a marvel of technology. Very few people realize that these small devices perform hundreds of millions of operations per second to be able to maintain a phone conversation. If we take a closer look at the module that converts the electronic version of the speech signal into a sequence of bits, we see that for every 20 ms of input speech, a set of speech model parameters is computed and transmitted to the receiver. The receiver converts these parameters back into speech. In this chapter, we will see how linear predictive (LP) analysis- synthesis lies at the very heart of mobile phone transmission of speech. We first start with an introduction to linear predictive speech modeling and follow with a MATLAB-based proof of concept. Multilingual Agents in a Dynamic Environment Experiments conducted by the Knowledge-Based Intelligent Information and Engineering Systems (KES) Centre use Java to gain its many advantages, especially in a distributed and dynamically scalable environment. Interoperability within and across ubiquitous computing operations has evolved to a level where plug ‘n′ play protocols that invoke common interfaces, provide the flexibility required for effective multi-lingual communications. One example includes: dynamic agent functionality within simulations that automatically adapt to incoming data and/or languages via scripts or messaging to achieve data management and inference. This has been shown using demonstrations at the Centre herein. Many aspects of the model involve web centric transactions, which involve data mining or the use of other types of Intelligent Decision Support System (IDSS). Section One of this paper provides an introduction, Section Two introduces the basic concepts of Decision Support System (DSS), Section Three discusses Intelligent Decision Support System (IDSS) enhancements, Section Four explains how agents use a multi-lingual dynamic environment,while Section Five highlights conclusions and future research direction. Routing Metrics for Wireless Mesh Networks Routing in wireless mesh networks has been an active area of research for many years, with many proposed routing protocols selecting shortest paths that minimize the path hop count. Whereas minimum hop count is the most popular metric in wired networks, in wireless networks interference- and energy- related considerations give rise to more complex trade-offs. Therefore, a variety of routing metrics has been proposed especially for wireless mesh networks providing routing algorithms with high flexibility in the selection of best path as a compromise among throughput, end-to-end delay, and energy consumption. In this paper, we present a detailed survey and taxonomy of routing metrics. These metrics may have broadly different optimization objectives (e.g., optimize application performance, maximize battery lifetime, maximize network throughput), different methods to collect the required information to produce metric values, and different ways to derive the end-to-end route quality out of the individual link quality metrics. The presentation of the metrics is highly comparative, with emphasis on their strengths and weaknesses and their application to various types of network scenarios. We also discuss the main implications for practitioners and identify open issues for further research in the area. Towards a Framework for Evolvable Network Design The layered Internet architecture that had long guided network design and protocol engineering was an “interconnection architecture” defining a framework for interconnecting networks rather than a model for generic network structuring and engineering. We claim that the approach of abstracting the network in terms of an internetwork hinders the thorough understanding of the network salient characteristics and emergent behavior resulting in impeding design evolution required to address extreme scale, heterogeneity, and complexity. This paper reports on our work in progress that aims to: 1) Investigate the problem space in terms of the factors and decisions that influenced the design and development of computer networks; 2) Sketch the core principles for designing complex computer networks; and 3) Propose a model and related framework for building evolvable, adaptable and self organizing networks We will adopt a bottom up strategy primarily focusing on the building unit of the network model, which we call the “network cell”. The model is inspired by natural complex systems. A network cell is intrinsically capable of specialization, adaptation and evolution. Subsequently, we propose CellNet; a framework for evolvable network design. We outline scenarios for using the CellNet framework to enhance legacy Internet protocol stack. Token Traversal Strategies of a Distributed Spanning Forest Algorithm in Mobile Ad Hoc - Delay Tolerant Networks This paper presents three distributed and decentralized strategies used for token traversal in spanning forest over Mobile Ad Hoc Delay Tolerant Networks. Such networks are characterized by behaviors like disappearance of mobile devices, connection disruptions, network partitioning, etc. Techniques based on tree topologies are well known for increasing the efficiency of network protocols and/or applications, such as Dynamicity Aware - Graph Relabeling System (DA-GRS). One of the main features of these tree based topologies is the existence of a token traversing in every tree. The use of tokens enables the creation and maintenance of spanning trees in dynamic environments. Subsequently, managing tree-based backbones relies heavily on the token behavior. An efficient and optimal token traversal can highly impact the design of the tree and its usage. In this article, we present a comparison of three distributed and decentralized techniques available for token management, which are Randomness, TABU and Depth First Search. Detecting the Minimal  Bernie Cosell In 1969 when the first two nodes of the ARPANET—the network that would become the core of the Internet—came on line, every packet that flowed over 50 kilobit/second leased lines was routed through two specialized computers called Interface Message Processors, or IMPs. The IMPs were designed and built by Bolt Beranek and Newman (BBN), and the software that ran the IMPs had been written by a team of three programmers, one of whom was Bernie Cosell, who had left MIT three years before, at the beginning of his junior year, to join BBN. Architecture, Infrastructure, and Broadband Civic Network Design: An Institutional View Cultural values frame architectures, and architectures motivate infrastructures — by which we mean the foundational telecommunications and Internet access services that software applications depend on. Design is the social process that realizes architectural elements in an infrastructure. This process is often a conflicted one where transformative visions confront the realities of entrenched power, where innovation confronts pressure from institutionalized interests and practices working to resist change and reproduce the status quo in the design outcome. We use this viewpoint to discuss design aspects of the Urban-net, a broadband civic networking case. Civic networks are embodiments of distinctive technological configurations and forms of social order. In choosing some technological configurations over others, designers are favoring some social structural configurations over alternatives. To the extent that a civic network sets out to reconfigure the prevailing social order (as was the case in the Urban-net project considered here), the design process becomes the arena where challengers of the prevailing order encounter its defenders. In this case, the defenders prevailed and the design that emerged was conservative and reproduced the status quo. What steps can stakeholders take so that the project’s future development is in line with the original aim of structural change? We outline two strategies. We argue the importance of articulating cultural desiderata in an architecture that stakeholders can use to open up the infrastructure to new constituents and incremental change. Next, we argue the importance of designing the conditions of design. The climate in which social interactions occur can powerfully shape design outcomes, but this does not usually figure in stakeholders’ design concerns. Ethernet Applications and the Future of the Microcontroller Introduction to the TCP/IP protocol and related protocols necessary to build a web server. Review an advanced networking project freely available on the Circuit Cellar® website that details how to build an 8051 web server.1 This project uses a mid-range 8051 and custom, partially implemented protocol stack. Review the Silicon Labs Ethernet Development kit. This kit uses a high-end 8051, SI’s TCP/IP Configuration Wizard and a third-part protocol stack. Demonstrate the use of AJAX technology to dynamically update data in a web page. Consider the future of the microcontroller.This chapter introduces the TCP/IP protocol and then presents two networked applications for the 8051. One project is a “from scratch” hobbyist design and the other is a “rapid prototype” design that uses more expensive hardware and third-party software. This chapter concludes with a look forward to the future of the microcontroller and embedded software development. Setting Up Mail In terms of Internet history, electronic mail is stone old. When the World Wide Web was conceived in the early 1990s, e-mail already had reached its age of maturity. The first electronic mail between two networked computers was sent in 1971, over the Internet’s predecessor, ARPANET. Even though e-mail predates most other applications you use on the Internet—with the exception of the File Transfer Protocol (FTP), which is about as old—e-mail is still one of the most popular functions of the Internet today (and this is not only because someone somewhere chose you to help him get his inheritance out of the country). Even with spam mails rising to levels of 70 to 90 percent of all mails sent, most Internet users write and receive mails on a daily basis. Distributed Query Processing  IP-Based Wireless Heterogeneous Networks ary mode of voice communication for a long time has been with 8 KHz band limited analog speech signals over circuit switched path. With advances in digitization of analog information, digital transmission and digital switching, and advances in computer technology, fully digital end-to-end voice communication over 64 kbps circuit switched path was possible. Such networks commonly known as public switched telephone networks (PSTN), and used ‘out-of-band’ signalling, commonly known as signalling system 7 (SS7).Data communication in the beginning had a more over local area networks (LAN), connecting terminals to a central computer. While the wide area PSTN networks are circuit switched, the LAN-s that carried inherently digital bursty data, evolved with a packet switched character. Wide area data communication needs primarily for text and fax, was achieved by modulating the digital data over the analog waveform band-limited to 8 kHz, and transmitted and switched over the same PSTN network. The resulting digital channel speed was obviously very slow- from 300 bps onwards. As the possibility of multiplexing and high speed switching of digital data steams were realisably, convergence of networks were first envisaged in the conceptualization of circuit switched Integrated Services Digital Networks or ISDN (also known as narrowband-ISDN). N-ISDN was designed to carry real-time and conversational services, but the digital channels are naturally capable also to carry non-conversational and non-real-times data services. The need for a wide area packet data network (PDN) was specifically addressed by the International Standardization Organization (ISO) by coming up with a seven layer Open System Interconnection (OSI) model. This model can be applied to both circuit-switched packet data networks (CSPDN) or packet switched packet data networks (PSPDN). The seven layer ISO/OSI model as shown in Figure 2.1 provides the concept of peer-to-peer communication between protocol layers. This is facilitated by encapsulating a protocol entity or protocol data unit (PDU) of a higher layer by a header and a trailer of a lower and then passing on to the next lower layer. Actual communication between systems happens only over the physical layer (commonly known as the PHY). The present day Internet was initiated through an initiation by the United States Department of Defence Advanced Research Projects Agency (ARPA) of a wide area packet switched data network ARPANET. Internet is described next. Divided by a common acronym: On the fragmentation of CSCW CSCW is in an advanced state of fragmentation. The acronym now, by and large, denotes widely diverging research programs that, apart from a shared name, have little or nothing in common. This situation obviously calls for clarification. Recounting the prehistory and formation of CSCW, the paper shows that CSCW, as a distinct research program devoted to the development of new technologies on the basis of understanding actual cooperative work practices, arose in response to the crises in which ‘Computer Mediated Communication’ (CMC) and Office Automation' (OA) had landed by the late 1980s. The paper finally discusses the reasons why CMC, although superseded as a research paradigm by the practice-oriented program of CSCW, has gained a new lease on life in CSCW and thus why CSCW has become fragmented. Designing Resilient Telecommunications Networks telecommunications networks be designed to withstand deliberate attacks by intelligent agents, possibly working in teams? This chapter continues discussing quantitative risk assessment (QRA) for systems with intelligent adversaries by reviewing progress in methods for designing communications networks that are resilient to attacks – that is, that are able to quickly and automatically reroute traffic around affected areas to maintain communications with little or no interruption. Current network architectures, routing and restoration protocols, and design methods already suffice to protect networks against the loss of any single link or node, so the main focus for defending against deliberate attacks is on the design of networks that can reroute traffic even when multiple simultaneous failures occur. A Long Journey to a Treaty Test ban negotiations proved to be a 50-year journey from the first initiative until the CTBT was signed. Still, after more than another ten years, we have not yet reached the final destination; a CTBT that has entered into force. We may have learned two things from all these years. International negotiations take time, move in small steps and require patience and further, that the possibility of making progress is held hostage by the general political situation, especially among the key players. Plant and Crop Databases atabases have become an integral part of all aspects of biological research, including basic and applied plant biology. The importance of databases continues to increase as the volume of data from direct and indirect genomics approaches expands. What is not always obvious to users of databases is the range of available database resources, their access points, or some basic elements of database querying. This chapter briefly summarizes the history of data access via the Internet and reviews some basic terms and considerations in dealing with plant and crop databases. The reader is directed to some of the major publicly available Internet-accessible relevant databases with summaries of the major focuses of those databases, and several examples are given to illustrate how to access plant genomics data. Finally, an outline is given of some of the issues facing the future of plant and crop databases. Framework of Services Offshoring In this second chapter, we develop a “Framework of Services Offshoring”, which contains a thorough analysis of the classification and drivers of services offshoring. The classification, in Sect. 2.1, examines how services offshoring is defined, which forms it can take, and which service activities are involved. First, we compare and contrast outsourcing and offshoring before deriving our own working definition of services offshoring. Second, we focus on the service part of services offshoring, defining services and describing the implications of their new tradability. We also classify services trade and the relevant service activities. Third, we focus on the offshoring part of services offshoring and discuss the make-or-buy decision in efficiency-based, resource-based, and transaction costs-based theories of firms. Section 2.2 identifies the main drivers of services offshoring. We first describe changes in the global environment, namely developments in ICTs as well as multilateral and regional liberalization of trade in services. We then discuss market-oriented, cost-oriented, and procurement-oriented services offshoring motives of firms, also including newer developments. Finally, we focus on developments in the destination countries, namely the availability of human capital, the presence of multinational companies, and the liberalization of service sectors. The Semantic and Ontological Contents  Applied Communications in Conflict and Catastrophe Medicine  La formalisation: F. Cohen et L. Adleman (1984–1989) Les résultats théoriques présentés dans le chapitre précédent contenaient implicitement toutes les informations nécessaires à la réalisation pratique de virus. Il faudra attendre la fin des années 1970 pour voir apparaître les premiers virus1 connus. La notion de programmes offensifs était déjà connue et évoquée ces années là (en particulier, les chevaux de Troie portés par un virus [7,165] ou non [217]) et les premiers modèles de protection commençaient à être définis (notamment [21]). Le célébrissime jeu «Core Wars» (affrontement de programmes dont le but est leur propre survie face aux autres programmes; à noter qu’une partie de ce jeu est née sur le site de développement et d’essais de missiles de l’armée américaine!) date également de cette époque.