Development and testing of a visualization application software, implemented with wireless control system in smart home care This article describes the development of a visualization application software used to control operational and technical functions in the Smart Home system or Smart Home Care system via the wireless xComfort control system. Graphic visualization of a home electrical control system gives the user unprecedented comfort when controlling home systems. The user is able to obtain the information necessary to optimise the management of operational and technical functions in the building as well as information about energy consumption. Selected definitions of requirements for the visualization system, online access via the Internet, control via USB interface, and control requirements executed via mobile phone are the reasons why these technical elements were selected. This article describes their mutual relations, functions and connections within the system. At the end of this article we propose a method to test the reliability of the created software application as well as the wireless xComfort system under different conditions which stimulate different implementation methods applicable to a real building/apartment unit. Measurement results can be used for the actual installation process and for optimal implementation of the active elements of the wireless system. Asymptotic behavior of a third-order nonlinear neutral delay differential equation The objective of this paper is to study asymptotic nature of a class of third-order neutral delay differential equations. By using a generalized Riccati substitution and the integral averaging technique, a new Philos-type criterion is obtained which ensures that every solution of the studied equation is either oscillatory or converges to zero. An illustrative example is included.MSC:34K11. Registration system of cloud campus by using android smart tablet Near Field Communication (NFC) standard covers communication protocols and data exchange formats. NFC technology is one of radio-frequency identification (RFID) standards. In Japan, Felica card is a popular way to identify the unique ID. We developed the attendance management system (AMS) as the Android application which works in the smart tablet with NFC. Generally, the AMS in the university is fixed to the wall and each student touches or slides his/her own card to the dedicated equipment. Because a teacher can use his/her own smart tablet and/or smartphone, the attendance records are viewed anytime and anywhere. Moreover, we developed the collecting system between PC and some tablets by using Android beam. Any personal data are encrypted and the file can be transferred over the NFC Bluetooth Handover between PC Linux and smart tablet. By the mining of the collected records, early discovery for chronic non-attenders are extracted in educational affairs section. In this paper, a registration system on the cloud campus system by using the personal smartphone with NFC is developed. The system enables to introduce the university courses that are open to the general public. A descriptive analysis of the indications for caesarean section in mainland China BackgroundIn recent decades we have observed a remarkable increase in the rate of caesarean section (CS) in both developed and developing countries, especially in China. However, the real reasons for this phenomenon are uncertain. Notably, the number of women requesting elective CS without accepted valid medical indication has also increased, generating a nationwide debate because several studies have shown that this may be the underlying cause of the increase in CS rates observed recently. Therefore, we carried out a multicentre, large-sample, cross-sectional study to describe the CS rate and indications for CS in mainland China during 2011.MethodsThis was a multicentre, large-sample, cross-sectional study of women who delivered infants in 39 hospitals in 14 provinces in China during 2011. We selected 111, 315 deliveries that occurred during 2011, excluding miscarriages or termination of pregnancy before 28 gestational weeks.ResultsThe overall rate of CS in mainland China was 54.90%. The most common indication for CS was caesarean delivery on maternal request (CDMR; 28.43%), followed by cephalo-pelvic disproportion (14.08%), fetal distress (12.46%), previous CS (10.25%), malpresentation and breech presentation (6.56%), macrosomia (6.10%) and other indications (22.12%). CDMR accounted for 15.53% of all the deliveries and 28.43% of all CS deliveries in mainland China.ConclusionsCDMR appears to be a considerable driver behind the increasing CS rate in mainland China. The relaxation of China’s “one-child policy” may translate into a greater number of CS because of previous CS delivery. To decrease the CS rate, we should first decrease the rate of CS on maternal request. Appropriate policies and guidelines should be considered to accomplish the goal. The impact of video-quality-level switching on user quality of experience in dynamic adaptive streaming over HTTP Dynamic adaptive streaming over HTTP (DASH) has become a promising solution for video delivery services over the Internet in the last few years. Currently, several video content providers use the DASH solution to improve the users’ quality of experience (QoE) by automatically switching video quality levels (VQLs) according to the network status. However, the frequency of switching events between different VQLs during a video streaming session may disturb the user’s visual attention and therefore affect the user’s QoE. As one of the first attempts to characterize the impact of VQL switching on the user’s QoE, we carried out a series of subjective tests, which show that there is a correlation between the user QoE and the frequency, type, and temporal location of the switching events. We propose a novel parameter named switching degradation factor (SDF) to capture such correlation. A DASH algorithm with SDF parameter is compared with the same algorithm without SDF. The results demonstrate that the SDF parameter significantly improves the user’s QoE, especially when network conditions vary frequently. If you want to know about a hunter, study his prey: detection of network based attacks on KVM based cloud environments Computational systems are gradually moving towards Cloud Computing Infrastructures, using the several advantages they have to offer and especially the economic advantages in the era of an economic crisis. In addition to this revolution, several security matters emerged and especially the confrontation of malicious insiders. This paper proposes a methodology for detecting the co-residency and network stressing attacks in the kernel layer of a Kvm-based cloud environment, using an implementation of the Smith-Waterman genetic algorithm. The proposed approach has been explored in a test bed environment, producing results that verify its effectiveness. Remote imaging via FaceTime for potential long-distance diagnosis of suspected cardiac structural and shunt anomalies on contrast echocardiography (The RIFLE Study) BackgroundCertain cardiac anomalies can be missed by standard transthoracic echocardiography and are usually confirmed through contrast echocardiography, a specialized procedure that warrants supervision by a cardiologist with advanced echo training. However, the maldistribution or lack of echo specialists in certain countries, especially in archipelagic settings, deprives many patients of such procedures and probably contributes to delayed diagnosis and treatment. We wanted to find out whether onsite interpretations of contrast echocardiograms of patients with suspected cardiac anomalies are comparable to offsite readings made via the real-time videoconferencing technology of FaceTime.MethodsContrast echocardiograms done on adults or grown-up patients with suspected cardiac structural or shunt anomalies during a one-year period at the Non-invasive Laboratory of the Philippine Heart Center were included in the study. Using the internet-enabled technology of FaceTime, prerecorded echo videos from DVD files were simultaneously viewed and interpreted by two echocardiographers, with the onsite reader using a TV monitor and offsite reader using an Ipad device. Interpretations were compared with respect to 22 pre-specified echocardiographic paramaters, which included chamber size, presence of defect on color Doppler and contrast study, and final diagnosis.Agreement in echo readings was reported as overall agreement rate (% of congruent readings divided by total number of data points for comparison) and Kappa coefficient. The impact of contrast echocardiography on the final diagnosis at both sites was likewise assessed (accuracy rates for pre- and post-contrast diagnoses, with the official test report as reference standard). Video and audio quality were evaluated using a subjective scoring scale, while internet connection was assessed in terms of lag time to transmission (milliseconds), as well as download and upload speeds (Megabytes per second).ResultsSixty-eight (68) videoconferencing sessions were conducted on patients 37 ± 14 years (62% females). Overall reading agreement rate was 89% (1333 congruent readings out of 1496 data points), with most disagreements arising from chamber size evaluation (left ventricle, left atrium and coronary sinus dimensions). Good concordance (kappa > 0.60) was demonstrated between onsite and offsite readings in 14 of 22 parameters. Accuracy rates were comparable for precontrast onsite (51%) and offsite (54%) readings, with a substantial improvement in post-contrast readings (88% onsite and 79% offsite). Only 11 of 68 sessions (16%) had discordant final diagnoses, most of which consisted of overdiagnosis by the offsite reader. Video (3.6) and audio (3.8) quality scores were high (maximum of 4) for both sites, with acceptable lag time (PING < 250 ms), download (2.1 Mbps) and upload (0.3 Mbps) speeds.ConclusionsOnsite interpretations of contrast echocardiograms are comparable to offsite readings made via real-time videoconferencing, with contrast echocardiography enhancing accuracy of diagnosis at both ends. Moreover, FaceTime was feasible, generating acceptable internet speeds as well as obtaining high audio and video quality scores at both ends of the transaction. These findings suggest a role for tele-echocardiography in long-distance, point-of-care diagnosis of patients with structural or shunt anomalies, especially in areas of the country where echocardiographic expertise is very much lacking. Game-based learning with native language hint and their effects on student academic performance in a Saudi Arabia community college The main goal of this study is to address the association between computer games and students’ academic performance. Computer games provide promising opportunities to motivate and involve students in learning. The exceptional growth in numbers of youth playing computer games in Saudi Arabia has stimulated this study to be conducted. In this study, we compared the effectiveness of two learning methods: game-based learning and nongame-based learning. We also observed the effectiveness of providing important terms’ Arabic meaning during the learning process. Both learning methods are designed and applied on teaching a computer networking course in community college. A total of 44 community college students were involved in the study; they were randomly assigned to two groups of equal size (N = 22). One of which used game-based learning method and the other one used nongame-based learning method. Furthermore, both groups were observed with and without native language hint. A knowledge test was conducted as the pretest and posttest. Students’ experiences and opinions on both methods were collected through feedback questionnaires. Data analyses showed that the game-based learning with native language hint demonstrated good performance as compared to nongame-based learning without native language hint. The results showed that students who use game-based method perform better and are more motivated than those who use the non-gaming method. The results suggest that computer game and native language hint can be used in education as a tool to increase student’s academic performance and motivation in non-English speaking countries. Data Mining-based DNS Log Analysis Domain name system (DNS) provides a critical function in directing Internet traffic. Defending DNS servers from bandwidth attacks is a significant task of DNS service providers. Traditional rule-based anomaly or intrusion detection methods are not able to update the rules dynamically. Data mining based approaches are able to find various patterns in the massive dynamic query traffic data. The patterns may assist the DNS service providers to detect anomalies in real time. In this paper, a novel frequent episode mining algorithm is proposed, as well as a volume trend prediction method which allows anomalies to be detected in real time. Density-based clustering approach is adopted to partition numerous domain names into different groups based on the characteristics of their query volume time series. Consistent episode mining method is proposed to find how the query traffic ‘propagate’ at different time between different domain names. Experiments are performed on a real-word DNS log data set. Interesting patterns are presented, indicating data mining based approaches are suitable and promising in the domain of DNS service. Painting with the Same Brush? Surveying Unethical Behavior in the Workplace Using Self-Reports and Observer-Reports Research by academics, professional organizations, and businesses on ethics in the workplace often relies on surveys that ask employees to report how frequently they have observed others engaging in unethical behavior. But what do these frequencies in observer-reports say about the frequencies of committed unethical behavior? This paper is the first to address this question by empirically exploring the relationship between observer- and self-reports. Our survey research among the Swiss working population shows that for all 37 different forms of unethical behavior investigated, observer-reports show higher frequencies than self-reports. Ratios of observer- to self-reports vary substantially among these forms, ranging from 1.46 for improperly gathering competitor’s confidential information to 6.4 for engaging in (sexual) harassment or creating a hostile work environment. The results indicate that researchers should not assume that the frequency in self-reports can generally be approximated by the frequency in observer-reports. Four possible explanations are presented for the differences in ratios, with recommendations for future research. PSHO-HF-PM: An Efficient Proactive Spectrum Handover Mechanism in Cognitive Radio Networks In the paper, we develop an efficient proactive spectrum handover mechanism by using packet scheduling algorithm, called PSHO-HF-PM, to reduce unusable channel. It effectively integrates several mechanisms (hole filling and packet migration) to reduce the bandwidth fragment and support QoS. Its basic idea is in that a new packet is scheduled by migrating some packets to other channels if none of holes in any channels can accommodate it; otherwise repeating the above processes after random waiting time. Meanwhile under an effective data structure, such as the balanced binary search tree, its computational complexity will be $$O(2n\log n)$$O(2nlogn) at most. In the proposed packet scheduling algorithm, packet migration plays a key role in the improvement of bandwidth efficiency and QoS. We also evaluate the performance of total service time for proactive spectrum handover mechanism based on a Preemptive Resume Priority M/G/1 queuing network model. The performance analysis and simulation results show that it performs much better than other proactive and reactive handover mechanism in collision rate, total service time, packet loss probability and bandwidth fragment ratio. BufferBank: A distributed cache infrastructure for peer-to-peer application Peer-to-peer (P2P) systems generate a major fraction of the current Internet traffic which significantly increase the load on ISP networks. To mitigate these negative impacts, many previous works in the literature have proposed caching of P2P traffic. But very few have considered designing a distributed caching infrastructure in the edge network. This paper demonstrates that a distributed caching infrastructure is more suitable than traditional proxy cache servers which cache data in disk, and it is viable to use the memory of users in the edge network as the cache space. This paper presents the design and evaluation of a distributed network cache infrastructure for P2P application, called BufferBank. BufferBank provides a number of application interfaces for P2P applications to make full use of the cache space. Three-level mapping is introduced and elaborated to improve the reliability and security of this distributed cache mechanism. Our measurement results suggest that BufferBank can decrease the data obtaining delay, compared with traditional P2P cache server based on disk. $$\upmu \mathrm{DC}^2$$μDC2: unified data collection for data centers Modern data centers are playing an important role in a world full of information and communication technologies (ICTs). Many efforts have been paid to build a more efficient, cleaner data center for economic, social, and environmental benefits. This objective is being enabled by emerging technologies such as cloud computing and software-defined networking (SDN). However, a data center is inherently heterogeneous, consisting of servers, networking devices, cooling devices, power supply devices, etc., resulting in daunting challenges in its management and control. Previous approaches typically focus on only a single domain, for example, traditional cloud computing for server resource (e.g., computing resource and storage resource) management and SDN for network management. In a similar context of networking device heterogeneity, network function virtualization has been proposed to offer a standard abstract interface to manage all networking devices. In this research, we take the challenge of building a suit of unified middleware to monitor and control the three intrinsic subsystems in a data centre, including ICT, power, and cooling. Specifically, we present $$\upmu \mathrm{DC}^2$$μDC2, a unified scalable IP-based data collection system for data center management with elevated extensibility, as an initial step to offer a unified platform for data center operations. Our system consists of three main parts, i.e., data-source adapters for information collection over various subsystems in a data center, a unified message bus for data transferring, and a high-performance database for persistent data storage. We have conducted performance benchmark for the key building components, namely messaging server and database, confirming that our system is scalable for a data center with high device density and real-time management requirements. Key features, such as configuration files, dynamical module loading, and data compression, enhance our implementation with high extensibility and performance. The effectiveness of our proposed data collection system is verified by sample applications, such as, traffic flow migration for load balancing, VM migration for resource reservation, and server power management for hardware safety. This research lays out a foundation for a unified data centre management in future. Frame-bitrate-change based steganography for voice-over-IP Steganography based on bits-modification of speech frames is a kind of commonly used method, which targets at RTP payloads and offers covert communications over voice-over-IP (VoIP). However, direct modification on frames is often independent of the inherent speech features, which may lead to great degradation of speech quality. A novel frame-bitrate-change based steganography is proposed in this work, which discovers a novel covert channel for VoIP and introduces less distortion. This method exploits the feature of multi-rate speech codecs that the practical bitrate of speech frame is identified only by speech decoder at receiving end. Based on this characteristic, two steganography strategies called bitrate downgrading (BD) and bitrate switching (BS) are provided. The first strategy substitutes high bit-rate speech frames with lower ones to embed secret message, which introduces very low distortion in practice, and much less than other bits-modification based methods with the same embedding capacity. The second one encodes secret message bits into different types of speech frames, which is an alternative choice for supplement. The two strategies are implemented and tested on our covert communication system StegVoIP. The experiment results show that our proposed method is effective and fulfills the real-time requirement of VoIP communication. Load Balancing and Job Migration Techniques in Grid: A Survey of Recent Trends Grid computing has recently become one of the most important research topics in the field of computing. The Grid computing paradigm has gained popularity due to its capability to offer easier access to geographically distributed resources operating across multiple administrative domains. The grid environment is considered as a combination of dynamic, heterogeneous and shared resources in order to provide faster and reliable access to the Grid resources, the resource overloading must be prevented which can be obtained by proper load balancing and job migration mechanisms. This paper presents an extensive survey of the existing load balancing and job migration techniques proposed so far. A detailed classification has also been included based on different parameters which are depending on the analysis of the existing techniques, a new Load balancing technique, along with Job Migration approach has been proposed and discussed to fulfill the existing research gaps. A novel method of mining network flow to detect P2P botnets Botnets are a serious threat to cyber-security. As a consequence, botnet detection has become an important research topic in network protection and cyber-crime prevention. P2P botnets are one of the most malicious zombie networks, as their architecture imitates P2P software. Characteristics of P2P botnets include (1) the use of multiple controllers to avoid single-point failure; (2) the use of encryption to evade misuse detection technologies; and (3) the capacity to evade anomaly detection, usually by initiating numerous sessions without consuming substantial bandwidth. To overcome these difficulties, we propose a novel data mining method. First, we identify the differences between P2P botnet behavior and normal network behavior. Then, we use these differences to tune the data-mining parameters to cluster and distinguish normal Internet behavior from that lurking P2P botnets. This method can identify a P2P botnet without breaking the encryption. Furthermore, the detection system can be deployed without altering the existing network architecture, and it can detect the existence of botnets in a complex traffic mix before they attack. The experimental results reveal that the method is effective in recognizing the existence of botnets. Accordingly, the results of this study will be of value to information security academics and practitioners. A survey on distributed compressed sensing: theory and applications The compressed sensing (CS) theory makes sample rate relate to signal structure and content. CS samples and compresses the signal with far below Nyquist sampling frequency simultaneously. However, CS only considers the intra-signal correlations, without taking the correlations of the multi-signals into account. Distributed compressed sensing (DCS) is an extension of CS that takes advantage of both the inter- and intra-signal correlations, which is wildly used as a powerful method for the multi-signals sensing and compression in many fields. In this paper, the characteristics and related works of DCS are reviewed. The framework of DCS is introduced. As DCS’s main portions, sparse representation, measurement matrix selection, and joint reconstruction are classified and summarized. The applications of DCS are also categorized and discussed. Finally, the conclusion remarks and the further research works are provided. Analogies between Internet network and logistics service networks: challenges involved in the interconnection Logistics networks that are currently formed by supply chains are intertwined but remain heterogeneous and not very interconnected. In computer networks, this stage was overtaken with the arrival of Internet. In this paper we explore the possible analogies and transpositions between computer networks, in particular Internet, and logistic networks. To this end, a new logistical concept was proposed: Physical Internet that aims at the interconnection of networks of logistic services. In fact, there are strong similarities between these networks in spite of the basic differences in the type of objects that prevent an integral transposition. To illustrate the pertinence of this analogy, the authors illustrate the interconnection potential of logistics networks with a stylised model. In view of the exploratory nature of this work, this impact will be assessed by means of an analytic model based on a method of continuous approximations. This illustration provides an indication of the potential inherent in the interconnection of logistics networks. IER: ID-ELoc-RLoc based architecture for next generation internet The scalability and mobility issues in current Internet architecture have drawn a lot of attentions from researchers. However, there are still many problems in current solutions. In this paper, we argue that three spaces, i.e., endpoint IDentifier (ID), Endpoint Locator (ELoc) and Routing Locator (RLoc), are necessary to realize two separations, i.e., separating identifier from locator and separating edge networks from the transit core. Following this argument, we design ID-ELoc-RLoc based architecture, i.e., IER, a separation approach to solve both mobility and scalability issues. After separating identifier from locator, mobile endpoints can ensure continuity of communications across IP address changes since their IDs do not change during moving. After separating edge networks from the transit core, the size and dynamics of global routing table would not be affected by traffic engineering, multi-homing, etc. in edge networks. In this paper, we introduce the definitions, framework, and implementation considerations of our IER architecture in details. Guest editorial: automated techniques for migrating to the cloud (II)  Compact multimode polymer optical 1 × 2 Y splitters with large core planar waveguide Design, fabrication and properties of compact large core multimode optical polymer splitter is presented. Norland optical adhesive glue was used as waveguide core layers, poly(methyl methacrylate) was used for substrate and the fabricated planar splitter is connected by standard plastic optical fiber. The structure was designed by beam propagation method and Y-groove for the waveguide core layer into poly(methyl methacrylate) substrate was made by using CNC engraving and polymer core layer was hardened by applied UV light. The splitter has insertion loss 5.2 dB for 532.8 nm and 4.1 dB for 650 nm. On steady state of continuous min-plus systems We study the steady state of a class of continuous min-plus linear systems by using the notion of system type. The aim is to control systems in order that outputs asymptotically track certain polynomial reference inputs in relation with the just-in-time criterion. As in conventional system theory, the use of system type property gives very simple expression for the resulting controllers. Disturbances acting on the system output are also considered. Mining the home environment Individuals spend a majority of their time in their home or workplace and for many, these places are our sanctuaries. As society and technology advance there is a growing interest in improving the intelligence of the environments in which we live and work. By filling home environments with sensors and collecting data during daily routines, researchers can gain insights on human daily behavior and the impact of behavior on the residents and their environments. In this article we provide an overview of the data mining opportunities and challenges that smart environments provide for researchers and offer some suggestions for future work in this area. CBC: Caching for cloud-based VOD systems Cloud-based video on demand (VOD) service is a promising next-generation media streaming service paradigm. Being a resource-intensive application, how to maximize resource utilization is a key issue of designing such an application. Due to the special cloud-based VOD system architecture consisting of cloud storage cluster and media server cluster, existing techniques such as traditional caching strategies are inappropriate to be adopted by a cloud-based VOD system directly in practice. Therefore, in this study, we have proposed a systemic caching scheme, which seamlessly integrates a caching algorithm and a cache deployment algorithm together to maximize the resources utilization of cloud-based VOD system. Firstly, we have proposed a cloud-based caching algorithm. The algorithm models the cloud-based VOD system as a multi-constraint optimization problem, so as to balance the resource utilization between cloud storage cluster and media server cluster. Secondly, we have proposed a cache deployment algorithm. The algorithm further manages the bandwidth and cache space resource utilization inside the media server cluster in a more fine-grained manner, and achieves load balancing performance. Our evaluation results show that the proposed scheme enhances the resource utilization of the cloud-based VOD system under resource-constrained situation, and cuts down the reject ratio of user requests. Identifying the effects of modifications as data dependencies Dependence analysis on an extended finite state machine (EFSM) representation of the requirements of a system under test has been used in model-based regression testing for regression test suite (RTS) reduction (reducing the size of a given test suite by eliminating redundancies), for RTS prioritization (ordering test cases in a given test suite for early fault detection), or for RTS selection (selecting a subset of a test suite covering the identified dependencies). These particular uses of dependence analysis are based on the definitions of various types of control and data dependencies (between transitions in an EFSM) caused by a given set of modifications on the requirements. This paper considers the existing definitions of data dependencies for capturing the effects of the modifications, gives examples of their inaccuracy and incompleteness, proposes new definitions, and proves the soundness and completeness of these new definitions. Any previous work on regression testing using definitions of data dependencies capturing the effects of modifications can benefit from the proposed definitions. Some capabilities of enumeration control in the DSM method. Part one The capabilities of enumeration optimization in intelligent data analysis by the DSM method of automated hypothesis formation are discussed. Options for enumeration control are considered, as well as options for enumeration reduction by decomposing the entire multitude of DSM hypotheses into independently formed fragments. The concept of the approximate DSM method is formulated. The mean-field computation in a supermarket model with server multiple vacations While vacation processes are considered to be ordinary behavior for servers, the study of queueing networks with server vacations is limited, interesting, and challenging. In this paper, we provide a unified and effective method of functional analysis for the study of a supermarket model with server multiple vacations. Firstly, we analyze a supermarket model of N identical servers with server multiple vacations, and set up an infinite-dimensional system of differential (or mean-field) equations, which is satisfied by the expected fraction vector, in terms of a technique of tailed equations. Secondly, as N→ ∞ we use the operator semigroup to provide a mean-field limit for the sequence of Markov processes, which asymptotically approaches a single trajectory identified by the unique and global solution to the infinite-dimensional system of limiting differential equations. Thirdly, we provide an effective algorithm for computing the fixed point of the infinite-dimensional system of limiting differential equations, and use the fixed point to give performance analysis of this supermarket model, including the mean of stationary queue length in any server and the expected sojourn time that any arriving customer spends in this system. Finally, we use some numerical examples to analyze how the performance measures depend on some crucial factors of this supermarket model. Note that the method of this paper will be useful and effective for performance analysis of complicated supermarket models with respect to resource management in practical areas such as computer networks, manufacturing systems and transportation networks. Some capabilities of enumeration control in the DSM method. Part two Capabilities of enumeration optimization in the intelligent data analysis by the DSM method of automated hypothesis formation are discussed. Some options for enumeration control by using specifically generated combinatory objects called pseudo-trees are considered. Algorithms for targeted restoration of pseudo-trees according to their pseudomodels are proposed. The concept of the approximate DSM method is developed. The capabilities of additional acceleration of DSM data processing by using parallel algorithms, specialized cloud computing, and some problem-oriented hardware-circuit solutions are described. Non-Intrusive Online Quality of Experience Assessment for Voice Communications Recently user quality of experience (QoE) is employed in evaluating end user satisfaction in communications systems. Generally, current approaches for QoE assessment are obtrusive, laboratory based and offline. Estimation of user satisfaction in static manner based on mean opinion score is not directly related to instantaneous individual end user contentment. In this paper, based on correlations between user’s physiological signals and her/his feelings about the service quality, a non-intrusive and user centric QoE assessment system for voice communications is developed. The findings of this study indicate that the emotional patterns in response to the changes in channel quality can be adapted to estimate the level of satisfaction in a QoE assessment system in a live manner. Based on experimental results, two categories of users are identified: sensitive and insensitive towards quality degradations. The results indicate that for the sensitive users, our non-intrusive subjective quality assessment method outperforms ITU-T P.563 standard with respect to root mean square error; while, the results are much better among the insensitive users. Modeling of real-time multimedia streaming with deterministic access To provide Quality of service (QoS) in wireless networks, deterministic channel access is often used. The key idea of the method is to provide a prerogative to a station to access the channel during allocated time intervals called reservations. The existed protocol standards define how to set up a reservation but do not describe how many reservations should be allocated to meet QoS requirements. This challenging issue is complicated in case of a real-time multimedia stream of variable bitrate. To address this issue, in the paper, we propose an algorithm of dynamic resource allocation for the stream in the presence of noise. Prioritized Medium Access Control in Cognitive Radio Ad Hoc Networks: Protocol and Analysis Cognitive radio (CR) technology enables opportunistic exploration of unused licensed channels. By giving secondary users (SUs) the capability to utilize the licensed channels (LCs) when there are no primary users (PUs) present, the CR increases spectrum utilization and ameliorates the problem of spectrum shortage. However, the absence of a central controller in CR ad hoc network (CRAHN) introduces many challenges in the efficient selection of appropriate data and backup channels. Maintenance of the backup channels as well as managing the sudden appearance of PUs are critical issues for effective operation of CR. In this paper, a prioritized medium access control protocol for CRAHN, PCR-MAC, is developed which opportunistically selects the optimal data and backup channels from a list of available channels. We also design a scheme for reliable switching of a SU from the data channel to the backup channel and vice-versa. Thus, PCR-MAC increases network throughput and decreases SUs’ blocking rate. We also develop a Markov chain-based performance analysis model for the proposed PCR-MAC protocol. Our simulations, carried out in $$NS-3$$NS-3, show that the proposed PCR-MAC outperforms other state-of-the-art opportunistic medium access control protocols for CRAHNs. Data secure transmission model based on compressed sensing and digital watermarking technology Since the Internet of Things(IoT) secret information is easy to leak in data transfer, a data secure transmission model based on compressed sensing(CS) and digital watermarking technology is proposed here. Firstly, for node coding end, the digital watermarking technology is used to embed secret information in the conventional data carrier. Secondly, these data are reused to build the target transfer data by the CS algorithm which are called observed signals. Thirdly, these signals are transmitted to the base station through the wireless channel. After obtaining these observed signals, the decoder reconstructs the data carrier containing privacy information. Finally, the privacy information is obtained by digital watermark extraction algorithm to achieve the secret transmission of signals. By adopting the watermarking and compression sensing to hide secret information in the end of node code, the algorithm complexity and energy consumption are reduced. Meanwhile, the security of secret information is increased. The simulation results show that the method is able to accurately reconstruct the original signal and the energy consumption of the sensor node is also reduced significantly in consideration of the packet loss. AQM controller design for TCP networks based on a new control strategy When the network suffers from congestion, the core or edge routers signal the incidence of congestion through the active queue management (AQM) to the sources. The time-varying nature of the network dynamics and the complex process of retuning the current AQM algorithms for different operating points necessitate the development of a new AQM algorithm. Since the non-minimum phase characteristics of the network dynamics restrict direct application of the proportional-integral-derivative (PID) controller, we propose a compensated PID controller based on a new control strategy addressing the phase-lag and restrictions caused by the delay. Based on the unstable internal dynamics caused by the non-minimum phase characteristics, a dynamic compensator is designed and a PID controller is then allowed to meet the desired performance objectives by specifying appropriate dynamics for the tracking error. Since the controller gains are obtained directly from the dynamic model, the designed controller does not require to be tuned over the system operating envelop. Moreover, simulation results using ns2 show improvements over previous works especially when the range of variation of delay and model parameters are drastic. Simplicity, low computational cost, self-tuning structure and yet considerable improvement in performance are exclusive features of the proposed AQM for the edge or core routers. A Low-Cost RFID Authentication Protocol Against Desynchronization with a Random Tuple Radio frequency identification (RFID) technology will become one of the most popular technologies to identify objects in the near future. However, the major barrier that the RFID system is facing presently is the security and privacy issue. Recently, a lightweight anti-desynchronization RFID authentication protocol has been proposed to provide security and prevent all possible malicious attacks. However, it is discovered that a type of desynchronization attacks can successfully break the proposed scheme. To overcome the vulnerability under the desynchronization attacks, we propose a low-cost RFID authentication protocol which integrates the operation of the XOR, build-in CRC-16 function, permutation, a random tuple and secret key backup technology to improve the security functionality without increasing any cost than the utralightweight protocols. The analysis shows that our proposal has a strong ability to prevent existing malicious attacks, especially the desynchronization attacks. A formal model and verification problems for software defined networks Software Defined Networking (SDN) is an approach to building computer networks that separate and abstract data planes and control planes of these systems. In a SDN a centralized controller manages a distributed set of switches. A set of open commands for packet forwarding and flow table updating was defined in the form of a protocol known as OpenFlow. In this paper we describe an abstract formal model of SDN, introduce a tentative language for specification of SDN forwarding policies, and set up formally model-checking problems for SDNs. Non-Parametric Change-Point Estimation using String Matching Algorithms Given the output of a data source taking values in a finite alphabet, we wish to estimate change-points, that is times when the statistical properties of the source change. Motivated by ideas of match lengths in information theory, we introduce a novel non-parametric estimator which we call CRECHE (CRossings Enumeration CHange Estimator). We present simulation evidence that this estimator performs well, both for simulated sources and for real data formed by concatenating text sources. For example, we show that we can accurately estimate the point at which a source changes from a Markov chain to an IID source with the same stationary distribution. Our estimator requires no assumptions about the form of the source distribution, and avoids the need to estimate its probabilities. Further, establishing a fluid limit and using martingale arguments. Distributed phase-shift beamforming power balancing in ad-hoc and sensor networks Ad-hoc and sensor networks are a well-studied area which gained a lot of attention in the research in the last decades. Two of the problems of battery-powered radio devices are limited transmitter power and finite amount of energy. This paper continues the path opened by the development of a new technology for radio communication which allows cluster communication beyond the horizon of each individual transmitter and the distribution of power need among the modules forming a cluster. This in terms decreases the average power need per device and contributes to a longer lifetime of the entire network. Error- and loss-tolerant bundle fragment authentication for space DTNs To ensure the authenticity and integrity of bundles, the in-transit PDUs of bundle protocol (BP) in space delay/disruption-tolerant networks (DTNs), the bundle security protocol specification (IRTF RFC6257) suggested using a digital signature directly over each bundle. However, when bundle fragment services are needed, this mechanism suffers from heavy computational costs, bandwidth overheads and energy consumption. In this paper, we address the fragment authentication issue for BP by exploiting the combination of RS error correction and erasure codes with the help of batch transmission characteristic of DTNs. The RS error correction and erasure codes are adopted to allow the receivers to locate the false/injected fragments and reconstruct the only one signature shared by all fragments of a bundle, even if some other fragments are lost or routed to a different path. Getting only partial authentic fragments, a DTN node is able to detect and filter the false/injected fragments, and authenticate the origin of a bundle as well. Such an approach tolerates high delays, unexpected link disruption and the BP nature of routing fragments of the same bundle possibly via different paths. The performance analysis demonstrates that both of our schemes, which follow our generic idea based on RS codes, significantly reduce bandwidth overheads and computational costs as compared to the prior works. Adaptive Combiner for MapReduce on cloud computing MapReduce is a programming model to process a massive amount of data on cloud computing. MapReduce processes data in two phases and needs to transfer intermediate data among computers between phases. MapReduce allows programmers to aggregate intermediate data with a function named combiner before transferring it. By leaving programmers the choice of using a combiner, MapReduce has a risk of performance degradation because aggregating intermediate data benefits some applications but harms others. Now, MapReduce can work with our proposal named the Adaptive Combiner for MapReduce (ACMR) to automatically, smartly, and trainer for getting a better performance without any interference of programmers. In experiments on seven applications, MapReduce can utilize ACMR to get the performance comparable to the system that is optimal for an application. The Cyborg Revolution This paper looks at some of the different practical cyborgs that are realistically possible now. It firstly describes the technical basis for such cyborgs then discusses the results from experiments in terms of their meaning, possible applications and ethical implications. An attempt has been made to cover a wide variety of possibilities. Human implantation and the merger of biology and technology are important factors here. The article is not intended to be seen as the final word on these issues, but rather to give an initial overview. Most of the experiments described are drawn from the author’s personal experience over the last 15 years. Internet-mediated drug trafficking: towards a better understanding of new criminal dynamics Cyberspace has increasingly become an online marketplace for recreational drugs. The consequences that Internet usage has brought to drug trafficking, however, are still under-investigated. In this study script analysis was used to identify the structure of criminal opportunities that the Internet supplies for drug trafficking, and to allow a richer and deeper understanding of the dynamics of this criminal activity in the Internet age. This article provides an accurate description of how actors involved in drug trafficking behave in cyberspace and highlights how not only has the Internet opened the way for new criminal actors, but it also has re-configured relations among suppliers, intermediaries, and buyers. The conclusions suggest new directions for research and possibilities for a proactive law-enforcement approach. A novel image encryption technique based on Hénon chaotic map and S8 symmetric group The structure of cryptographically resilient substitution boxes (S-boxes) plays a central role in devising safe cryptosystems. The design of chaos-based S-boxes by means of chaotic maps obtained more devotion in current ages. We have suggested novel S-boxes based on the chaotic maps and S8 symmetric group. We have experimented our chaos-based S-box for image encryption applications and analyze its strength with statistical analyses. Multicast and customized deployment of large-scale operating systems With the recent paradigm shift of cloud computing, deployment of operating systems (OSs) onto a large-scale computer network is becoming necessary. Note that there are usually numerous nodes with various functions in a cloud computing system. Thus, it is usually required to deploy different operating systems onto different nodes. In such a customized setting, conventional techniques of using unicast deployment to distribute a massive cloud OS onto thousands of nodes is time consuming and bandwidth-intensive. In this work, we thus propose a multicast deployment approach so as to significantly improve deployment efficiency. Furthermore, our multicast deployment approach can leverage existing configurations of the unicast counterpart. Specifically, the advantageous features of the proposed approach include the support of a reliable multicast protocol, a heterogeneous infrastructure, and cloud hypervisor environments. To evaluate the feasibility of the proposed approach in practical applications, CentOS and Ubuntu are used when implementing our deployment approach on several tens of nodes. Empirical studies show that both the required time for the entire distribution process, i.e., from starting delivery until the OS is ready, and the network bandwidth consumption are significantly reduced as compared with conventional unicast approaches. Consequently, less effort is required on monitoring and maintenance for system administrators. Insights from pre-service teachers using science-based augmented reality This research examined the results of an exploratory case study on the use of the augmented reality (AR) platform Aurasma in the higher education science classroom. Thirty-one pre-service students, enrolled in an undergraduate science methodology course, participated in the study. Research methods included data collection of teaching reflections regarding pre-service teachers’ experiences using the platform Aurasma for learning. Findings included student perceptions regarding the usability of AR in the classroom setting and insights into how the Aurasma platform facilitated inquiry and understanding of science concepts. An analysis of the data suggested that AR has the potential to positively impact classroom learning experiences including an increase in motivation and engagement, teacher enthusiasm, and the facilitation of a community of practice. However, the incorporation of AR into the classroom was not without challenges, highlighting the fact that AR may be time consuming, teachers may not have the skills needed to use such technology, or there may be a lack of infrastructure. Each topic is discussed and supported by relevant literature and excerpts from the student reflections. Recommendations are given for future classroom implementation. Reexamining anomaly temporal behaviors in SPEC CPU workloads: self-similar or not? This paper studies the correlation of memory accesses in high-performance computer systems from a time dependence perspective, and concludes that correlations in memory access-arrival times are inconsistent, either with little correlation or with evident and abundant correlations. Thus neither independent identically distributed or self-similar is appropriate to characterize all memory activities. For memory workload with evident correlations, we present both pictorial and statistical evidence that memory accesses have self-similar like behavior. In addition, we implement a memory access series generator in which the inputs are the measured properties of the available trace data. Experimental results show that this model can accurately emulate the complex access arrival behaviors in both workloads with little and strong correlations, particularly for the burstiness characteristics in the memory workloads. Minimum total coloring of planar graph Graph coloring is an important tool in the study of optimization, computer science, network design, e.g., file transferring in a computer network, pattern matching, computation of Hessians matrix and so on. In this paper, we consider one important coloring, vertex coloring of a total graph, which is familiar to us by the name of “total coloring”. Total coloring is a coloring of $$V\cup {E}$$V∪E such that no two adjacent or incident elements receive the same color. In other words, total chromatic number of $$G$$G is the minimum number of disjoint vertex independent sets covering a total graph of $$G$$G. Here, let $$G$$G be a planar graph with $$\varDelta \ge 8$$Δ≥8. We proved that if for every vertex $$v\in V$$v∈V, there exists two integers $$i_{v},j_{v} \in \{3,4,5,6,7,8\}$$iv,jv∈{3,4,5,6,7,8} such that $$v$$v is not incident with intersecting $$i_v$$iv-cycles and $$j_v$$jv-cycles, then the vertex chromatic number of total graph of $$G$$G is $$\varDelta +1$$Δ+1, i.e., the total chromatic number of $$G$$G is $$\varDelta +1$$Δ+1. Outage Capacity Analysis of a Cooperative Relaying Scheme in Interference Limited Cognitive Radio Networks In this study, we investigate the outage capacity of a cooperative relaying based cognitive radio network in slow fading channel. Our network scenario consists of a primary transmitter (PT) and primary receiver (PR) as well as a group of $$M$$M secondary transmitter (ST)–receiver (SR) pairs. We grouped STs into active and inactive. Only one active ST may transmit data at a time in parallel with the PT satisfying a predefined interference threshold $$I_{th}$$Ith to the PR. Due to fading/shadowing or interference caused by ST to the PR, primary user (PU) may fail to achieve its target rate $$R_{{\textit{PT}}}$$RPT over a direct link. To overcome this, we can boost up primary capacity by using inactive STs as cooperative relay (Re) nodes for the PU. In addition, one of the inactive STs that achieves $$R_{{\textit{PT}}}$$RPT will be act as a best decode-and-forward relay to forward the primary information. In this paper, a closed-form expression of the outage capacity is derived. Results show that outage capacity improves with increasing cooperative nodes as well as when the active ST is located farther away from the PR. OATS: online aggregation with two-level sharing strategy in cloud Online aggregation (OLA) is an attractive sampling-based technology to response aggregation queries by an approximate estimate to the final result, with the confidence interval becomes tighter over time. It has been built into the MapReduce-based cloud system for big data analytics, which allows users to monitor the query progress, and save money by killing the computation early once sufficient accuracy has been obtained. However, there is a serious limitation that restricts the performance of OLA that is the sharing issue of multiple OLA queries processing. Note that, in the original MapReduce paradigm, each query is processed independently without considering the potential sharing opportunities, leading to two major unnecessary additional execution costs: (1) the large redundant I/O cost, and (2) the replicative statistical computation cost. To eliminate such additional execution cost and improve the overall performance, we present online aggregation with two-level sharing strategy in cloud (OATS) based on MapReduce framework in this paper to effectively support online aggregation for large scale concurrent query processing in skewed data distribution. In the first-level sharing, we propose a sample buffer management mechanism to share the sampling opportunities among multiple OLA queries to reduce redundant I/O cost. While in the second-level sharing, we propose a heuristic algorithm (with a good scalability for large input) for the statistical computation to share partial statistics calculation to decrease the number of final aggregation operations, reducing the statistical computation cost. Based on such two-level sharing strategy, we have implemented OATS in Hadoop and conducted an extensive experiments study on the TPC-H benchmark for skewed data distribution. Our results demonstrate the efficiency and effectiveness of OATS. Changing Water Depths in the Eastern Part of Sydney Harbour due to Human Impacts 
Sydney Harbour has been significantly modified by human impacts from the start of the European settlement in 1788. Land clearing has accelerated soil erosion, resulting in increased sedimentation. Dredging has deepened many areas to accommodate ever-larger ships. In this paper a GIS method is used to map bathymetric changes in the eastern part of the harbour from 1903 to more recently. Dredged areas are apparent in the entrance and in wharfage areas, while sedimentation is most marked around the deepest section, which is well inside the harbour itself. In this latter region sediment has built up considerably, to over 3 m in some locations, and ship-induced motions appear to have had an impact. Despite these changes the overall depth of the eastern part of the harbour has changed little. This work is of interest to maritime archaeologists because it brings out the types of processes by which sediments can accumulate and be removed thus altering a harbour’s seabed and potentially burying, exposing or erasing archaeological sites and artefacts.
 Which controls are better for service outsourcing? Integrating service-dominant logic and service characteristics A comprehensive examination of the role of outsourced services’ characteristics in determining control systems is lacking in the literature. While the service characteristics paradigm (intangibility, heterogeneity, inseparability, and perishability [IHIP]) has informed much of the services research in the past four decades, in the last decade, the service-dominant logic (SDL) has provided a new way to conceptualize the marketing of goods and services. However, there has been no attempt to integrate these two perspectives. This study fills these gaps by examining control systems in service outsourcing by means of a midrange theory that integrates the new SDL framework and the legacy of the service characteristics approach. The authors add a fifth characteristic, digitizability, to IHIP and develop a conceptual framework to derive research propositions based on the foundational premises of the SDL. They also examine the moderating effects of globalization and the outsourced service’s closeness to the organization’s core competence.