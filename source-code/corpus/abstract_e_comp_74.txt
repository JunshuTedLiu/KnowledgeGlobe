Variation in assortative mating in two colonies of Arctic skuas THE Arctic skua is a polymorphic seabird with pale, intermediate and dark plumaged phenotypes. Sexual selection of the phenotypes takes place when pairs are being formed; during the breeding season the darker males generally mate before the paler males1,2. This gives them a selective advantage because a pair which breeds earlier in the season will fledge a greater average number of chicks1,3. Computer models have been used to estimate the females' mating preferences for the darker males3. Solar energetic particle event with 3He/4He>1 OBSERVATIONS of γ rays and the short lived isotope 3H provide evidence for the occurrence of nuclear reactions by high energy particles accelerated in solar flares1,2. Stable isotopes such as 2H and 3He should also be produced in these reactions. Solar 3He particles were first detected by Hsieh and Simpson3. In the energy range 10 to 100 MeV per nucleon they obtained 3He/4He = (2.1±0.4) × 10−2 by summing over seven solar particle events. Garrard et al.4 and Anglin et al.5 have reported that 3He/4He was highly variable from event to event. (Table 1). In the ‘3He-rich events’, 2H and 3H were not detected and the resulting upper limits were much less than expected from the theory of nuclear reactions6. These events were small, and the number of 3He particles observed was low (∼ 70). This anomalous production of 3He should provide new insight into the acceleration and confinement process of energetic particles in solar flares. Computer diagnosis of goiters. The optimal size of optimal subsymptomatologies Optimization for diagnostic recognition rate was performed for subsets of symptoms of various sizes. The diagnostic problem was the recognition and identification of thyroid diseases. Unbiased evaluation of performance was obtained and the extent of the bias in other evaluation methods was determined. Interdependence of symptoms was shown to be a negligible nuisance in the application of Bayesian inference to the present data. An optimal size of optimized subsets of symptoms was observed. A comparison with sequential diagnosis shows that the two procedures are different, although theyare related, and that the optimality of subsets is sensitive to departures from their composition. Time series model for texture synthesis A general method is proposed for the synthesis of texture. It is based on a model which treats the pixels (picture elements) of a digitized textural scene as a two-way seasonal time series. This method possesses the desirable characteristic that the parameters needed for synthesis are derived directly from the analysis of the “parent” texture (texture to be imitated). With the help of well-developed methods in the time series analysis the process that generates the pixels of the parent texture is identified. From a set of boundary conditions the future values of the time series are generated which in essence are the pixels of the synthesised texture. The effectiveness of this method is illustrated with examples. Some considerations in management of computer-processable files of geological data Many sets of data in the natural sciences have a hierarchical structure that is either inherent in the data or that is imposed on them during collection. These data structures can be defined using mathematical set-theory notation. If data from two or more computer-processable files are accessed simultaneously, the application of operations of set union and intersection leads to emergence of certain criteria which must be adhered to when managing data files using a generalized, database management system. One of the situations in which these principles come into play is illustrated by the merging of two computer-processable files of geotechnical data, the data for both files having been derived from the same source but structured differently. Performance bounds on the splitting algorithm for binary testing n machine fault-location, medical diagnosis, species identification, and computer decisionmaking, one is often required to identify some unknown object or condition, belonging to a known set of M possibilities, by applying a sequence of binary-valued tests, which are selected from a given set of available tests. One would usually prefer such a testing procedure which minimizes or nearly minimizes the expected testing cost for identification. Existing methods for determining a minimal expected cost testing procedure, however, require a number of operations which increases exponentially with M and become infeasible for solving problems of even moderate size. Thus, in practice, one instead uses fast, heuristic methods which hopefully obtain low cost testing procedures, but which do not guarantee a minimal cost solution. Examining the important case in which all M possibilities are equally likely, we derive a number of cost-bounding results for the most common heuristic procedure, which always applies next that test yielding maximum information gain per unit cost. In particular, we show that solutions obtained using this method can have expected cost greater than an arbitrary multiple of the optimal expected cost. Choosing a storage schema n using a general-purpose language such as PL/I, a data base management system such as IMS, or a special-purpose language such as SNOBOL, program designers must decide on strategies for storing their data. This paper contains a description of an algorithm for choosing suitable data linking methods from a class of available storage schemata. Emphasis is placed on specifying the types of parameters which are involved in this choice, such as the volatility of the data, the density of the keys, and the amount of storage available. A multivariate analytical strategy for classifying paleoenvironments A multivariate analytical strategy is proposed for aiding the investigator in extracting maximum information from environmental data. Data are carefully coded and scaled and are tested for redundancy using R-mode cluster analysis. The samples are partitioned into environmental classes using Q-mode cluster analysis. Q-mode ordination facilitates interpretations, which usually can be verified by comparison with field relationships. Discriminant analysis serves as an identification procedure for extending the classification to unknown samples. The strategy is demonstrated by application to Cape Hatteras microorganism distributions and Devonian sedimentary facies. Selective review of research studies showing media effectiveness: A primer for media directors  Some restrictions onW-grammars The effect of some restrictions onW-grammars (the fonnalization of the syntax ofalgol 68) are explored. Two incomparable families examined at length areWrb (languages generated by normal regular-basedW-grammars) andWs (languages generated by simpleW-grammars). Both properly contain the context-free languages and are properly contained in the family of quasi-realtime languages. In addition,Wrb is closed under nested iterated substitution (but is not anAFL) and is properly contained in the family of index languages. Description of developmental languages using recurrence systems Recurrence systems have been devised to describe formally certain types of biological developments. A recurrence system specifies a formal language associated with the development of an organism. The family of languages defined by recurrence systems is an extension of some interesting families of languages, including the family of context-free languages. Some normal-form theorems are proved and the equivalence of the family of recurrence languages to a previously studied family of developmental languages (EOL-languages) is shown. Various families of developmental and other formal languages are characterized using recurrence systems. Some closure properties are also discussed. Sequential syntactical decoding It has been proposed that syntactical information be used as an aid in error detection, location, and correction. In the present paper the minimum distance syntactical decoding algorithm of Souza and Scholtz is improved with the use of sequential decoding techniques. Fano's algorithm is adapted to the syntactical case; simulation results show the drastic reduction in number of backtrackings afforded by the new algorithm. Comments are made on the results obtained, their implications, and topics for future research. A nonlinear programming approach to a very large hydroelectric system optimization A large scale hydroelectric system optimization is considered and solved by using a non-linear programming method. The largest numerical case involves approximately 6 000 variables, 4 000 linear equations, 11 000 linear and nonlinear inequality constraints and a nonlinear objective function. The solution method is based on(i)partial elimination of independent variables by solving linear equations,(ii)essentially unconstrained optimization of a compound function that consists of the objective function, nonlinear inequality constraints and part of the linear inequality constraints. The compound function is obtained via penalty formulation.
The algorithm takes full advantage of the problem's structure and provides useful solutions for real life problems that, in general, are defined over empty feasible regions. The statistical zap versus the shotgun approach Multivariate analysis is used in the search for one or more types of structure. The statistical zap applies a single method to determine one preselected type of structure. Several zaps suffice to ascertain several types of structure. The statistical shotgun represents an alternative approach. Here, a series of methods is applied to the data with the intent of ascertaining all possible types of structure that may exist. If strong structure is present, an appropriate zap will probably reveal it, and a variety of techniques will determine the same general structure. If only the main structure is required, the zap is adequate. In this situation, the shotgun will display a basic consistency which is at least reassuring. However, zaps may fail to detect a more subtle secondary structure of geological interest which will be displayed by the shotgun. For weakly structured data, a zap will only determine one type of structure but the shotgun reveals all. Study of the ontogeny of Parastylonurus myops(Clarke), a Lower Silurian eurypterid from New York (USA) shows the virtues of the statistical shotgun. Paleocurrent analysis of sedimentary crossbed data with graphic output using three integrated computer programs Basin analysis for paleocurrent directions commonly makes use of hundreds of cross-bedding dip and strike measurements taken on structurally tilted sedimentary beds which may be parts of plunging folds. For regional interpretations the desired forms of data are vector means and rose diagrams of dip directions after reduction to the assumed horizontal plane of deposition. Three FORTRAN computer programs have been integrated to perform the data reduction from raw field measurements to graphic plotted rose diagrams. Program PLUNGR rotates the raw field data back to the original horizontal plane of deposition. Output from PLUNGR is a deck of punched cards suitable for input into the second and third programs, with provision for various regroupings of data. VECMEN computes vector mean and other statistics on data groups defined by inserted control cards. Program ROSE, with a variety of options, plots a rose diagram of dip direction azimuths on a CalComp plotter. An improved version of the out-of-kilter method and a comparative study of computer codes The primary objectives of this paper are: (1) to present an improved formulation of the out-of-kilter algorithm; (2) to give the results of an extensive computational comparison of a code based on this formulation with three widely-used out-of-kilter production codes; (3) to study the possible sensitivity of these programs to the type of problem being solved; and (4) to investigate the effect of advanced dual start procedures on overall solution time.The study discloses that the new formulation does indeed provide the most efficient solution procedure of those tested. This streamlined version of out-of-kilter was found to be faster than the other out-of-kilter codes tested (SHARE, BSRL and Texas Water Development Board codes) by a factor of 2–5 on small and medium size problems and by a factor of 4–15 on large problems. The streamlined method's median solution time for 1500 node networks on a CDC 6600 computer is 33 seconds with a range of 33 to 35 seconds. Models of deterministic systems The definition of “model of a system” in terms of a homomorphism of the states of the system is evaluated and an alternative definition in terms of sequence generators is proposed. Sequence generators are finite graphs whose points represent complete states of a system. Sequence generators include finite automata and other information processing systems as special cases. It is shown how to define models in terms of a projection operator which applies to any sequence generator which has an output projection and yields a new sequence generator. A model produced by the projection operator is embedded in the system it models. The notion of embedding is discussed informally and some questions raised about the relations of deterministic, indeterministic, and probabilistic models and systems. Nonterminals, homomorphisms and codings in different variations of OL-systems ontinuing the work begun in Part I of this paper, we consider now variations of nondeterministic OL-systems. The present Part II of the paper contains a systematic classification of the effect of nonterminals, codings, weak codings, nonerasing homomorphisms and homomorphisms for all basic variations of non-deterministic OL-languages, including table languages. Generalized addressing schemes for data graphs Addressable data graphs enjoy structural properties which are at once mathematically attractive and practically useful. This paper is devoted to studying generalizations of addressability, with a twofold goal. The first aim of the paper is to gain understanding of what features of addressing schemes account for the attractive properties of addressable data graphs. The second purpose of the paper is to seek a variant of the property of addressability, which retains the practical concomitants of the original property, but which is enjoyed by a broader class of data graphs. Two such variants are discovered,quasi-addressability andweak-addressability. Although “most” data graphs do not enjoy any form of addressability, every data graph can be slightly modified to render it quasi or weakly addressable—in sharp contrast to the stringent demands of addressability. However, the added breadth of these new addressabilities is obtained at the expense of the invariance and selectivity which accompany the original property. On the selection of parameters in Self Scaling Variable Metric Algorithms This paper addresses the problem of selecting the parameter in a family of algorithms for unconstrained minimization known as Self Scaling Variable Metric (SSVM) Algorithms. This family, that has some very attractive properties, is based on a two parameter formula for updating the inverse Hessian approximation, in which the parameters can take any values between zero and one. Earlier results obtained for SSVM algorithms apply to the entire family and give no indication of how the choice of parameter may affect the algorithm's performance. In this paper, we examine empirically the effect of varying the parameters and relaxing the line-search. Theoretical consideration also leads to a switching tule for these parameters. Numerical results obtained for the SSVM algorithm indicate that with proper parameter selection it is superior to the DFP algorithm, particularly for high-dimensional problems. Letters to the editor  Computer simulation of the response of frog skin epidermis to changes in [Na+]0 he operation of the multicompartmental frog skin epidermal model 10E described in the preceding paper was tested to find out by computer simulation whether it responds to changes in [Na+] in the same manner as frog skin. In the range from 5 to 115mm [Na+]0, the rate of net Na+ flux across skin is known to increase. The results can be fitted to Michaelis-Menten's law of reaction kinetics, or, alternately, to Hoshiko's linear function, plotting fluxvs. log [Na+]0. Model 10E simulated the laboratory results on skin, provided that the rate coefficients at the site of entry of Na+ into the system were varied in exactly the same manner as they actually were found to vary in skin. In model studies, Na+ backflux (outflux) decreased with increasing [Na+]0, contrary to observations on skin. This discrepancy may be related to adaptive reactions in skins (decrease in permeability) when [Na+]0 is lowered, a feature that has not been modeled. It is known that the skin p.d. changes, mostly, by approximately 35 mV per decade change in [Na+]0. Model 10E gave very nearly the same result when the rate coefficients for entry of Na+ were changed as mentioned above (i.e., varied exactly as they were found to vary in skin). Skin and model 10E behaved similarly in that, at [Na+]0=[Na+]i=115mm, the extent to which labeling with Na* from the outside (12%) and from the inside (88%) is possible was the same. Model data are presented which show in which way the Na+ pools, [Na+] in the individual compartments, and intercompartmental fluxes changed with changing [Na+]0. Because of lack of experimental data on skin for comparison, these calculated results are purely hypothetical, but they are not unreasonable. The role of rhythms in homeostasis fassungEs wird allgemein angenommen, daß biologische Rhythmen eine Rolle als Zeitgeber für Systeme spielen. Zirkadische Rhythmen z.B. stellen die Beziehung zwischen dem hell-dunkel Zyklus der Umwelt und dem zeitlichen Ablauf interner Vorgänge her. Schnellere Rhythmen sind wahrscheinlich auf Ein-/Ausschaltungen homeostatischer Regler zurückzuführen. Das hier beschriebene Modell besteht aus einem Rückkopplungszweig, einem Ein-/Ausschalt-element und einem frequenzabhängigen Teil (Filter). Es reproduziert die wichtigsten Eigenschaften des lebenden Originals — exakte Regulierung, amplituden begrenzte spontane Schwingungen (Rhythmen) und das Synchronisieren dieser Schwingungen durch äußere periodische Störungen, die eine von der Frequenz abhängige Mindestamplitude überschreiten.Das Modell wurde an zwei Regelvorgängen im menschlichen Körper überprüft — Regulierung von Körpertemperatur und Blutdruck. Der Rückkopplungszweig entspricht den Rezeptoren samt Nervenverbindung zum Gehirn. Das Ein/Aus-Element stellt das Zwischenstück zwischen den afferenten und den efferenten Leitungen dar. Das Filter gibt das Verhalten der glatten Muskeln nach Reizung durch Nervimpulse wieder. Beide Regelprozesse zeigen beim Menschen das Phänomen der frequenzselektiven Synchronisierung, offensichtlich eine spezielle Eigenschaft ihrer Filter. Eine mathematische Untersuchung zeigt, wie diese Regelprozesse automatisch die „An”- und „Aus”-Zeiten einer Impulsefolge einstellen, um entsprechend der Störung eine genaue Regelung zu erzielen. Minimality and complementarity properties associated with Z-functions and M-functions A nonlinear generalization of square matrices with non-positive off-diagonal elements is presented, and an algorithm to solve the corresponding complementarity problem is suggested. It is shown that the existence of a feasible solution implies the existence of a least solution which is also a complementary solution. A potential application of this nonlinear setup in extending the well-known linear Leontief input—output systems is discussed. Computer simulation of Na* wash-out kinetics in frog skin epidermis he multicompartmental frog skin epidermis model proposed in a previous paper was applied to computer simulation studies on the kinetics of the wash-out process of Na* from frog skin. Both the kinetics of loading of the model membrane with Na* from the outside to reach steady-state conditions in all internal compartments, and of the wash-out process were followed. This was done for the case when two Na+ pumps were operative, or inoperative, simulating the inhibitory effect of ouabain on active Na+ transport in frog skin. The two pumps were characterized as transmembrane Na+ flow pumps, and internal Na+ maintenance pumps which contribute but little to net inward Na+ flux. The simulation results were in good agreement, both qualitatively and quantitatively, with data in the literature on the behavior of frog skin epidermis. This analysis gives support especially to the views held by Zerahn on location and size of the active Na+ transport pool in skin epithelium. Beyond this, however, this study clearly delineates the experimental conditions under which the estimation of the Na+ transport pool by the method of measuring the wash-out rate of Na* may be successful, and under which conditions this method will fail. Marketing information systems—The problem of system usage Much literature has been devoted to singing the praises of present and potential computer utilization in marketing. Highly touted are “total information systems,” “on-line, real-time systems,” and “simulation-based systems.” One would be led to believe that each company has devoted or should be devoting its moneys and energies to the design and implementation of such sophisticated computer-based marketing information systems. Yet recent evidence has indicated a lack of enthusiasm-indeed, in some cases antagonism exists toward information systems in marketing. One reason for this disillusionment has been low usage of existing marketing information systems by marketing management. System utilization holds the key to a favorable return on investment in marketing information systems. This article explores many of the causes of infrequent to non-existent system usage and offers a plea and a plan for active elimination of this problem. Cross covariance function estimation by an average response computer The relationship between a continuous process such as the EEG of the central nervous system and a point process such as the discharge activity of a single neuron can often be profitably studied by way of the cross covariance function (CCVF). The individual events in the point process can be used to trigger an average response computer so as to obtain the average response of the continuous process to an event in the point process. Under certain circumstances it has been shown that this average response is a useful estimate of the CCVF. Because the technique is simpler to use than conventional methods for estimating the CCVF, it has been suggested that the technique be used for this purpose. However, when the point process is considered in terms of its expectation density, it can be shown that the CCVF estimate obtained by averaging is valid only when the properties of the processes are special enough as not to be encountered in many of the experimental conditions of interest. The derivation indicates explicitly how the differences between the averaging estimator and the true CCVF arise. Application of some correlation coefficient techniques to time-series analysis Application of the sliding correlation technique has permitted detailed stratigraphic correlation over entire basins. The main values of the technique are to (1) demonstrate correlation statistically, (2) extend the range of visual correlation, and (3) establish precise correlation where correlation is known to exist but is difficult to establish visually. The technique is especially valuable in aiding correlation of stratigraphic sequences such as varves and turbidites, which are characterized by monotonous repetition of two or more lithologic components. The moving correlation technique is a valuable aid in examining variations in degree of correlation between correlative sections, and in analysis of component associations within a single stratigraphic sequence. Geologic data analysis with computers  Some problems in automatic process grouping and file consolidation The purpose of this paper is to define the operations for process grouping and file consolidation, and also the circumstances under which these operations can be applied, with sufficient accuracy so that they could be performed by a computer program used for system design. The usual consolidation of standing files is made in three steps, using precise matrix operations. Concerning the problem of losing many possibilities for later process grouping when consolidating standing files, two solutions are given. The first uses a matrix, which gives information about the relations between the different update versions of the same file. In the second a matrix is used, which gives information about the precedence relations caused by the original relations of the system, to indicate when the change of the execution order of two processes is possible. A study in test reproducibility between laboratories: Report of a Pseudomonas Working Party The results are reported of a collaborative study in laboratories of 17 tests commonly used for pseudomonads, together with statistical analysis of the results in the form of analyses of variance. The studies involved 59 strains.The following tests showed good or reasonably good consistency between different laboratories: motility if checked by a Craigie tube, growth at 4 C, production of fluorescin, oxidation of glucose and sucrose, gluconate oxidation, hydrolysis of aesculin, arginine and casein, and gelatin hydrolysis by the tube method.The following were less satisfactory: shape, size and arrangement of organismis, growth at 42 C, oxidase, acid from lactose, reduction of nitrate, acetic acid from ethanol, gelatin hydrolysis by the plate method, and the egg yolk reaction.The following showed poor consistency or noticeable difficulties in performing or interpreting results of the test: growth at 37 C, the denitrification test of Stanier, Palleroni and Doudoroff, and hydrolysis of urea.However, with several of the unsatisfactory tests the replicates within a laboratory showed much better reproducibility. This (together with other evidence on the importance of exact control of variables such as temperature, and time of reading, and the occurrence of mutants in some strains) suggests that careful attention to standardization may give much better testing methods. The considerable value of statistical analyses in such work is discussed. Erratum  Trivial integer programs unsolvable by branch-and-bound  Important announcement  Convective tube furnace for hydrocarbon conversion, and mathematical model of furnace ons1.A method is proposed for the design calculation of tube furnaces, this method being based on computerized calculation of the basic characteristics.2.A mathematical model is proposed for a convective tube furnace used in steam conversion of hydrocarbons, a computerized method of solution is proposed, and certain results from the calculation of basic parameters are examined.3.The tube-furnace model can be used in optimal design work and also in establishing qualitative relationships between control parameters and the criterion of the furnace equation. Primary basalts and magma genesis Three Eocene lavas from Skye, NW Scotland, have been subjected to anhydrous experimental studies within their melting ranges at pressures up to 30 kb. Two of these, an olivine-phyric magnesian alkali basalt and a near-aphyric Mg-poor transitional basalt, appear to show four-phase points on their liquidi at high pressures which are thought to have genetic significance. From experimental and mineralogical evidence, the magnesian basalt is postulated to be a primary magma, erupted without significant compositional change from its genesis by slight partial melting of a relatively Fe-rich spinel lherzolite upper mantle at about 60 km depth. The liquid seems to have had a reaction relationship with Ca-poor pyroxene (pigeonite) in the residual lherzolite. Partial crystallization of batches of this magma, delayed during its ascent at depths of about 40 km, is thought to have given rise to the Mg-poor basaltic liquids. The third lava studied experimentally, a sparsely olivine-phyric hawaiite, does not have olivine on the liquidus in any part of its anhydrous P-T diagram and therefore cannot have been derived under anhydrous conditions from olivine-saturated sources. The mineralogy and chemistry of the lavas are used to support an hypothesis that the hawaiites are products of partial crystallization of pockets of basalt magma at depths approximating to the crust/ mantle boundary beneath Skye, with 
$$P_{{\text{H}}_{\text{2}} {\text{O}}}$$
 rising to sufficient values to make the residual liquids comparatively rich in normative feldspar. Finally, the genesis of all other Skye Eocene lavas is reviewed in the light of the new experimental data. Computational experience in sensitivity analysis for nonlinear programming A method for sensitivity analysis in nonlinear programming has recently been developed using the sequential unconstrained minimization technique. It is applied here to perform sensitivity analyses on four example problems to demonstrate the computational feasibility and characteristics of the approach. The sensitivity analysis is conducted along the minimizing trajectory for each problem. The convergence characteristics of the first partial derivatives of the variables and objective function with respect to the parameters in the sensitivity analysis are illustrated. Reviews  Computer simulation of sodium fluxes in frog skin epidermis he operation of a seven-compartment model is described with respect to flows of Na+ within and across this system, simulating published results obtained on frog skin. The seven compartments represent: one outside and one inside solution compartment; the subcorneal space; the first reacting cell layer (1. RCL); the remaining cell compartment; the non-, or slowly exchangeable Na+ compartment; the extracellular space. Assuming reasonable volumes for the epidermal compartments and further chosing, by trial and error, appropriate rate constants, a set of seven simultaneous linear differential equations was solved by the application of the Continuous System Modeling Program (CSMP), using an IBM 1130 computer. Initial conditions for influx, backflux and net flux were taken which correspond to [Na+]0; [Na+]i=115mm. Print-out data were obtained at 0.5-min intervals for 30 min, when steady states were obtained in 13 models studied, varying certaink's thus simulating actions of chemical agents (hormones; drugs). Simulation was achieved with regard to rate of influx, backflux and net flux, steady-state time (30 min), and electrical potentials. In addition, this approach gave detailed information on Na+ pool sizes and their variations with changes ink's. These results are compared to published data on frog skin and good agreement between operation of skin epidermis and model was found. Representation of continuous curves on the 3-dimensional rotation group  Analysis of a multi-level time-sharing model The multi-level time sharing algorithm is studied. Each new job joins the bottom level queue. After each quantum of service it goes to the next higher level until it has completed service. The server always selects the job at the head of the lowest level nonempty queue. A constant overhead is incurred for each quantum of service. Mean waiting time and mean system delay cost are better than those of the round robin model. Bernoulli equilibrium states for axiom A diffeomorphisms  Induced function theorems in topology  Note on an extension of “Davidon” methods to nondifferentiable functions Some properties of “Davidon”, or variable metric, methods are studied from the viewpoint of convex analysis; they depend on the convexity of the function to be minimized rather than on its being approximately quadratic. An algorithm is presented which generalizes the variable metric method, and its convergence is shown for a large class of convex functions. Computational experiences with discrete Lp-Approximation fassungDie diskreteLp-Approximation ist insbesondere für 1≤p≤2 von praktischer Bedeutung. Wir schildern die Erfahrungen mit einem Algorithmus zu ihrer numerischen Berechnung.AbstractDiscreteLp-approximation, especially for 1≤p≤2 is useful for practical purposes. We describe the experience with an algorithm for its numerical computation. A stiffness derivative finite element technique for determination of crack tip stress intensity factors A finite element technique for determination of elastic crack tip stress intensity factors is presented. The method, based on the energy release rate, requires no special crack tip elements. Further, the solution for only a single crack length is required, and the crack is “advanced” by moving nodal points rather than by removing nodal tractions at the crack tip and performing a second analysis. The promising straightforward extension of the method to general three-dimensional crack configurations is presented and contrasted with the practical impossibility of conventional energy methods.Résumé Une technique d'analyse par éléments finis est présentée pour la détermination des facteurs d'intensité des contraintes élastiques à la pointe d'une fissure. Basée sur le taux de relaxation d'énergie la méthode ne nécessite pas d'éléments de forme particulière à la pointe de la fissure. En outre, seule est requise la solution pour une longueur déterminée de fissure simple: le processus d'extension de la fissure est obtenu en déplaçant les points nodaux du réseau plutôt qu'en ôtant les composantes de traction nodale à la pointe de la fissure et en procédant à une seconde analyse.On présente les possibilités prometteuses d'extension de la méthode à des configurations tridimensionnelles plus générales de fissures, en contraste avec les impossibilités auxquelles se heurtent les méthodes conventionnelles basées sur des considérations énergétiques.ZusammenfassungDas Verfahren der endlichen Elementen wird angewandt zur Bestimmung der elastischen Spannungsintensitäts-faktoren an einer Rißspitze. Begründet auf die Geschwindigkeit der Energiefreilassung braucht dieses Verfahren keine spezielle Rißspitzenangaben. Weiterhin braucht man nur die Lösung für eine einzige Rißlänge, und der Rill wird fortbewegt eher durch Versetzung von Knotenpunkten als durch Entziehung von Knotenzugspannung an der Rißspitze und durch Ausführung einer zweiten Analyse. Die vielversprechende direkte Ausdehnung der Methode auf allgemeine dreidimensionale Rißgestaltungen wird vorgestellt and der praktischen Unmöglichkeit der klassischen Energie-methoden entgegengestellt. Grades based on criterion-referenced assessment  On the possibilities of studying impulsive elastic waves in an inhomogeneous medium by the finite difference method he method of finite differences is used to study the propagation of impulsive SH waves in simple models of an inhomogeneous elastic medium in the vicinity of a linear source. “Pictures” of the wave field at selected times, theoretical seismograms and wave “profiles” are presented. Even on very simple models the method enables one to study successfully some types of SH waves (e.g., head waves, inhomogeneous, pseudo-spherical waves, etc.), in the study of which other methods break down partly or completely. The numerical algorithm used may be generalized relatively easily in many ways to treat more realistic models. Attention is devoted to the advantages, disadvantages and prospective possibilities of the finite difference method in seismology with a view to the available computer equipment. Sequences with minimal block growth II  An algebraic approach to data organization The paper is concerned with a formal treatment of data structures. The introduction outlines the main features of Generalized Data Base Management Systems and gives the general diagram of information flow in such a system. Section 1 is concerned with the structuring of data on levels. For a given real object setB, the levels of data representation as item, record, and file are discussed and the structure of the levels is characterized. In section 2 a basic algorithm for file search is given. To characterize the file data structure, the concept of directory is defined. A conventional example closes this section. In section 3 the concepts of data base and subdata base are defined as next level of data organization, and the access algorithms to such data structures are described.