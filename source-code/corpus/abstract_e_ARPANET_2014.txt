Improving routing in large networks inside autonomous system The internet is seen as the interconnection between different large networks together. Administrators use interior gateway protocol for routing inside these large networks. Managing routing issues in the network is a challenging task for administrators although they have a choice of selecting from a primitive routing protocol like routing information protocol (version 1) to the advanced routing protocol like open shortest path first protocol or intermediate system-to-intermediate system (IS–IS) protocol. Irrespective of the above mentioned protocols issue of scalability is troubling administrators. They let their network grow up to the limit where it becomes unmanageable. Our analytic study with the help of simulations shows that the these large networks become manageable and improves overall performance of routing and forwarding if route reflectors are used to segment large networks inside autonomous systems. Adaptive maximum-lifetime routing in mobile ad-hoc networks using temporal difference reinforcement learning Mobile ad-hoc NETworks (MANETs) are very dynamic environments. A routing protocol for MANETs should be adaptive in order to operate correctly in presence of variable network conditions. Reinforcement learning (RL) is a recently used technique to achieve adaptive routing in MANETs. In comparison to other machine learning and computational intelligence techniques, RL achieves optimal results at low processing and medium memory costs. To deal with adaptive energy-aware routing issue in MANETs, a RL-based maximum-lifetime routing strategy is proposed. Each mobile node learns how to adjust its route-request packets forwarding-rate according to its energy profile. In terms of RL-resolution methods, Q-Learning, SARSA, Q(λ) and SARSA(λ) which are Temporal difference RL-algorithms are used. The proposed RL model is implemented on the top of AODV routing protocol. Simulation results show that the RL-based AODV achieved good performances in comparison to Time-Delay and Probability based AODV. Particularly, the Q-Learning based AODV has marked the best global performances in terms of energy efficiency and end to end delay. Dynamic distributed unicast routing: optimal incremental paths Developing new mathematical frameworks such as distributed dynamic routing algorithms for constructing optimal incremental paths from a node to another node is an important challenge in data communication networks. These new algorithms can model network resources optimally and increase network performances. A bundle of single routes in a current communication path, which starts from a source node and ends to a destination node, can consist of several successive nodes and links. The Incremental term emphasizes that the number of routes (links and nodes) in a current path can change so that achieving more data rate and optimal efficiency in the network. In this paper, our problem is to add/omit some routes consisting of some nodes and links to/from the current unicast path dynamically and optimally. We call this problem the Optimal Dynamic Distributed Unicast Routing (ODDUR) problem and it is a NP-complete problem. This problem can be formulated as a new type of Linear Programming Problem (LPP) for finding a minimum cost multichannel unicast path, which this path will minimize end-to-end delay and bandwidth consumption along the routes from the source node to the destination node. In this paper, at first a new mathematical framework will be constructed and then this framework will propose the new optimal dynamic distributed unicast routing algorithm for solving our LPP problem. This algorithm will compute an optimal solution for our LPP based on the simplex method and postoptimality computations and will reduce computations and consumed time. Simulation results will show that our new algorithm is more efficient than other available algorithms in terms of utilization of bandwidths and data rate. History of Computer Security The first events in the history of exploiting security date back to the days of telephony. Telephone signals were sent via copper cables. Telephone lines could be tapped and conversations could be heard. In the early days of telephone systems, telephone operators intentionally misdirected calls and eavesdropped on conversations. In the 1970s, a set of people known as phreakers exploited the weakness of digital switching telephone systems for fun. Phreakers discovered the signal frequency at which the numbers are dialed and tried to match the frequency by blowing a whistle and fooling the electronic switching system to make calls for free. Among these phreakers, John Draper found that he could make long-distance calls for free by building an electronic box that could whistle different frequencies The Internet The progenitor of the Internet was the Advanced Research Projects Agency Network (ARPANET), which was funded by the US Department of Defense to enable computers at universities and research laboratories to share information. ARPANET was first deployed in 1969, fully operationalized in 1975, and superseded by NSFNET in 1990. ARPANET’s packet switching and TCP/IP communication protocols formed the backbone of what became the global Internet. TCP/IP Model This chapter gives a brief introduction to the TCP/IP model and the protocols used in each layer. The chapter starts by a historical background about the TCP/IP model and how it all started. The TCP/IP model layers are briefly explained in contrast with the OSI model and its layers. The network access layer is introduced and the flexibility of the TCP/IP model in this layer is also explained. Then, the internetwork layer is explained with brief description of the most important protocols operating in the network layer; IP, ICMP, ARP and RARP, and InARP. The next section provides brief description about the transport layer and the two protocols operating in this layer; TCP and UDP. The last section of this chapter describes the application layer of the TCP/IP model. A brief description of the most commonly used application protocols; HTTP, DNS, FTP and TFTP, SMTP, POP3, and Telnet, is also given. The Impact of the Internet on Global Networks: A Perspective The last quarter of the twentieth century was a time of significant upheaval. Unprecedented advances in computer technology began to collapse vast geographical distances and differences in time and made it possible for people from different parts of the world to form connections in manner not thought possible before. Centred around information, this technological revolution has today transformed the way in which people around the world think, work, share, and communicate. The rise in the number of non-state actors, particularly the emergence of civil society bodies such as NGOs, and the increase of their political influence has thrown up significant questions about how best the Internet and its associated technologies may be harnessed to aid the activities of such organisations and facilitate the formation of international networks.Can the Internet truly augment the effects of those activists seeking to alter the landscape of international relations and advocacy? This chapter seeks to answer this question through an examination of the impact of the Internet on two iconic global networks: the Zapatista Solidarity Movement and the campaign against the Multilateral Agreement on Investment, and the consequences that their innovative use of the World Wide Web and associated platforms in the early 1990s has had on the eventual outcome of each social movement. The work aims to contribute to that body of empirical political science which recognises the impact that information and communication technologies and digital platforms have had on transnational protest and advocacy ever since their development and rapid proliferation. IP-Based Networks and Future Trends A growing number of people are using the Internet, the network of the networks; this is also evident from the different bandwidth-intensive applications supported by Internet and by the considerable number of Internet books, video, etc. that have become available during these years. The widespread diffusion of social networks (Facebook, YouTube, etc.), peer-to-peer traffic, and cloud applications have further contributed to the impressive growth in the Internet use. IP traffic has globally grown eight times in the period 2008–2012 (5 years) and is expected to increase threefold in the next 3 years. The annual global IP traffic will surpass the Zettabyte (i.e., 1021 bytes) threshold by the end of 2016 [1]. This chapter focuses on the protocols and the network technologies to support Internet traffic. The Fragmented Securitization of Cyber Threats Cybersecurity is one of the most pressing national security issues nowadays. Cyber threats reached truly global scales, cyber attacks that potentially or actually cause physical damage are on the rise, and securing critical infrastructures against cyber incidents is seen as a priority by many. Virtually every national cybersecurity strategy points out the importance of the international cooperation in this field, and there have been initiatives for a global cybersecurity treaty as well. Although a number of national and regional policy and legal instruments exist in this field, the conclusion of a truly international treaty remains a highly controversial topic. The aim of this chapter is to identify the factors that make such a global cybersecurity treaty (un)viable. It will begin with an overview of the history of cybersecurity and its early securitization process by the USA and Russia, and then, the focus will shift to the present strategic approaches and responses. Introduction The word ‘mathematics’ was coined by Pythagoras, who flourished around 500 bc. It meant ‘a subject of instruction,’ and its first part, ‘math,’ comes from an old Indo-European root that is related to the English word ‘mind.’ The Pythagoreans grouped arithmetic, astronomy, geometry, and music together and for several centuries mathematics referred to only these four subjects. However, as we proceed it will become clear that the study of arithmetic, astronomy, and geometry began long before Pythagoras. In fact, a fair majority of the biggest breakthroughs in mathematics were made possible through the work of people other than those who have been credited in the history books. Playful Interfaces: Introduction and History In this short survey, we have some historical notes about human–computer interface development with an emphasis on interface technology that has allowed us to design playful interactions with applications. The applications do not necessarily have to be entertainment applications. We can have playful interfaces to applications that have educational goals or that aim at behavior change, whether it is about change of attitude or opinion, social behavior change, or physical behavior. For the developer and the designer of these applications and their interfaces, there is no need any more to assume that, in addition to focusing on the application, the user has to pay attention to manipulating a mouse, using the keyboard and monitoring the screen. Smart sensors and actuators embedded in a user’s physical environment, objects, wearables, and mobile devices can monitor a user, detect preferences and emotions and can re-actively and proactively adapt the environment and the behavior of its actuators to demands of the user or changing conditions. In this way, interface technology can be employed in such a way that the emphasis is not on offering means to get tasks done in the most efficient way, but on presenting playful interaction opportunities to applications that provide fun, excitement, challenges, and entertainment. Clearly, many applications that have more serious goals than “just” providing fun can profit from this interface technology as well. In this introductory chapter, we shortly survey the chapters in this book that show the many applications of these playful interfaces. 1960–1979 This chapter chronicles the development of permutation statistical methods from 1960 to 1979. This was a period that witnessed dramatic improvements in computer technology, a process that was integral to the development of permutation tests. Prior to 1960, computers were based on vacuum tubes and were large, slow, expensive, and availability was severely limited. Between 1960 and 1979 computers became based on transistors and were smaller, faster, more affordable, and more readily available to researchers. During this period, work on permutation tests fell primarily into three categories: writing algorithms that efficiently generated permutation sequences; designing exact permutation probability values for known statistics; and, for the first time, the development of statistics specifically designed for permutation methods. Introduction Recent development of information and communication technology enables us to acquire, collect, analyse data in various fields of socioeconomic-technological systems. In this chapter, we will address data from several different perspectives and define the applied data-centric social sciences. I will explain that limitation of our ability to understand our society from inductive approach is origins of complexity. Concepts and methodologies of data-centric science will be introduced and their potential applications and existing studies will be mentioned. Introduction Exchanging information electronically has almost become a necessity in today’s world. Starting with the traditional telephone networks, electronic communication has evolved significantly over the years. Today we have networking technologies that cater for information exchange at all geographical scales. Historical Overview Probably nothing has impacted the advance of humankind more significantly than its ability to communicate and exchange information with one another. The knowledge of how to retain the contents of communication and to pass it on – also over great distances – has given communities a significant advantage, assured them of survival and cemented their position of supremacy. The development of writing and paper, as a transportable communication medium, soon led to the establishment of regular messenger services and the first postal system. Already in ancient civilizations optical telegraphic media, such as smoke or torch signals, were used. These enabled the fast transport of messages over large distances with the help of relay stations. The Industrial Revolution, and the heightened need for information and communication accompanying it, accelerated the development of the optical and electrical telegraph, both of which appeared at the same time. Initially available only to the military, government and business, this long-distance communication media gained increasing importance in private communication. The development of the telephone started a huge demand for private communication, also to far-off locations, and led to rapid growth. Development surged in the 19th and 20th centuries thanks to the invention of the phonograph, gramophone, photography, film, radio and television. Mass media was born and continues to shape our society. On the road to total networking the world has became a global village with Europe, America and Asia only a mouse click from each other in the WWW. Hacking against the Apocalypse: Tony Stark and the Remilitarized Internet Manuel De Landa’s War in the Age of Intelligent Machines (WAIM) is an excellent history of the cross-pollinations of military thought and technological advances, focusing alternately on the evolution of larger hardware components (ballistic missiles, computers) and on the “control machinery” (155), or software, that directs that hardware. Through the eyes of his robot historian, the Internet and modern computing was incubated by the American military: driven by a scarcity of computing supplies and resources, Paul Baran at the Research ANd Development (RAND) Corporation had “convinced government officials the United States needed to establish a distributed communications system that could withstand a nuclear attack” (Moshovitus, 35). This meant the creation of a “distributed network,” one that sent “small ‘packets’ of information” over a number of different routes (Ryan, 15); this proposed system shunned the “traditional network” (one unit of information over one single path, as exampled by “AT&T’s highly centralized national telephone network” [Ryan, 13]) and would instead “allow information to be sent over many different possible routes” (Moshovitus, 35). The first such space, ARPAnet, was created in December 1969 (ibid., 61); while it publicly debuted at the 1972 International Conference on Computer Communications, it wasn’t until “a rumor, suggesting army intelligence officers had used ARPAnet files to relocate files concerning the whereabouts and behaviors of political activities” that knowledge of its existence became widespread (ibid., 87). Web Technologies When developing a mashup, different Web standards and languages may be needed and taken into account. Client-side scripting languages are, for instance, largely adopted for Web mashups. In the last decade, they renovated the Web by introducing business logic capabilities at the client-side, and they are among the most important technologies that promoted mashups and can be perhaps considered characteristic for such a class of applications. The adoption of lightweight development practices, based on the integration of resources through client-side logic, is one of the aspects that most differentiate mashups from other integration practices. However, other classes of mashups may also require server-side technologies. This happens, for example, for enterprise mashups where there is a need for integrating enterprise data assets and remote resources, possibly in conjunction with complex workflow systems. This chapter gives an overview of the most relevant technologies and their opportunities and limitations for the development of mashups that are delivered through the Web. It starts by illustrating the basic ingredients of the Web, for example, the Hypertext Transfer Protocol (HTTP) and the Hypertext Markup Language (HTML), and then introduces the different client-side technologies that in the last years have led to the development of rich and interactive applications. The chapter then reviews the most prominent server-side technologies and shortly discusses common data formats adopted for representing data exchanged on the Web. Introduction to Security Scenario 1: A post on http://threatpost.com, Threatpost, the Kaspersky Lab Security News Service, dated August 5th, 2013 with the title “BREACH Compression Attack Steals HTTPS Secrets in Under 30 Seconds” by Michael Mimoso, states1: Digitization and Sustainability When the first Earth Day was celebrated on April 22, 1970, the assemblage of ideas, artifacts, and practices that is now known as the Internet was a research and development program of the Advanced Research Projects Agency (ARPA) in the U.S. Department of Defense. At the time, ARPANET, as it was called, connected a few dozen researchers at eight corporate and university sites around the country. Outside this limited circle, few could imagine what lay in store, but the ensuing tsunami of digital devices and systems that has since washed over society is arguably the most significant sociotechnical development of the intervening decades.1 Digital Art and Cultural Commentary This chapter establishes grounds for perceiving digital art as a critical voice—one that can facilitate both personal and affective responses to globalisation. It posits that digital art is borne of a turbulent era, sharing an intimate history with globalisation. This chapter shows how art can function as a platform for cultural commentary. It also provides means for considering and contextualising the digital art case studies presented within the text. Playing Hide-and-Seek with Virus Scanners This chapter tries to bridge the gap between a fundamental topic in Computer Science, namely how computer processors execute programs, and a topic in information security, namely computer viruses.It starts by introducing the concept of a fetch-decode-execute loop, and the implication of Harvard versus von Neumann architectures. By adopting a step-by-step approach and some very simple programs, the goal is show there is no magic involved: even complex, modern computer processors are based on fairly simple principles which everyone can understand. Using this background, the chapter explores a technical mechanism used by computer viruses to evade detection by virus scanners.Specifically, the ability for a program to modify itself during execution (so-called self-modifying code) allows polymorphic viruses to hide their intentions from a scanner seeking to detect them. How Amsterdam Invented the Internet: European Networks of Significance, 1980–1995 In January of 1994, the Internet became available to the general public in the Netherlands via a new dial-in service and virtual access area called De DigitaleStad (Digital City, called DDS). Hailed as a new form of public sphere, DDS visualized the Internet as a form of a virtual city. Rather than trace how DDS gave shape to an online city, however, this chapter explores how an existing and emerging culture of the city gave rise to this new digital sphere. In particular, it highlights how actors from a range of independent media labs and cultural centers helped to invent the participatory city culture that was visualized within DDS. First, it traces the growth of Amsterdam as a central node and gateway of the Internet in Europe in parallel with the rise of independent media and cultural centers in the 1980—a culture related, among other things, to the squatter’s movement and worldwide activist groups fighting social injustice. The chapter then shows how these sectors came together in the late 1980s with the involvement of a third set of actors, the hacking community, to shape what would become Digital City and Amsterdam’s booming digital culture. Through a series of network events that brought these groups together, a digital culture took shape that eventually gave shape to the city’s digital culture. From Total Information Awareness to 1984 President Barack Obama, in his 2011 State of the Union Address, called America “the nation of Edison and the Wright brothers” and “of Google and Facebook” (1). On May 18, 2012, Facebook offered the largest technology IPO in history and the third largest U.S. IPO ever, trailing only Visa in March 2008 and General Motors in November 2010 (2). Selling 421.2 million shares to raise $16 billion, Facebook has a $104.2 billion market value and is more costly than almost every company in the Standard & Poor’s 500 Index (3). Former U.S. Treasury secretary Lawrence H. Summers called Facebook’s offering “an American milestone” comparable to Ford and IBM’s, “Many companies provide products that let people do things they’ve done before in better ways. Most important companies, like Ford in its day or I.B.M. in its, are those that open up whole new capabilities and permit whole new connections. Facebook is such a company” (4). However, the Facebook IPO was also one of the biggest opening flops in stock market history. The stock was down by 16.5% at the end of its first full week of trading; and it took over a year for the stock to recover (see Appendix B). Social History of Computing and Online Social Communities  Introduction to Telecommunication Networks Before focusing our interest on telecommunication networks, it is important to take a brief look at the history of telecommunications, referring to the most important steps, which are at the basis of modern transmissions of signals at distance. Global Cyber-Security Policy Evolution The aim of this chapter is to show how cyber-security has evolved as a policy issue globally and how the Swiss cyber-security approach has been influenced by this evolution. To this end, this chapter introduces factors that shape cyber-security policy development more generally and then introduces four different ways of “framing” the cyber-security issue: a technical, a crime-espionage, a civil defense, and a military variation. All four are interrelated and exist side by side in every country, but they can be distinguished by a main set of actors with that particular view, by the main referent object these actors/communities tend to focus on and on the particular threats/risks they would be mainly concerned with. This chapter gives examples of specific Swiss institutions and developments that belong to each of the four variations. Art, Society and Ethics: Adorno’s Dialectic of Enlightenment, Aesthetic Theory and Pynchon ‘Myth is already enlightenment, and enlightenment reverts to mythology’ proclaims the introduction to Adorno and Horkheimer’s Dialectic of Enlightenment (xvii). This chiastic statement lies at the core of this work’s account of a fundamental incompatibility between enlightenment’s goals of ‘liberating human beings from fear’ (the freedom that Adorno and Horkheimer believe is inseparable from enlightenment thinking) and the simultaneous state of ‘the wholly enlightened earth’ as ‘radiant with triumphant calamity’ (DoE, 1). The key to grasping this interrelation of enlightenment and myth lies in the depiction of nature, to which one subsection will here be dedicated. Nature, for the longest period, was deemed to hold a degree of enchantment; it was intrinsically meaningful. The abstracted tales that correlate to such a foundationalist stance are myths. Conversely, at the dawn of the Age of Reason there began a progressive disenchantment of nature: ‘[f]rom now on, matter was to be controlled without the illusion of immanent powers or hidden properties’ (DoE, 3). The world and all aspects therein were available to be used and understood; there was no longer any intrinsic meaning: ‘[o]n their way toward modern science human beings have discarded meaning’ (3). This disenchantment of nature is termed enlightenment. From 1984 to Total Information Awareness In April 1984, President Ronald Reagan signed the National Security Decision Directive (NSDD) 138: Combating Terrorism, which authorized the increase of intelligence collection directed against groups or states involved in terrorism (1). I was a graduate student at Virginia Tech majoring in computer science with specialization in artificial intelligence (AI). Introduction, Money Laundering and Cyber crime Cyberlaundering refers to the way in which the mechanism of the internet is used to launder illegal proceeds of crime in order to make such proceeds appear clean. The advent of the internet has yielded this new breed of crime. On the internet, there are various avenues exploited by criminals to convert ‘dirty’ money into ‘clean’ money (Prevalent avenues, amongst a host of others, include online banking, online gambling, e-gaming, online auctioning and digital payments methods. These methods will be further explained in detail subsequently). What has been understood as money laundering in the past is not necessarily the case anymore. Money laundering now wears the cloak of cyberlaundering, because criminals today are one step ahead of the law. New ways are constantly being devised to evade the prying eyes of law enforcement, hence the shift to cyberlaundering.This chapter lays the foundation for the core of this study, embodied in the subsequent chapters that follow. The chapter begins by establishing the importance of the study; the significance and rationale for the study. More importantly, the chapter identifies money laundering and cyber crime as the fundamental two phenomenons that have birthed cyberlaundering. It explores these two disciplines separately in order to solidify a foundational understanding of cyberlaundering, with a view to its legal regulation. Computer Algebra Systems First attempts to use computers for calculations not only with numbers but also with mathematical expressions (e.g., symbolic differentiation) were made in the 1950s. In the 1960s research in this direction became rather intensive. This area was known under different names: symbolic calculations, analytic calculations, and computer algebra. Recently this last name is most widely used. Why algebra and not, say, calculus? The reason is that it is most useful to consider operations usually referred to calculus (such as differentiation) as algebraic operations in appropriate algebraic structures (differential fields). Introduction There is no doubt that the development and implementation of information and communication technology (ICT) during the last decades has had – and still has – a major impact on all levels of society. One only has to think of the Arab Twitter Revolution in the spring of 2011. Both society as a whole and individual lives have changed dramatically as a result of ICT implementation. In this book, the focus is on the effects of ICT on work and, more in particular, on quality of working life. That ICT has had an impact on working life is without a doubt: whole jobs that had been around for “ages” have disappeared (e.g., the teller at a bank, the stenotypist) and have been replaced with other, new sort of jobs (e.g., database manager, information technology specialist, web designer, etc.). Apart from jobs that have disappeared, nearly all existing jobs have changed tremendously. Even in “old-fashioned jobs” such as the agricultural industry or construction industry, ICT has a major impact. Most of the jobs have changed tremendously because access to and exchange of information have become many times faster. Only 20 years ago, if we needed specific information from colleagues and friends, such as a copy of a recent published journal article, we sent letters by postal mail, which would take at least 2 days to arrive, and it would last at least another 2 days before the response would get back to the sender. Nowadays, the exchange of information is almost instantaneously. Instead of having to go to the library and use all kinds of antiquated search systems to find a certain article or book, we use Google and the results are shown in milliseconds. The big question however is: does this instantaneous access to and exchange of information make us happier? Does it improve the quality of our working lives? In this chapter, we briefly describe the development of information and communication technology (ICT) and the effects of ICT in particular on work. Social Networking Analysis A social network indicates relationships between people or organisations and how they are connected through social familiarities. The concept of social network provides a powerful model for social structure, and that a number of important formal methods of social network analysis can be perceived. Social network analysis can be used in studies of kinship structure, social mobility, science citations, contacts among members of nonstandard groups, corporate power, international trade exploitation, class structure and many other areas. A social network structure is made up of nodes and ties. There may be few or many nodes in the networks or one or more different types of relations between the nodes. Building a useful understanding of a social network is to sketch a pattern of social relationships, kinships, community structure, interlocking dictatorships and so forth for analysis. Cloud Computing Evolution Cloud computing has emerged as a cost effective alternative to having reliable computing resources without owning any of the infrastructure. The growth of this technology mirrors the growth of computing in general. The options offered by cloud services fit the needs of businesses of all types. As a truly global technology, cloud computing is growing rapidly, albeit without any global standards. The benefits of cloud computing are too numerous to hold back adoption. At present the goal is to meet the business needs and as the technology matures it will accommodate changes emanating from global standards. As the first step in this direction many of the major cloud service providers are joining multiple consortia to develop the standards. This chapter addresses the history of the growth of cloud computing and the three basic service types—SaaS, PaaS, IaaS—that help businesses of all types. We identify the major cloud service providers and the cloud service types that they offer. We discuss the ways in which cloud computing is supporting entrepreneurial activities. Our analysis shows further that the advancements in communications technology is benefiting cloud computing and makes it a truly global service. Moreover, cloud computing technology is making a major contribution to ecommerce. Understanding Networks and Network Security Before we discuss network vulnerabilities and threats, we should understand why such threats exist. In order to understand this, we need to know the basics of computer communication and networking. In this chapter, we will be discussing the basics of computer networking, Open System Interconnection (OSI), and Transport Control Protocol/Internet Protocol (TCP/IP) models, and types of networking vulnerabilities that exist and then will explore on the relevant vulnerabilities and threats. A Brief History of Acoustics 
              Although there are certainly some good historical treatments of acoustics in the literature, it still seems appropriate to begin a handbook of acousticshistory acoustics with a brief history of the subject. We begin by mentioning some important experiments that took place before the 19th century. Acoustics in the 19th century is characterized by describing the work of seven outstanding acousticians: Tyndall, von Helmholtz, Rayleigh, Stokes, Bell, Edison, and Koenig. Of course this sampling omits the mention of many other outstanding investigators.
            
              To represent acoustics during the 20th century, we have selected eight areas of acoustics, again not trying to be all-inclusive. We select the eight areas represented by the first eight technical areas in the Acoustical Society of America. These are architectural acoustics, physical acoustics, engineering acoustics, structural acoustics, underwater acoustics, physiological and psychological acoustics, speech, and musical acoustics. We apologize to readers whose main interest is in another area of acoustics. It is, after all, a broad interdisciplinary field.
             Introduction Humanity has dreamt of traveling beyond our cradle and into space since, at least, the time of the Roman conquest of Greece, after the Battle of Corinth in 146 BC. In what is considered the earliest known fiction about travel to outer space, alien life-forms and interplanetary warfare, Lucian of Samosata wrote a satirical piece in 150 AD called True History. In part of this story a company of adventuring heroes are swept upwards in a giant waterspout shortly after sailing westward through the Pillars of Hercules (today known as the Strait of Gibraltar). After seven days and seven nights, they arrive on the Moon to find themselves embroiled in a war between the King of the Moon and the King of the Sun over the Morning Star (Venus). The Sun wins. Bibliography ■■■ Understanding Cyber Warfare The history of computing devices goes back to the invention of the abacus in Babylonia in the sixteenth century BC. In the three and a half millennia which followed, a variety of calculating devices were introduced, some of them stunningly sophisticated but all sharing a common limitation: a device could store data only as long as programs for the data manipulation remained in the mind of the user. Supporting Learning with Interactive Surfaces and Spaces In recent years, educational research on interactive surfaces such as tablets, tabletops, and whiteboards, and spaces such as smart rooms and 3D sensing systems has grown in quantity, quality, and prominence. Departing from the mouse-and-keyboard form of input, users of these systems manipulate digital information directly with fingers, feet, and body movements, or through a physical intermediary such as token, pen, or other tractable object. Due to their support for natural user interfaces, direct input and multiple access points, these educational technologies provide significant opportunities to support colocated collaborative and kinesthetic learning. As hardware becomes affordable, development environments mature, and public awareness grows, these technologies are likely to see substantial uptake in the classroom. In this chapter, we provide a foothold on the current technology development and empirical literature, highlighting a range of exemplary projects that showcase the potential of interactive surfaces and spaces to support learning across age groups and content domains. We synthesize across the existing work to formulate implications of these technological trends for the design of interactive educational technologies, the impetus for academic research based on such systems, and the advancement of future educational practice. Distributed Probabilistic Search and Tracking of Agile Mobile Ground Targets Using a Network of Unmanned Aerial Vehicles As technologies in digital computation, sensing, wireless and wired communications, embedded systems, and micro-electro-mechanical systems continue to advance in the coming years, it is certain that we will see a variety of distributed sensor networks (DSNs) being deployed in an increasing number of systems such as power distribution systems, engineering structures and buildings, smart homes, environmental monitoring systems, biomedical systems, military systems, and others. In addition, unlike the traditional networks of sensors, the mobility afforded by autonomous systems, embedded systems, and humans who carry smart sensing devices will contribute in creating new and exciting future sensor networks. These future networks of sensors that take advantage of man-machine interactions will also introduce new applications yet unknown to us. In this paper, we present the origin and time line of DSN development, analyze the benefits and challenges of DSNs, and present a mobile sensor network in the form of an unmanned aerial vehicle (UAV) team using distributed mission area probability maps to search and track mobile ground targets. We propose a novel update strategy for the probability map used by UAVs to store probability information of dynamic target locations in the search area. Two update laws are developed to accommodate maps with different scales. Simulation results are used to demonstrate the validity of the proposed probability-map update strategy. The Utility of Timeless Thoughts: Hannah Arendt’s Conceptions of Power and Violence in the Age of Cyberization This chapter links power relations in the technologically dominated context of cyberspace to Hannah Arendt’s theoretical considerations of power and violence. Even if her work is often marked by skepticism on the technological domination of the human world, some of Arendt’s most important works provide a surprisingly rich framework to conceptualize the structure and character of cyberspace. It is argued here that the structure of power and violence in cyberspace can abstractly be captured by dividing cyberspace into two parts that refer to Arendt’s conceptions of power as power to and violence as power over. Cyberspace is thus both, a modern space of appearance and political freedom and an unexplored context for Arendt’s conception of power as well as an anti-space of appearance, a space filled with Arendt’s conception of violence that denies the positive attributes of a space of appearance when filtering and control techniques are implemented. The empirical cases of the Arab Spring protests, Weibo and the Fifty Cent Party as well as Denial of Service (DoS) attacks during elections or inter-state conflicts will underline this argumentation. Information and Communication Technology and Quality of Working Life: Backgrounds, Facts, and Figures The Digital Revolution that took place in the past 30 years has had and continuous to have a major impact on our lives. The introduction of personal computers in the 1980s, the Internet in the 1990s, smartphones in 2000–2010, and social networking since 2005 has caused changes in the way we live our lives and work that can only be compared to the effects of the Industrial Revolution. In this chapter, the developments of the development and implementation of information and communication technology (ICT) will be described briefly, as well as the impact it has on our quality of working life. Data from five countries will be used to examine the impact of ICT on intensification and satisfaction of employees in these five countries. The Virtual Centimeter World Model Limitless sensing at ever greater detail, storage at nearly no cost and GPU-enhanced high performance computing have vastly reduced previous limits to the processing and use of digital images in Computer Vision. If citizens collect images of ever improved quality at a centimeter pixel size and with great density, thus high image overlaps, of our environment, if Internet-based image management systems assemble these photographs to meaningful image blocks at quantities in the realm of Exabytes, when 1 million images can be processed per day fully automatically into 3D Geo-information, can we then expect an emergence of very detailed 3D models of our entire urban and rural World? We argue that yes, 3D models of the World are feasible at a detail in the range of centimeters with current technology. Since that technology continues to evolve, the likelihood increases rapidly that such detailed World models will be created. Global aerial orthophotos in the decimeter range are being produced today; centimeter-type pixels are being collected along the entire street network of major cities. Very little is needed to convert such data into the reality of the Virtual Centimeter World Model at pixel-accuracy for a mixed reality experience.
 Local Area Networks and Analysis Traditional networks make use of point-to-point links, i.e., channels, which are dedicated to couples of nodes. There is no interference among these channels: the transmission between a pair of (source/destination) nodes has no effect on the transmission between another pair of nodes. However, point-to-point links require the topology to be fixed, determined during the network design phase. Distances in Networks A network is a graph, directed or undirected, with a positive number (weight) assigned to each of its arcs or edges. Real-world complex networks usually have a gigantic number N of vertices and are sparse, i.e., with relatively few edges. Computer Architectures for Health Care and Biomedicine After reading this chapter, you should know the answers to these questions: The Herd Instinct Person-to-person interactions underlie many kinds of group behaviour, from panicking crowds, to clothing fads, to booms and busts in the share market. The growth of the Internet, for example, led to the dot-com bubble at the turn of the Millennium. Communication provides the glue that keeps social groups together. Peer pressure forces people to conform to group morality. New technologies are changing the ways in which people communicate and therefore the way influence spreads. They also make it possible for groups to cooperate in new ways, as highlighted by spontaneous planning during the Paris riots of 2005 and by the electronic collaboration used by groups, such as SETI@home. Socket Names and DNS Having spent the previous two chapters learning the basics of UDP and TCP, the two major data transports available on IP networks, it is time for me to step back and talk about two larger issues that need to be tackled, regardless of which data transport you are using. In this chapter, I will discuss the topic of network addresses, and I will describe the distributed service that allows names to be resolved to raw IP addresses. Introduction to Multimedia In this chapter, we discuss the uses of the term “multimedia” since people may have quite different, even opposing, viewpoints on what this means. This textbook is aimed at computer science or engineering students, and consequently a more application-oriented view of what multimedia consists of is what is emphasized. The convergence going on in this field, with computers, smartphones, games, digital TV including 3D, multimedia-based search, and so on converging in technology, means that multimedia is a field that is essentially mandatory for such students to study. Moreover with the pervasive penetration of wireless mobile networks and development of mobile applications for smartphones and tablets, and the advent of social media, the contents of a multimedia course arguably forms the basis for much of the further studies many students will engage in. The components of multimedia are first introduced and then current multimedia research topics and projects are discussed to put the field into a perspective of what is actually at play at the edge of work in this field. For a fuller perspective, the remarkably short history of multimedia is synopsized, from the development of the World Wide Web up to current pervasive social media and anytime/anywhere access. Since multimedia is indeed a practical field, Chapter 1 also supplies an overview of multimedia software tools, such as video editors and digital audio programs, that are typically used to produce multimedia products such as those that are indeed produced in a course in this subject. How Does the Web Work? We’re going to take a break from writing Dart programs (but we hardly got started, you say!) to understand the fundamentals of the World Wide Web, in preparation for working with it more extensively in our Dart programs. Hopefully you will find this detour and the hodge-podge of information about the Web that it features informative and interesting. If you already know everything in this chapter, then you’re quite well informed about technology for someone reading an introductory programming book… Given what a large impact the Web has had on everyone’s lives, it would probably be good if the public at large knew more about how the Web works.