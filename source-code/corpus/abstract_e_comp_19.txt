Regressive effects of regulation Regulation of health and safety has placed an unacknowledged burden on low-income households and workers. Billions of dollars are spent every year on regulations that seek to reduce life-threatening risks that arise from auto travel, air travel, air and water pollution, food, drugs and construction; the list goes on. Today, some form of regulation affects nearly every aspect of our lives (Shleifer, in: Kessler (ed) Regulation vs. litigation: perspectives from economics and law, University of Chicago Press, Chicago, 2010). All of the regulatory rules ostensibly intend to make consumers or workers better off, but the cost of regulation usually is borne by the same consumers and workers, reducing their ranges of choice; it therefore crowds out private spending. The crowding out effect can be particularly detrimental for low-income households. This special issue explores the various ways in which regulation may have such regressive effects as well as the political determinants of how regulation, despite its unfavorable consequences for low-income households, may come about. RegData 2.2: a panel dataset on US federal regulations How much regulation exists? Can short- and long-term growth trends in regulation be identified? Which agencies produce the most regulation? Are some sectors of the economy more regulated than others, and how big are the differences? RegData 2.2, a recent panel dataset from the RegData Project at George Mason University’s Mercatus Center, offers answers to these questions and more. RegData 2.2 quantifies various aspects of US federal regulations by industry, by agency, and over time. The resulting datasets include metrics on volumes, restrictiveness, and relevance of federal regulations to different economic sectors and industries. RegData datasets are publicly released at http://quantgov.org. We explain the features of and methodology underlying RegData 2.2. Stratification by regulation: Are bootleggers and Baptists biased? This paper investigates whether and to what extent regulation may be associated with wage inequality. Using regulation measures created by Al-Ubaydli and McLaughlin (Regul Govern 11:109–123, 2017), I find that regulation is associated with larger within-occupation wage inequality. Specifically, I show that a worker at the 90th wage percentile realizes a raise of $1.19 per hour relative to the 10th percentile earner for each standard deviation increase in regulation. That represents a 3.5% raise for a worker at the 90th percentile. Overall, increases in the regulatory burden are associated with 42–45% of the change in the 90th–10th percentile wage ratio from 2002 through 2014. Regulation and poverty: an empirical examination of the relationship between the incidence of federal regulation and the occurrence of poverty across the US states We estimate the impact of federal regulations on poverty rates in the 50 US states using the recently created Federal Regulation and State Enterprise (FRASE) index, which is an industry-weighted measure of the burden of federal regulations at the state level. Controlling for many other factors known to influence poverty rates, we find a robust, positive and statistically significant relationship between the FRASE index and poverty rates across states. Specifically, we find that a 10% increase in the effective federal regulatory burden on a state is associated with an approximate 2.5% increase in the poverty rate. This paper fills an important gap in both the poverty and the regulation literatures because it is the first one to estimate the relationship between the two variables. Moreover, our results have practical implications for federal policymakers and regulators because the greater poverty that results from additional regulations should be considered when weighing the costs and benefits of additional regulations. How to keep your systems records safe The success of an institution depends on its ability to acquire accurate and timely data about its operations, to manage and use this data effectively. Computer laboratory record management plays a vital role in the organization and maintenance of computers in an institution. In existing system of Mehran University of Engineering and Technology, Jamshoro lab assistants are responsible to view the current status and configuration of individual computers in LAN based computer labs. The other and very important problem which lab assistants face is that they don’t know automatically which hardware and software changes are being done on any system. In this paper a Computer Laboratory Record Management System (CLRMS) is proposed to avoid such kind of problems. CLRMS is a network based Software which automatically keeps all the information of system including hardware and software. This software secures the records of system configuration for future information reporting, user management and security management. It also updates system database if any hardware is removed or any software changes occur via automatically retrieval method. CLRMS interfaces are easily understandable hence user friendly. It generates reports for both software and hardware. It shows complete structured configuration of a computer. It will keep records that which type of user was log on the system and what tasks he has performed like installation information of software, updates of software, version and vendor information. Design and implementation of I2C interface on FPGA for space borne AIS receiver in embedded system This paper concentrates on the Automatic Identification System (AIS receiver) and various interfaces on FPGA which is used in the satellite. The AIS is used to detect a collision of ships. Vessels equipped with AIS provide vessel navigation information like their speed, location etc. Satellites that are fitted with S-AIS receivers, receives the information and transmits it to the on-board computers. Transmission of information requires various interfaces. The paper presents the design and implementation of I2C and MIL-STD-1553 bus protocol, which interfaces FPGA board and on board computers in satellite and synthesized on Virtex-5 FPGA in Xilinx ISE 14.2 platform. The functional simulation of the bus is also carried under different test cases. Small satellites make use of an I2C bus. For the purpose of interfacing low-speed peripheral device on FPGA, I2C bus which is a multi-master, the two-wire bi-directional serial bus is used. The MIL-STD-1553 bus is a standard data bus, mostly use in spacecraft on-board data handling subsystem in the military. In this, communication is in between one master terminal called bus controller and other slave terminals called remote terminal (RT). MIL 1553 bus can hold up to 30 RT. Component based metric for evaluating availability of an information system: an empirical evaluation The aim of the paper is to empirically evaluate the quantitative Availability metric derived from the dependencies among the individual measurable components of an information system. The Availability metric is twofold, based on the operating program and the network delay metric of the information system (for the local bound component composition the availability metric is purely based on the software/operating program, for the remote bound component composition the metric incorporates the delay metric of the network). The metric is used for measuring Availability of an information system from the security perspective, the measurements may be done at the system-design level or for a developed system the metric is applied to the individual working components (software/program code. The system to be evaluated is a network based video monitoring system EES and all the measurements are done using the source code of the system. The steps mentioned in the availability evaluation algorithm are followed in the evaluation process of the system and the final output of the algorithm is the availability score IAV(SyS) for the EES system. The score gives an indication of security of the system with the current design. Editorial  Wavelet neural network model for network intrusion detection system Network Intrusion Detection is the process of analyzing the network traffic so as to unearth any unsafe and possibly disastrous exchanges happening over the network. In the nature of guaranteeing the confidentiality, availability, and integrity of any networking system, the accurate and speedy classification of the transactions becomes indispensable. The potential problem of all the Intrusion Detection System models at the moment, are lower detection rate for less frequent attack groups, and a higher false alarm rate. In case of networks and simulation works signal processing has been a latest and popular technique. In this study, a hybrid method based on coupling Discrete Wavelet Transforms and Artificial Neural Network (ANN) for Intrusion Detection is proposed. The imbalance of the instances across the data-set was eliminated by SMOTE based oversampling of less frequent class and random under-sampling of the dominant class. A three-layer ANN was used for classification. The experimental results on KDD99 data-set advocate about the fact that the proposed model has higher accuracy, detection rate and at the same time has reduced false alarms making it suitable for real-time networks. Secure routing in mobile Ad hoc networks: a predictive approach In recent years, wireless technologies have gained enormous popularity and used vastly in a variety of applications. Mobile Ad hoc Networks (MANETs) are temporary networks which are built for specific purposes; they do not require any pre-established infrastructure. The dynamic nature of these networks makes them more utilizable in ubiquitous computing. These autonomous systems of wireless mobile nodes can be set up anywhere and anytime. However, due to high mobility, absence of centralized authority and open media nature, MANETs are more vulnerable to various security threats. As a result, they are prone to more security issues as compared to the traditional networks. Ad hoc networks are highly susceptible to various types of attacks. Sequence number attacks are such hazardous attacks which greatly diminish the performance of the network in different scenarios. Sequence number attacks suck some or all data packets and discard them. In past few years, various researchers proposed different solutions for detecting the sequence number attacks. In this paper, first we review notable works done by various researchers to detect sequence number attacks. The review thoroughly presents distinct aspects of the proposed approach. In addition, we propose a proactive predictive approach to mitigate sequence number attacks which discovers misbehaving nodes during route discovery phase. The proposed approach suggests modifications in Ad hoc on-demand distance vector (AODV) routing protocol. Research and implementation of event extraction from twitter using LDA and scoring function With the fast growth of social media, interest is increasing in detecting popular events from tweets. Event extraction is a work which identifies events from tweets or database of tweets. Each and every day, hundreds of megabytes of current stories are being added into the news archives of the major news agencies, containing much important and interesting news. Aim of this event extraction strategy is to extract and retrieve major life events from twitter data. Example of events extraction include seminar presentation, Job opening, Admission in Top universities, new technology etc. The role of this extraction is to collect major life events in the form of retrievable entries that include structured data about major life event name, location and time. Most of previous research on event extraction was mainly on textual level extraction such as News, medical systems, text summarization, whereas less work has been done on event extraction from noisy text such as tweets. For instance, tweets are short and self-contained which make them lack useful information. The target of this research is to develop algorithm and methodology that extract and efficiently conclude major life events extracted from social media. Load prediction analysis based on virtual machine execution time using optimal sequencing algorithm in cloud federated environment Virtual machine (VM) prediction and an effective resource management are the attractive areas in the cloud environment. VM prediction is an important task to execute the jobs for delay minimization and unnecessary states avoidance. Cloud computing attracted towards the increase in a number of applications that run on remote servers in parallel manner. Increase in parallelism reduces the CPU utilization adversely. Hence, the proper VM prediction and management are necessary stages in provisioning scheme. Also time required for allocating jobs is more in existing algorithms due to the number of computations involved. Therefore a novel algorithm is required to improve the performance of the job allocation with makespan reduction. In this paper the new algorithm is proposed that includes the VM capacity and execution time for load prediction and performance improvement purpose. Our proposed research work utilizes the VM clustering and optimization algorithms to improve job sequencing performance. The cost computation prior to clustering includes the VM capacity as a major factor. Clustering of VM with high-cost and isolation of low-cost and high-cost clusters reduces the searching time of VM and solve the imbalance state problem in traditional methods. The optimization algorithm with suitable initialization function reduces the time and steps for selection of VM for suitable job. The proposed model outperformance is established by the selected parameters. A study of SQL query processing using soft computing techniques: a hybrid vague logic approach As we know that in this new era, the availability of modern data sets is massive. It is very difficult to find the variation and appropriate class for a data set. So, this paper introduces a comparison between fuzzy and vague sets for handling Structured Query Language (SQL) processing problems. This paper proposed a new method to convert crisp set into vague set with help of Positive Ordered Transformation formula (POTF). Further, vague sets are converted into fuzzy sets with help of Transforming Vague Set into Fuzzy Set method proposed by Liu et al. (Trans Comput Sci II LNCS 5152:133–144, 2008). Further the similarity measures have been used to obtain similar tuple for classical fuzzy, vague and converted fuzzy sets based on SQL query processing. This proposed system diverse a resultant as a set based on supply limit/α-cut for fuzzy/vagueness/unclear information. After testing through many cases, this paper discussed a very good finding about proposed method for SQL query processing problems. Efficient daily news platform generation using natural language processing In today’s digital world, online journalism plays a vital role across various facets of humanity, starting from day-to-day lives, to the extent of deciding presidential elections. Moreover, with the growth in popularity of automation in every possible field known to humans, automated journalism is an important domain to research into. In this, there are many factors that come into play such as relative importance of news and human emotions, apart from just statistical data. To determine the impact that artificial intelligence can have on online journalism, an automatically generating information platform using AI has been developed. The objective is to generate a fully functional information platform that creates content and news articles automatically, which is achieved by analyzing internet trends, mining data related to the trends from other news sources, classifying the data, and categorizing and generating information to resemble those written by human journalists, both grammatically and linguistically. This is achieved with the help of machine learning and NLTK (Natural Language Toolkit) modules for Python which is used to process trends across social media platforms, especially Twitter. Spatial progression of estate property management system with customized freeware GIS Estate property management system mainly deals with geographical area with associated information for monitoring and managing estate properties. Geographic Information System (GIS) has potential to handle spatial and aspatial data in a single platform which makes GIS a favourable choice for estate property management system. The availability of free and open source GIS helped this technology affordable and popular in large number of applications. Hence there is a demand to develop a new or upgrade an existing non-GIS based system to a GIS based property management system. In this paper a novel case study is presented where an Oracle based system is integrated with GIS platform using freeware ArcGIS Explorer (AGX) customization. The integration of dynamic data from Oracle server is challenging due to non-supportive database type in AGX. The issue is overcome by introducing an intermediate file handling mechanism in the GIS interface using Oracle client in .net frame work. The proposed system can run in multiple nodes in a local area network and capable of fetching data from Oracle server. It serves user queries on both spatial and aspatial data that enables spatial visualisation of aspatial data. An enhanced ensemble classifier for telecom churn prediction using cost based uplift modelling Telecom, being a dynamic and competitive industry which contains an inherently high potential for customer churn, necessitating of accurate churn prediction models. Regular classification approaches fail to effectively predict churn due to low correlation levels between conventional performance metrics and business goals. This work presents an ensemble stacking incorporated with uplifting-based strategies for telecom churn prediction model. Evaluations have been performed based on conventional performance and a cost heuristic, with a major focus upon the cost heuristic. This mode of operation exhibits a high correlation levels between performance indicators and business goals, thus enabling the algorithm suitable for most cost-sensitive applications. A heterogeneous ensemble is created by using multiple algorithms to provide first level predictions. Those predictions with discrepancies are processed at the secondary level using a heuristic based combiner to provide the final predictions. Combination heuristics are fine-tuned based on the cost to predict more accurately concentrating on business goals. Subsequently, Customer uplifting is performed on final predictions, thus making the proposed model 50% more cost efficient than the state-of-the-art ensemble models. Efficient DANNLO classifier for multi-class imbalanced data on Hadoop In recent years, multi-class imbalance data classification is a major problem in big data. In such situations, we focused on developing a new Deep Artificial Neural Network Learning Optimization (DANNLO) Classifier for large collection of imbalanced data. In our proposed work, first the dataset reduction using principal component analysis for dimensionality reduction and initial centroid is computed. Then, parallel hierarchical pillar k-means clustering algorithm based on MapReduce is used to partitioning of an imbalanced data set into similar subset, which can improve the computational cost. The resultant clusters are given as input to the deep ANN for learning. In the next stage, deep neural network has been trained using the back propagation algorithm. In order to optimize the n-dimensional weight space, firefly optimization algorithm is used. Attractiveness and distance of each firefly is computed. Hadoop is used to handle these large volumes of variable size data. Imbalanced datasets is taken from ECDC (European Centre for Disease Prevention and Control) repository. The experimental results illustrated that the proposed method can significantly improve the effectiveness in classifying imbalanced data based on TP rate, F-measure, G-mean measures, confusion matrix, precision, recall, and ROC. The experimental results suggests that DANNLO classifier exceed other ordinary classifiers such as SVM and Random forest classifier on tested imbalanced data sets. Optimizing semantic LSTM for spam detection Classifying spam is a topic of ongoing research in the area of natural language processing, especially with the increase in the usage of the Internet for social networking. This has given rise to the increase in spam activity by the spammers who try to take commercial or non-commercial advantage by sending the spam messages. In this paper, we have implemented an evolving area of technique known as deep learning technique. A special architecture known as Long Short Term Memory (LSTM), a variant of the Recursive Neural Network (RNN) is used for spam classification. It has an ability to learn abstract features unlike traditional classifiers, where the features are hand-crafted. Before using the LSTM for classification task, the text is converted into semantic word vectors with the help of word2vec, WordNet and ConceptNet. The classification results are compared with the benchmark classifiers like SVM, Naïve Bayes, ANN, k-NN and Random Forest. Two corpuses are used for comparison of results: SMS Spam Collection dataset and Twitter dataset. The results are evaluated using metrics like Accuracy and F measure. The evaluation of the results shows that LSTM is able to outperform traditional machine learning methods for detection of spam with a considerable margin. Resource provision and QoS support with added security for client side applications in cloud computing Resource provision and security requirement for large-scale cloud applications is a challenging issue while design of any web based client oriented business application. Extensive research on various issues in real environment has reported that on-demand provision of resources in cloud where connectivity issue persists for heterogeneous communication channel, requires developers to consider network infrastructure and the environment, which is beyond certain control. In wireless mobile network, the network condition is always changeable and cannot be predicted neither controlled. In this paper resource provisioning and checking of continuous availability of resource to the clients has been carried out using a web based application software that uses cloud servers and data centers. Secondly, an improved security feature has been added to the mobile stations in a registered group to eliminate the unnecessary utilization of resource by unauthorized station which maliciously consumes bandwidth and other facility provided by the cloud provider. Simulation results show that the proposed system performs better than other similar approaches when compared with specific network parameters. Concurrent bacterial foraging with emotional intelligence for global optimization The integration of concurrent bacterial foraging with emotional PSO known as concurrent bacterial foraging with emotional intelligence (CBFEI) is proposed in this paper. This technique is used to optimize the functions with multiple local optima with high dimensions and real time applications with less computational cost and better accuracy. In original BFO, the bacteria positions are updated sequentially and its performance is degraded due to fixed step size. But in CBFEI, positions of bacteria are updated concurrently, which is called as concurrent bacterial foraging and mutation is used for dynamic step size to attain accurate optima with fast convergence. The psychology factors of emotion such as joyful and sad are introduced in CBF, which is treated as mutation based on emotional intelligence. The joyful bacterium enjoys in reproducing more accurate global best while bacterium will shrink from its current position, if it is sad. The premature convergence is avoided by mutation. The seven benchmark functions are used to validate the performance of CBFEI. The different evaluation parameters and ANOVA are used to compare the results of CBFEI with other optimization algorithms. The proposed technique achieves more accurate results in terms of optimum solution and better convergence as compared to other techniques. Performance investigation of MLR optical WDM network based on ITU-T conforming fibers in the presence of SRS, XPM and FWM This article investigates the performance degradation of a mixed line rate (MLR) optical wavelength division multiplexing network in the presence of the combined FWM, XPM and SRS effects. For performance evaluation a novel mathematical model is developed which evaluates the results in terms of quality-factor. Further, the simulations are performed (i) based on the optical frequency grid defined by the ITU-T Recommendation G.692, and (ii) considering the ITU-T compliant optical fibers viz., G.652, G. 652D, G. 653, G. 654, G.655, and LEAF. The obtained results show that among the various considered fibers and modulation format configuration cases, irrespective of spacing between the channels (i) SMF (G.652) and DSF (G.653) fibers provide the best and the worst performances, respectively, and (ii) a combination of the central channel transmitting at 40 Gbps using the duo-binary modulation format with adjacent channels transmitting at 10 Gbps using OOK modulation format, provides the best performance. However, with increase in spacing between the channels, the performance is enhanced owing to mitigation of the deleterious non-linear effects. Overall, the results clearly show that choice of the data rate of both, the central channel and its adjacent channel has a major effect on the MLR network performance. Managing crowds with technology: cases of Hajj and Kumbh Mela During the first 15 years of this century, seven thousand people have been crushed to death in stampedes. Many would argue that these fatalities could have been prevented by better control and management. Crowd management today needs to minimise the chances of occurrence of stampedes, fires and other disasters and also to deal with the ongoing threat of terrorism and outbreak of communicable diseases like EBOLA, HIV Aids, Swine Influenza H1N1, H1N2, various strands of flu, Severe Acute Respiratory Syndrome (SARS) and Middle Eastern Respiratory Syndrome (MERS). These challenges have created a need for using all available resources, especially modern tools and technology, when dealing with crowds. Radio Frequency Identification (RFID), which is already benefiting many industrial and government organisations around the world, may be useful for scanning crowded locations and hence in helping to prevent overcrowding. Other wireless technologies should also be considered for possible use in crowded events. Ideally, some of the regular crowded event locations should be transformed into smart cities. In this article we shall discuss different kinds of crowds and technologies for their management. In particular, we shall analyse cases where wireless and mobile technologies can be utilised effectively. The Hajj, which has witnessed several stampedes, is chosen as the case study but most of our findings would be applicable in other events like the Kumbh Mela. Assessment tool for e-services take-up (ATEST): a small Island states context e-Services help in fostering the development of a knowledge society as more citizens and businesses can access, adopt and use government services through online channels. Existing evidence in Mauritius, however, shows low usage of online government services. The goal of this research consisted of developing a framework to guide government agencies in Mauritius in increasing the likelihood of success and uptake of e-Services. This study uses a novel approach, i.e. the Real-Time Delphi method based on a consensus of expert opinions as the main research method to identify critical success factors influencing the user uptake of e-Services. Moreover, it not only proposes a framework as a solution but also operationalises the framework into a tool which can be used in practice to predict the likelihood of take-up of an e-Service implemented by a Government agency. Additionally, the tool was made flexible so that it can be used not only in the Mauritian context but can also be adapted to reflect the context of other countries. Government policy-makers can leverage on the developed framework to formulate better policies and guidelines for the successful take-up of online public services in Mauritius. Furthermore, e-Service owners can use the tool to evaluate their e-Service strategy and identify priority areas where resources and effort have to be dedicated so that their e-Services can be successfully adopted and used by citizens. The validity and effectiveness of the tool was evaluated using existing e-Services in Mauritius having varying user uptake and the results were found to reflect to a great extent the prevailing uptake situation of existing e-Services. DSP implementation of modified variable vector quantization based image compression using DCT and synthesis on FPGA In the extant context of data compression, numerous data reduction techniques have evolved and produced many innovative solutions. These elucidations may have resulted in further complexities during physical realizations. This research endeavor put forth experimentation outcomes in the field of Discrete Cosine Transform (DCT) based image compression using modified vector quantization and prototyping the algorithm on Digital Signal Processor (DSP) TMS320C6713 platform. In addition, such an algorithm is synthesized on Virtex5 XC5VSX50T Field Programmable Gate Array (FPGA). The performance metrics used and calculated here at algorithm level are Mean Square Error (MSE), Peak Signal to Noise Ratio (PSNR_, Compression Ratio (CR), Bits per Pixel (bpp), percentage Space Saving in accordance with modified variable vector quantization levels from 10 to 90. Histogram and entropy based digital image watermarking scheme In today’s era, Fortifying robustness and imperceptibility of digital watermarking has become non trivial. In this paper, a digital image watermarking algorithm based on entropy of blocks and histogram is proposed to improve imperceptibility. In this, first host image is divided into blocks and then blocks are culled on the basis of entropy value for watermark embedding. After that, watermark is embedded into selected blocks by using histogram shape method. Histogram shape method divides selected blocks into groups and further locations of pixels within groups are identified for watermark embedding. Histogram shape method makes algorithm more robust against attacks. The integration of block based histogram shape approach and entropy makes algorithm more imperceptible. Experiment results demonstrate that proposed method has an excellent imperceptibility. Proposed method also resists against noise and scaling attacks. The attack and defense of weakest-link networks We experimentally test the qualitatively different equilibrium predictions of two theoretical models of attack and defense of a weakest-link network of targets. In such a network, the attacker’s objective is to assault at least one target successfully and the defender’s objective is to defend all targets. The models differ in how the conflict at each target is modeled—specifically, the lottery and auction contest success functions (CSFs). Consistent with equilibrium in the auction CSF model, attackers utilize a stochastic “guerrilla-warfare” strategy, which involves attacking at most one target arbitrarily with a random level of force. Inconsistent with equilibrium in the lottery CSF model, attackers use the “guerrilla-warfare” strategy and assault only one target instead of the equilibrium “complete-coverage” strategy that attacks all targets. Consistent with equilibrium in both models, as the attacker’s valuation increases, the average resource expenditure, the probability of winning, and the average payoff increase (decrease) for the attacker (defender). Valuation structure in incomplete information contests: experimental evidence We experimentally examine perfectly discriminating contests under three valuation structures: pure common-value, pure private-value and a case with both private and common value components. In line with the results from the previous literature, we find that, regardless of valuation structure, contestants often choose very conservative expenditures, and very aggressive expenditures. Average expenditures exceed Nash equilibrium predictions. In valuation structures with a common value component, contestants often choose expenditures in excess of the expected value of the prize conditional on winning the contest. That is, they often guarantee themselves negative payoffs in expectation. Facile fabrication of self-healing superhydrophobic nanocomposite films enabled by near-infrared light A novel robust self-healing superhydrophobic F-SiO2/Fe3O4/EP(epoxy)/PCL nanocomposite film was fabricated through a facile two-step method including resin spraying and NPs perpendicular gradient deposition. Specially, the epoxy/polycaprolactone (EP/PCL) mixed resin matrix was prepared through a spraying process and the functioned silica/magnetic iron oxide (F-SiO2/Fe3O4) nanoparticles were then deposited on the resin matrix by gradient distribution under gravity and solvent evaporation. The THF/ethanol mixture solvent which acted as film microstructure-directing agent guided the F-SiO2/Fe3O4 nanoparticles gradient distribution in the nanocomposite films. The results showed that the cutting cracks on the nanocomposite film could be rapidly self-healed and the film would maintain its superhydrophobicity after irradiation by near-infrared light. The nanocomposite films, especially the F-SiO2(8.5wt%)/Fe3O4(1.0wt%)/EP/PCL nanocomposite film, showed high resistant properties on abrasion, corrosive liquids and UV irradiation. The recovery mechanism of the nanocomposite films was also investigated. Controlled synthesis of multi-branched gold nanodendrites by dynamic microfluidic flow system The synthesis of gold nanostructures with unique architectures has attracted a great deal of attention because of their architecture-dependent sensing, optical, and electrical properties. Gold nanodendrites with a tailored morphology have unique properties due to their enhanced surface areas caused by nanoscale branches. Although gold nanodendrites have been synthesized by many different methods, controllable and high-yield synthesis of gold nanodendrites remains a challenge. Here, for the first time, we show that multi-branched gold nanodendrite synthesis can be controlled using a dynamic microfluidic flow system with high yield and fluid dynamics that control the branching structure of the nanodentrites. The study shows that the architecture of the gold nanodendrites mainly depends on synthesis conditions such as flow dynamics, HAuCl4 concentration, and reaction time. Dendrites grew faster when the flow rate reached 3 µL min−1. We further show that by using microfluidic-assisted synthesis, simple and rapid gold nanodendrite length tuning (0.7 cm) is possible with a threefold branching and textured structure. It is shown that the growth of gold nanodendrites is significantly enhanced (1.7 times faster) under flow conditions in the microfluidic channel. This bottom-up method reduces undesirable effects related to the poor control of static growth and increased reproducibility. Such highly controllable and inexpensive microfluidic flow systems could potentially be used to fabricate high-yield gold nanodendrites for bioelectronics and sensing applications. Analysis of theoretical and experimental X-ray diffraction patterns for distinct mordenite frameworks Experimental and theoretical XRD patterns of mordenite frameworks were correlated in this work. The experimental XRD analysis showed that the incorporation of Ag and Fe ions in mordenite modified the intensity of peaks in the diffraction patterns.
 For theoretical studies, two framework models of mordenite (MOR6 and MOR7) were used. Theoretical results conducted through DFT computational simulations were able to predict correctly the angular positions of the experimental peaks observed in XRD patterns. These theoretical results showed that the ion exchange of $${\hbox {Na}^{+}}$$Na+ by $${\hbox {Ag}^{+}}$$Ag+ cations in the zeolitic framework leads to a decrease in intensity of XRD peaks {2 0 0}, {0 2 0} and {1 5 0}, similar to observed experimentally. This is caused by local structural rearrangements produced by the ion exchange. For $${\hbox {Fe}}$$Fe incorporation in zeolite, two options were considered theoretically: ion exchange and isomorphous substitution of $${\hbox {Al}^{3+}}$$Al3+ in tetrahedral positions. Kernel Cuts: Kernel and Spectral Clustering Meet Regularization This work bridges the gap between two popular methodologies for data partitioning: kernel clustering and regularization-based segmentation. While addressing closely related practical problems, these general methodologies may seem very different based on how they are covered in the literature. The differences may show up in motivation, formulation, and optimization, e.g. spectral relaxation versus max-flow. We explain how regularization and kernel clustering can work together and why this is useful. Our joint energy combines standard regularization, e.g. MRF potentials, and kernel clustering criteria like normalized cut. Complementarity of such terms is demonstrated in many applications using our bound optimization Kernel Cut algorithm for the joint energy (code is publicly available). While detailing combinatorial move-making, our main focus are new linear kernel and spectral bounds for kernel clustering criteria allowing their integration with any regularization objectives with existing discrete or continuous solvers. Robust and Optimal Registration of Image Sets and Structured Scenes via Sum-of-Squares Polynomials This paper addresses the problem of registering a known structured 3D scene, typically a 3D scan, and its metric Structure-from-Motion (SfM) counterpart. The proposed registration method relies on a prior plane segmentation of the 3D scan. Alignment is carried out by solving either the point-to-plane assignment problem, should the SfM reconstruction be sparse, or the plane-to-plane one in case of dense SfM. A Polynomial Sum-of-Squares optimization theory framework is employed for identifying point-to-plane and plane-to-plane mismatches, i.e. outliers, with certainty. An inlier set maximization approach within a Branch-and-Bound search scheme is adopted to iteratively build potential inlier sets and converge to the solution satisfied by the largest number of assignments. Plane visibility conditions and vague camera locations may be incorporated for better efficiency without sacrificing optimality. The registration problem is solved in two cases: (i) putative correspondences (with possibly overwhelmingly many outliers) are provided as input and (ii) no initial correspondences are available. Our approach yields outstanding results in terms of robustness and optimality. Locality Preserving Matching Seeking reliable correspondences between two feature sets is a fundamental and important task in computer vision. This paper attempts to remove mismatches from given putative image feature correspondences. To achieve the goal, an efficient approach, termed as locality preserving matching (LPM), is designed, the principle of which is to maintain the local neighborhood structures of those potential true matches. We formulate the problem into a mathematical model, and derive a closed-form solution with linearithmic time and linear space complexities. Our method can accomplish the mismatch removal from thousands of putative correspondences in only a few milliseconds. To demonstrate the generality of our strategy for handling image matching problems, extensive experiments on various real image pairs for general feature matching, as well as for point set registration, visual homing and near-duplicate image retrieval are conducted. Compared with other state-of-the-art alternatives, our LPM achieves better or favorably competitive performance in accuracy while intensively cutting time cost by more than two orders of magnitude. Understanding Image Representations by Measuring Their Equivariance and Equivalence Despite the importance of image representations such as histograms of oriented gradients and deep Convolutional Neural Networks (CNN), our theoretical understanding of them remains limited. Aimed at filling this gap, we investigate two key mathematical properties of representations: equivariance and equivalence. Equivariance studies how transformations of the input image are encoded by the representation, invariance being a special case where a transformation has no effect. Equivalence studies whether two representations, for example two different parameterizations of a CNN, two different layers, or two different CNN architectures, share the same visual information or not. A number of methods to establish these properties empirically are proposed, including introducing transformation and stitching layers in CNNs. These methods are then applied to popular representations to reveal insightful aspects of their structure, including clarifying at which layers in a CNN certain geometric invariances are achieved and how various CNN architectures differ. We identify several predictors of geometric and architectural compatibility, including the spatial resolution of the representation and the complexity and depth of the models. While the focus of the paper is theoretical, direct applications to structured-output regression are demonstrated too. Deep Appearance Models: A Deep Boltzmann Machine Approach for Face Modeling The “interpretation through synthesis” approach to analyze face images, particularly Active Appearance Models (AAMs) method, has become one of the most successful face modeling approaches over the last two decades. AAM models have ability to represent face images through synthesis using a controllable parameterized Principal Component Analysis (PCA) model. However, the accuracy and robustness of the synthesized faces of AAMs are highly depended on the training sets and inherently on the generalizability of PCA subspaces. This paper presents a novel Deep Appearance Models (DAMs) approach, an efficient replacement for AAMs, to accurately capture both shape and texture of face images under large variations. In this approach, three crucial components represented in hierarchical layers are modeled using the Deep Boltzmann Machines (DBM) to robustly capture the variations of facial shapes and appearances. DAMs are therefore superior to AAMs in inferencing a representation for new face images under various challenging conditions. The proposed approach is evaluated in various applications to demonstrate its robustness and capabilities, i.e. facial super-resolution reconstruction, facial off-angle reconstruction or face frontalization, facial occlusion removal and age estimation using challenging face databases, i.e. Labeled Face Parts in the Wild, Helen and FG-NET. Comparing to AAMs and other deep learning based approaches, the proposed DAMs achieve competitive results in those applications, thus this showed their advantages in handling occlusions, facial representation, and reconstruction. Fabrication of melamine–urea–formaldehyde/paraffin microcapsules modified with cellulose nanocrystals via in situ polymerization The study aims to obtain leakage-free, thermally stable melamine–urea–formaldehyde (MUF)/paraffin microcapsules modified with cellulose nanocrystals (CNCs) by in situ polymerization technique. The morphological features and physicochemical characteristics of CNCs were investigated by transmission electron microscopy, zeta potential measurement, X-ray diffraction, and Fourier transform infrared spectroscopy. Optical microscope measurement, field emission scanning electron microscopy, thermogravimetric analyzer, and differential scanning calorimetry were employed to characterize the morphology, size distribution, and phase change properties of MUF/paraffin microcapsules. The characterization results showed that MUF/paraffin microcapsules were successfully fabricated and modified with CNCs. The CNCs had no impact on the chemical structure or crystal type of MUF/paraffin microcapsules. When the CNC addition was 0.2 wt%, the phase change latent heat values of melting and crystallization of MUF/paraffin microcapsules were, respectively, about 104.5 J/g and 102.8 J/g and the encapsulation efficiency was about 59.8%. Improving corrosion resistance of additively manufactured nickel–titanium biomedical devices by micro-arc oxidation process Nickel–titanium (NiTi) alloys have recently attracted considerable attention due to their unique properties, i.e., shape memory effect and superelasticity. In addition, these promising alloys demonstrate unique biocompatibility, represented in their high stability and corrosion resistance in aqueous environments, qualifying them to be used inside the human body. In recent years, additive manufacturing (AM) processes have been envisioned as an enabling method for the efficient production of NiTi components with complex geometries as patient-specific implants. In spite of its great capabilities, AM as a novel fabrication process may reduce the corrosion resistance of NiTi parts leading to the excess release of the harmful Ni ions as the main corrosion byproducts. The main goal of this study is to create and evaluate a micro-arc oxidation (MAO) coating in order to enhance the corrosion resistance of additively manufacture NiTi medical devices. To this end, the process voltage and electrolyte used to produce MAO coating have been investigated and optimized. The corrosion characteristics of the MAO-coated specimens revealed that the proposed coating methodology significantly improves the corrosion resistance of NiTi parts produced using AM process. A wearable strain sensor based on the ZnO/graphene nanoplatelets nanocomposite with large linear working range Flexible strain sensors are attracting more and more attentions in wearable devices and electronic skins. Currently, the fabrication of flexible strain sensor with features of high sensitivity and wide linear working range is still a great challenge. Herein, a stretchable and wearable strain sensor is fabricated with the ZnO nanoparticles (NPs)/graphene nanoplatelets nanocomposite (ZnO/GNP NC) as both the sensing element and reinforcement phase. The ZnO/GNP NC strain sensor exhibits fascinating performance, including high mechanical properties (fracture strength of 0.6 MPa and elongation of ~ 90%), large working range of 0–44%, high sensitivity (gauge factor of 8.8–12.8), and good reproducibility over 1700 cycles. Importantly, the ZnO/GNP NC strain sensor holds perfect linearity (R2 = 0.999) in the whole working range, which can be attributed to the coupling effect between the ZnO NPs and the GNP. The ZnO/GNP NC strain sensor can not only detect large human motions such as elbow rotation, wrist rotation, clenching fist, and waving badminton racket, but also monitor subtle human motions in real time, such as pulse, phonation, coughing, and swallowing. The wide linear working range of the ZnO/GNP NC strain sensor makes it a potential choice for the application of wearable devices. Characterizing elastic and piezoelectric constants of piezoelectric materials from one sample using resonant ultrasound spectroscopy The full matrix material constants of a piezoelectric sample obtained by ultrasonic pulse-echo and IEEE resonance methods probably are inconsistent because they are obtained from multiple samples. To guarantee the self-consistency of the characterization, all material constants should be derived from the same sample. Resonant ultrasound spectroscopy (RUS) can characterize all elastic and piezoelectric constants of piezoelectric materials using only one sample. The most challenging barrier in RUS is the identification of sufficient resonance modes from the ultrasound resonant spectrum. To reduce the difficulty of mode identification and improve its reliability, the ultrasound resonant spectra of a rectangular parallelepiped piezoelectric sample are suggested to be measured under two different boundary conditions in this study. One boundary condition is that all surfaces of the sample are free. Another is that one surface of the sample is equipotential, i.e., this surface is attached with an electrode, and the other surfaces are free. Subsequently, resonance frequencies are identified from the aforementioned ultrasound resonant spectra and used in the RUS inversion. If 2n modes are required in the inversion, only n modes are to be identified from each spectrum. It is noteworthy that the higher the frequency, the more difficult is the mode identification. Therefore, identifying n modes from the spectrum is far easier than identifying 2n modes from it. The characterization of a PZT-8 sample is used as an example to verify the validity of the method presented herein. Design of a biaxial tensile testing device and cruciform specimens for large plastic deformation in the central zone A critical issue in biaxial tensile tests is that the central area of the cruciform specimen does not deform to a desired level when the specimen has fractured in other areas. Contrary to the central area thickness reduction approach adopted by the bulk of researchers, this study introduces a thickness-increased sandwich specimen for large strains in the central zone to evaluate the formability and hardening behaviors of sheet metals under complex forming situations. The initial cruciform specimen is strengthened outside the central area by attaching a metal plate in similar shape to each side. This novel specimen has the ability to avoid the inherent characteristics of materials being altered in a thickness-reduced specimen during the material-removing process; moreover, it is applicable to thin sheet metals. Finite element simulations are used for parameter optimization. The optimum set of three key parameters: fillet (R) between the arms, radius (r) of the central zone, and line angle between the notch edge and the horizontal axis (θ), appear to be R = 5.0 mm, r = 4.0 mm, and θ = 21°. The optimized design is corroborated using a new testing device with better synchronization on which various displacement ratios of 1:0 and 0:1 can be implemented by applying 90° wedges so that plane strain conditions in the central zone can be achieved. A maximum equivalent plastic strain of approximately 15% is achieved in the central zone, indicating the effectiveness of the strengthened specimen in increasing the plastic deformation in the desired zone. Physiological response of the toxic and non-toxic strains of a bloom-forming cyanobacterium Microcystis aeruginosa to changing ultraviolet radiation regimes Microcystis aeruginosa, a common bloom-forming cyanobacterium with both non-toxic and toxic strains, experiences a variable light environment due to buoyancy regulation and the variable mixing of the water columns. However, little is known on how non-toxic and toxic strains respond to changing photosynthetically active radiation (PAR) and ultraviolet radiation (UVR). Here, the non-toxic and toxic strains of M. aeruginosa were exposed to simulated solar radiation for 6 h, and their physiological changes were investigated at different irradiance levels of UVR (295–400 nm) in combination with PAR (400–700 nm). Our results showed that UVR at each level resulted in a larger inhibition on the maximum photochemical yield of Photosystem II (PSII) in the toxic strain. The non-toxic strain showed a quicker rise in the non-photochemical quenching when PAR + UVR were below 40.8 + 5.0 W m−2 and higher exopolysaccharide content at each radiation level, while the toxic strain exhibited stronger recovery capacity and superoxide dismutase (SOD) activity compared with the non-toxic strain, particularly for cells treated in the presence of UVR. In addition, UVR induced much higher content of microcystin in the toxic strain with the increase of irradiance levels, but decreased it when UVR was higher than 7.3 W m−2. Although UVR led to growth inhibition in both strains, the toxic strain showed much higher specific growth rate under UVR in comparison with the non-toxic strain. Our results indicate that the toxic strain has a competitive advantage relative to the non-toxic strain in a changing light environment with increase of UVR and PAR via stronger antioxidant capacity (higher SOD activity and the synthesis of microcystin) and quicker PSII recovery capacity. Dynamics of spatiotemporal heterogeneity of cyanobacterial blooms in large eutrophic Lake Taihu, China Cyanobacterial blooms caused by eutrophication in Lake Taihu, China are recognized as highly heterogeneous spatiotemporally. It is assumed that the high spatiotemporal heterogeneity of algal blooms is determined by divergence/convergence processes in the fluid medium. To address this issue, three episodes of the dominant spatial patterns of hourly simulated divergence fields of current in Lake Taihu in July of 2012 were analyzed using a hydrodynamic numerical model combined with the Empirical Orthogonal Function (EOF) method. The results showed that, on days that blooms occurred, the first two EOF modes explained 89.4% of the variability and the dominant spatial patterns of stronger convergence zones were in agreement with the regions of bloom occurrence and accumulation. When no blooms occurred, the first EOF mode explained 72.5% of the variability and divergence zones were dominant in the lake. Both the simulated hourly average divergence field and the first EOF mode in the time interval in which blooms occurred further confirmed that blooms accumulate in the current convergence zones. These findings explain the dynamic mechanism of occurrence of cyanobacterial blooms and will facilitate forecasting of short-term blooms for protecting drinking water supplies and managing risk. The shock-induced chemical reaction behaviour of Al/Ni composites by cold rolling and powder compaction Al/Ni composites are typical structural energetic materials, which have dual functions of structural and energetic characteristics. In order to investigate the influence of manufacturing methods on shock-induced chemical reaction (SICR) behaviour of Al/Ni composites, Al/Ni multi-layered composites with 3–5 cold-rolling passes and Al/Ni powder composites were obtained. Microstructural observation using scanning electron microscopy (SEM) and two-step impact initiation experiments were performed on the four Al/Ni composites. Furthermore, mesoscale simulations, through importing SEM images into the finite element analysis to reflect the real microstructures of the composites, were performed to analyse the particle deformation and temperature rise under shock compression conditions. The experimental results showed the distinct differences on the SICR characteristics among the four Al/Ni composites (i.e. by 3, 4 and 5 cold-rolling passes and powder compaction). The manufacturing methods provided the control of the particle sizes, particle distribution and the content of the interfacial intermetallics at scale of different microstructures, which ultimately affected the temperature distribution, as well as the contact between Al and Ni in Al/Ni composites under shock loading. As a result, the Al/Ni powder composites showed the highest energy release capacity among the four composites, while the energy release capability of Al/Ni multi-layered composites decreased with the growth of rolling passes. Easily recyclable photocatalyst Bi2WO6/MOF/PVDF composite film for efficient degradation of aqueous refractory organic pollutants under visible-light irradiation A novel composite film, Bi2WO6/MIL-53(Al)/PVDF, was successfully fabricated through a hydrothermal process combined with immersion phase inversion method and characterized by XRD, SEM, XPS, FTIR and UV–vis DRS techniques. The photocatalytic activity of the as-synthesized composite film was investigated through the degradation of dyes Rhodamine B (RhB), Methylene Blue and Malachite Green in water under visible-light irradiation. And the composite film prepared at the ratio of 6 wt% of 1.25-BWO/MIL (with optimal ratio of Bi2WO6 to MIL-53(Al)) to poly(vinylidene fluoride) (PVDF) casting solution showed the highest photocatalytic activity. A 95.3% photocatalytic degradation of RhB was achieved at 1.6 dm2/L of 6-BWO/MIL/PVDF dosage and 10 mg/L of initial RhB concentration. The increased photocatalyst dosage, optimal initial RhB concentration and weak acidity should be responsible for the increased photocatalytic activity. The photocatalytic degradation mechanism was investigated by quenching tests, revealing that the predominant reactive species in the “BWO/MIL/PVDF-RhBaq-visible light” system were h+, O2−· and ·OH. The formation of heterojunction structure between Bi2WO6 and MIL-53(Al) improved the photocatalytic activity. Moreover, the recycling test of 6-BWO/MIL/PVDF composite film displayed its excellent reusability up to 15 cycles. The polymer coupling was demonstrated to be one of the valuable immobilization methods for powder photocatalysts. Effects of piezoelectric-phase orientations on magnetoelectric coefficient in AlN–FeCoSiB multiferroic composites In this study, orientation dependences of ps11′, ps12′, pε33′ and d31′ for piezoelectric phase were investigated along arbitrary directions using relative experimental data and original matrices for AlN with point group of 6mm. Then, effects of various orientations of piezoelectric phase on magnetoelectric coefficient of AlN–FeCoSiB laminated multiferroic composites were studied. It was found that ps11′ is a constant while ps12′, pε33′ and d31′ exhibit high dependence on crystal orientations with their MAX values existing along [001]-axis. The magnetoelectric coefficient αE33′ increases with decreased angle θ apart from c-axis of AlN in range of 90°–0°; the MAX αE33′ = 5405 mV/cm Oe exists in AlN ∥ [001]—FeCoSiB. Along specific preferred orientations of piezoelectric phase, twofold–fivefold of enhancement of αE33 can be obtained compared to randomly arranged composites. Volume fraction is found to be independent of orientations of component phases, and the volume fraction for magnetic phase f around 0.75 obtains the largest αE33. The results suggest an approach of enhancing magnetoelectric coefficient of composite multiferroic materials through crystal-orientation controls of single crystals or textured ceramics. 3D pore analysis of gasoline particulate filters using X-ray tomography: impact of coating and ash loading Since particulate emissions control technologies are dependent on filtration technologies, development of porous materials with optimized pore structures is crucial to improve filtration efficiency and pressure drop across filters. Despite increasing attention to 3D measurements of porous materials, there are few reports of pore structure investigations of diesel/gasoline particulate filters (D/GPFs) using 3D visualization techniques. In this work, GPF 3D pore structures were examined using X-ray tomography (XRT) to identify the impacts of catalyst coating or ash loading. Voxel resolution of 2.93 µm made it impossible to distinguish coating or ash materials from the cordierite substrate or to recognize smaller pores than the voxel resolution. However, pores up to 200 µm, which are responsible for the most pore volume, were successfully analyzed. Coating and ash loading resulted in lowering average pore diameter, total porosity, and open porosity, as the peak density of pore diameter at 60 µm decreased, while pores below 20 µm increased. Also, the visualized closed pores, which were homogeneously distributed throughout the bare filter, tended to get larger from inlet to outlet sides and more likely to be on the surface inlet due to coating and ash loading, indicating gas pathways originally existing in open pores were blocked due to coating and ash loading, leading to increased pressure drop. The ash impact was found to be more noticeable on mid and back positions than on front position. In addition, the investigation of areal porosity along the direction of gas flow suggested that while ash penetration could reach the outlet side as noted from increased closed pore population, most ash particles would be contained up to 150 µm. The 2D crosscut microscopic analysis that requires destructive procedures with the limited examination area could provide underestimation of ash penetration, whereas the 3D XRT analysis seems to provide more accurate information of pore structure changes due to ash loading at different locations. New underfill material based on copper nanoparticles coated with silica for high thermally conductive and electrically insulating epoxy composites With the microelectronics technology going toward its physical limits and the emergence of three-dimensional chip stack architectures, now more than ever there are both needs and opportunities for novel materials to help address some of these pressing thermal management challenges. In this paper, a high-thermal-conductivity insulative SiO2-coated nano-Cu particle is prepared for new-type underfill materials of high-performance microelectronics packaging. It was found that nano-Cu can be successfully coated with SiO2 by using the surface modification between cetyltrimethyl ammonium bromide and silane coupling agent although nano-Cu particles have silicon-disordered property during the coating process of tetraethyl orthosilicate hydrolysis. Moreover, the thermal conductivity of epoxy mixed with nano-Cu@SiO2 as the packaging underfill is dramatically increased from 0.15 W/m K of the pure-EP and 0.60 W/m K of the EP/SiO2 to 2.9 W/m K due to electronic heat transfer, heat network and fast heat transfer center. It effectively releases the heat generated by the IC device, and the service life of the device is significantly improved from 63 min of pure-EP and 350 min of the EP/nano-SiO2 to 1039 min. The new material creates a challenging environment for keeping modern electronic devices cool, a critical factor in determining their speed, efficiency and reliability. Program implementation and effectiveness of a national workplace physical activity intervention: UPnGO with ParticipACTION InterventionUPnGO with ParticipACTION (UPnGO) is a 6-week workplace physical activity (PA) initiative aiming to increase habitual PA (steps) during the workday. Core intervention components included (1) self-monitoring of steps and action planning behaviours using a Web/mobile app with incentives and (2) organizational support, which included senior management’s role modeling and endorsement of the program.Research questionWhat is the effectiveness and levels of implementation of the UPnGO intervention? What is the relationship between effectiveness and levels of implementation?MethodsA single-arm, pre-/post-test study design was used. Participants were 660 employees from nine organizations who had valid step data and complete socio-demographic information at baseline. The primary outcome (mean daily steps) was assessed by Garmin VivoFit. Using the usage data from the UPnGO web-based system, a composite score for levels of implementation was calculated based on participant’s compliance with the self-monitoring component and senior management’s role modeling. Associations of interest were analyzed using linear mixed-effects models.ResultsLevels of implementation were highly variable across organizations (mean = 68.22% ± 18.75, range = 19.8 to 100%). A significant Time × Implementation (IM) status interaction effect was observed. When stratified by IM status, a significant increase in mean daily steps at week 6 was found among participants in the high (β = 540.01 ± 202.69, p = 0.011) but not low (β = − 81.54 ± 291.96, p = 0.78) implementation group.ConclusionFindings suggest significant intervention effects in increasing average daily steps among participants who were exposed to optimal levels of implementation (~ 70%). UPnGO may be a scalable workplace PA intervention at a national level, although this needs further verification with more rigorous study designs.RésuméInterventionUPnGO with ParticipACTION (UPnGO) est une initiative d’activité physique (AP) en milieu de travail d’une durée de 6 semaines qui vise à augmenter l’AP habituelle (le nombre de pas) durant la journée de travail. Les éléments centraux de l’intervention étaient : 1) l’autosurveillance des pas et des comportements de planification d’actions avec une appli Web/mobile assortie d’incitations et 2) un appui organisationnel incluant l’exemple de la haute direction et son appui moral au programme.Questions de rechercheQuelle est l’efficacité et quels sont les niveaux de mise en œuvre de l’intervention UPnGO? Quelle est la relation entre l’efficacité et les niveaux de mise en œuvre?MéthodeUn protocole d’étude pré- et post-test sur groupe unique a été utilisé. Les participants étaient 660 employés de neuf organismes pour lesquels il existait au départ des données validées sur le nombre de pas et un profil sociodémographique complet. Le résultat principal (les pas moyens quotidiens) a été calculé avec l’appareil VivoFit de Garmin. À l’aide des données d’utilisation du système en ligne UPnGO, une note composite pour le niveau de mise en œuvre a été calculée d’après la conformité des participants à l’élément d’autosurveillance et l’exemple de la haute direction. Les associations intéressantes ont été analysées à l’aide de modèles linéaires à effets mixtes.RésultatsLes niveaux de mise en œuvre ont beaucoup varié d’un organisme à l’autre (moyenne = 68,22 % ± 18,75, intervalle = 19,8 % à 100 %). Un effet d’interaction significatif Temps x Mise en œuvre (MEO) a été observé. Après stratification selon le statut de MEO, une hausse significative des pas moyens quotidiens a été constatée la 6e semaine chez les participants du groupe de mise en œuvre élevée (β = 540,01 ± 202,69, p = 0,011), mais non dans ceux du groupe où la mise en œuvre était faible (β = −81,54 ± 291,96, p = 0,78).ConclusionCes résultats indiquent que l’intervention a eu des effets sensibles en augmentant les pas moyens quotidiens des participants exposés à des niveaux optimaux de mise en œuvre (~ 70 %). UPnGO pourrait être une intervention d’AP en milieu de travail extensible à l’échelle nationale, mais cela nécessiterait de plus amples vérifications avec des protocoles d’étude plus rigoureux. Peer engagement barriers and enablers: insights from people who use drugs in British Columbia, Canada ObjectivesGlobally, engaging people who have used drugs, or peers, in decision-making has been increasingly touted as a best practice approach to developing priorities, programs, and policies. Peer engagement ensures decisions are relevant, appropriate, and effective to the affected community. However, ensuring that inclusion is accessible and equitable for those involved remains a challenge. In this study, we examined the perspectives of people who use or have used illicit drugs (PWUD) on peer engagement in health and harm reduction settings across British Columbia (BC), Canada.MethodsThe Peer Engagement and Evaluation Project used a participatory approach to conducting 13 peer-facilitated focus groups (n = 83) across BC. Focus group data were coded and analyzed with five peer research assistants. Themes about the nature of peer engagement were generated. From this analysis, peer engagement barriers and enablers were identified.ResultsBarriers to peer engagement included individual, geographical, systemic, and social factors. Issues related to stigma, confidentiality, and mistrust were intensely discussed among participants. Being “outed” in one’s community was a barrier to engagement, particularly in rural areas. Participants voiced that compensation, setting, and the right people help facilitate and motivate engagement. Peer networks are an essential ingredient to engagement by promoting support and advocacy.ConclusionPWUD are important stakeholders in decisions that affect them. This cross-jurisdictional study investigated how PWUD have experienced engagement efforts in BC, identifying several factors that influence participation. Meaningful engagement can be facilitated by attention to communication, relationships, personal capacity, and compassion between peers and other professionals.RésuméObjectifsÀ l’échelle mondiale, le dialogue avec les personnes ayant consommé de la drogue (les « pairs ») à la prise de décisions est de plus en plus considéré comme une pratique exemplaire pour l’élaboration de priorités, de programmes et de politiques. Le dialogue avec les pairs mène à des décisions pertinentes, appropriées et efficaces dans la communauté touchée. Il demeure toutefois difficile d’assurer l’accessibilité et l’équité de l’intégration des personnes en cause. Dans cette étude, nous avons examiné les points de vue de personnes consommant ou ayant consommé de la drogue (PCACD) au sujet du dialogue avec les pairs dans les milieux de la santé et de la réduction des méfaits en Colombie-Britannique, au Canada.MéthodeCe projet de participation des pairs et d’évaluation a fait appel à une démarche participative pour mener 13 groupes de discussion animés par des pairs (n = 83) en Colombie-Britannique. Les données des groupes de discussion ont été codées et analysées avec cinq pairs adjoints à la recherche. Des thèmes sur la nature du dialogue avec les pairs sont ressortis. L’analyse a permis de cerner les éléments qui entravent ou qui favorisent le dialogue avec les pairs.RésultatsLes éléments qui entravent le dialogue avec les pairs étaient des facteurs individuels, géographiques, systémiques et sociaux. La stigmatisation, la confidentialité et la méfiance ont été des questions chaudement discutées entre les participants. Le fait d’être révélé dans son propre milieu comme étant consommatrice ou consommateur de drogue constituait un obstacle au dialogue, surtout en zone rurale. Les participants ont exprimé l’avis que la rémunération, le lieu et les « bonnes personnes » facilitent le dialogue et motivent les gens à s’impliquer. Les réseaux de pairs sont un ingrédient essentiel du dialogue, car ils favorisent l’entraide et la défense des intérêts.ConclusionLes PCACD sont d’importants acteurs dans les décisions qui les concernent. Cette étude à l’échelle intergouvernementale a porté sur la perception par des PCACD vivant en Colombie-Britannique des démarches visant à les faire participer; plusieurs facteurs influant sur le dialogue ont été recensés. Un dialogue sérieux peut être favorisé par une attention à la communication, aux relations, aux capacités individuelles et à la compassion entre les pairs et les autres professionnels. Influence of physical activity, screen time and sleep on inmates’ body weight during incarceration in Canadian federal penitentiaries: a retrospective cohort study ObjectiveRecent research found that inmates experience undesirable and rapid weight gain during incarceration in Canadian federal penitentiaries. However, little is known about what factors and daily movement behaviours (e.g., physical activity, screen time, and sleep) influence weight gain during incarceration. This study examines how these 24-h movement/non-movement behaviours contribute to weight gain during incarceration.MethodsThis retrospective cohort study explored how weight change outcomes during incarceration (weight change, body mass index (BMI) change, and yearly weight gain) were influenced by physical activity, screen time, and sleep in a convenience sample of 754 inmates. The outcome measures were taken twice, once from participants’ medical chart at admission and again during a face-to-face follow-up interview (conducted in 2016–2017; mean follow-up time of 5.0 ± 8.3 years). Physical activity, screen time, and sleep were self-reported. The statistical analysis was chi-square testing, non-parametric median comparison testing, and regression analysis to control for confounders.ResultsInmates who engaged in at least 60 min of daily physical activities gained less weight (4.5 kg) compared to inmates who reported not exercising (8.3 kg). Different types of exercise (cardiovascular exercises, weight lifting, and team sports) were helpful at limiting weight gain, but playing sports was the most effective. Screen time and sleep were not associated with weight gain outcomes.ConclusionAmong the behaviours examined, physical inactivity was significantly associated with higher weight gain during incarceration. However, even high levels of physical activity (> 60 min/day) were not sufficient to eliminate weight gain during incarceration in Canada.RésuméObjectifDes études récentes démontrent que les détenus gagnent du poids de façon excessive pendant leur incarcération en pénitenciers fédéraux au Canada. Cependant, il y a peu d’information sur les comportements (c’est-à-dire l’activité physique, le temps passé devant un écran, et le sommeil) qui contribuent à ce gain de poids. Cette étude examine comment ces comportements influencent le gain de poids des détenus durant leur incarcération.MéthodesCette étude de cohorte rétrospective examine comment les indicateurs de changement de poids (changement de poids, changement d’indice de masse corporelle (IMC), et gain de poids annuel) ont varié en fonction de l’activité physique, du temps passé devant un écran et du sommeil, dans un échantillon de convenance de 754 détenus. Le poids et la taille ont été mesurés à deux reprises, soit à l’admission (tiré du dossier médical), puis lors de l’entrevue réalisée en 2016–2017 (durée d’incarcération moyenne de 5,0 ± 8,3 ans). Les comportements évalués ont été autodéclarés pendant l’entrevue. Les analyses statistiques réalisées incluent des tests du khi-carré et des analyses de régression.RésultatsLes détenus les plus actifs (>60 min par jour d’activité physique) ont gagné moins de poids (4,5 kg) que les détenus inactifs (8,3 kg). Les exercices cardiovasculaires, la musculation et le sport d’équipe ont réduit le gain de poids, mais les sports d’équipes ont été les plus efficaces. Le temps passé devant un écran et le sommeil n’ont pas influencé le changement de poids.ConclusionParmi les facteurs évalués, l’activité physique est le principal facteur qui a limité le gain de poids durant l’incarcération. Toutefois, même à des niveaux d’activité physique élevés (>60 min par jour), les détenus canadiens ont gagné du poids pendant leur incarcération.